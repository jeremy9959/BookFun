<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>
  <meta charset="utf-8" />
  <meta name="generator" content="pandoc" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes" />
  <title>-</title>
  <style>
    html {
      line-height: 1.5;
      font-family: Georgia, serif;
      font-size: 20px;
      color: #1a1a1a;
      background-color: #fdfdfd;
    }
    body {
      margin: 0 auto;
      max-width: 36em;
      padding-left: 50px;
      padding-right: 50px;
      padding-top: 50px;
      padding-bottom: 50px;
      hyphens: auto;
      overflow-wrap: break-word;
      text-rendering: optimizeLegibility;
      font-kerning: normal;
    }
    @media (max-width: 600px) {
      body {
        font-size: 0.9em;
        padding: 1em;
      }
    }
    @media print {
      body {
        background-color: transparent;
        color: black;
        font-size: 12pt;
      }
      p, h2, h3 {
        orphans: 3;
        widows: 3;
      }
      h2, h3, h4 {
        page-break-after: avoid;
      }
    }
    p {
      margin: 1em 0;
    }
    a {
      color: blue;
    }
    a:visited {
      color: blue;
    }
    img {
      max-width: 100%;
    }
    h1, h2, h3, h4, h5, h6 {
      margin-top: 1.4em;
    }
    h5, h6 {
      font-size: 1em;
      font-style: italic;
    }
    h6 {
      font-weight: normal;
    }
    ol, ul {
      padding-left: 1.7em;
      margin-top: 1em;
    }
    li > ol, li > ul {
      margin-top: 0;
    }
    blockquote {
      margin: 1em 0 1em 1.7em;
      padding-left: 1em;
      border-left: 2px solid #e6e6e6;
      color: #606060;
    }
    code {
      font-family: Menlo, Monaco, 'Lucida Console', Consolas, monospace;
      font-size: 85%;
      margin: 0;
    }
    pre {
      margin: 1em 0;
      overflow: auto;
    }
    pre code {
      padding: 0;
      overflow: visible;
      overflow-wrap: normal;
    }
    .sourceCode {
     background-color: transparent;
     overflow: visible;
    }
    hr {
      background-color: #1a1a1a;
      border: none;
      height: 1px;
      margin: 1em 0;
    }
    table {
      margin: 1em 0;
      border-collapse: collapse;
      width: 100%;
      overflow-x: auto;
      display: block;
      font-variant-numeric: lining-nums tabular-nums;
    }
    table caption {
      margin-bottom: 0.75em;
    }
    tbody {
      margin-top: 0.5em;
      border-top: 1px solid #1a1a1a;
      border-bottom: 1px solid #1a1a1a;
    }
    th {
      border-top: 1px solid #1a1a1a;
      padding: 0.25em 0.5em 0.25em 0.5em;
    }
    td {
      padding: 0.125em 0.5em 0.25em 0.5em;
    }
    header {
      margin-bottom: 4em;
      text-align: center;
    }
    #TOC li {
      list-style: none;
    }
    #TOC a:not(:hover) {
      text-decoration: none;
    }
    code{white-space: pre-wrap;}
    span.smallcaps{font-variant: small-caps;}
    span.underline{text-decoration: underline;}
    div.column{display: inline-block; vertical-align: top; width: 50%;}
    div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
    ul.task-list{list-style: none;}
  </style>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
  
</head>
<body>
<h1 id="linear-regression">Linear Regression</h1>
<h2 id="sec:Intro">Introduction</h2>
<p>Suppose that we are trying to study two quantities <span class="math inline">\(x\)</span> and <span class="math inline">\(y\)</span> that we suspect are related – at least approximately – by a linear equation <span class="math inline">\(y=ax+b\)</span>. Sometimes this linear relationship is predicted by theoretical considerations, and sometimes it is just an empirical hypothesis.</p>
<p>For example, if we are trying to determine the velocity of an object travelling towards us at constant speed, and we measure measure the distances <span class="math inline">\(d_1, d_2, \ldots, d_n\)</span> between us and the object at a series of times <span class="math inline">\(t_1, t_2, \ldots, t_n\)</span>, then since “distance equals rate times time” we have a theoretical foundation for the assumption that <span class="math inline">\(d=rt+b\)</span> for some constants <span class="math inline">\(r\)</span> and <span class="math inline">\(b\)</span>. On the other hand, because of unavoidable experimental errors, we can’t expect that this relationship will hold exactly for the observed data; instead, we likely get a graph like that shown in <span class="citation" data-cites="fig:dvt">@fig:dvt</span>. We’ve drawn a line on the plot that seems to capture the true slope (and hence velocity) of the object.</p>
<figure>
<img src="img/distance-vs-time.png" id="fig:dvt" style="width:50.0%" alt="Physics Experiment" /><figcaption aria-hidden="true">Physics Experiment</figcaption>
</figure>
<p>On the other hand, we might look at a graph such as <span class="citation" data-cites="fig:mpg-vs-displacement">@fig:mpg-vs-displacement</span>, which plots the gas mileage of various car models against their engine size (displacement), and observe a general trend in which bigger engines get lower mileage. In this situation we could ask for the best line of the form <span class="math inline">\(y=mx+b\)</span> that captures this relationship and use that to make general conclusions without necessarily having an underlying theory.</p>
<figure>
<img src="img/mpg-vs-displacement.png" id="fig:mpg-vs-displacement" style="width:50.0%" alt="MPG vs Displacement ( @irvine )" /><figcaption aria-hidden="true">MPG vs Displacement ( <span class="citation" data-cites="irvine">@irvine</span> )</figcaption>
</figure>
<h2 id="sec:Calculus">Least Squares (via Calculus)</h2>
<p>In either of the two cases above, the question we face is to determine the line <span class="math inline">\(y=mx+b\)</span> that “best fits” the data <span class="math inline">\(\{(x_i,y_i)_{i=1}^{N}\}\)</span>. The classic approach is to determine the equation of a line <span class="math inline">\(y=mx+b\)</span> that minimizes the “mean squared error”:</p>
<p><span class="math display">\[ MSE(m,b) = \frac{1}{N}\sum_{i=1}^{n} (y_i-mx_i-b)^2 \]</span></p>
<p>It’s worth emphasizing that the <span class="math inline">\(MSE\)</span> is a function of two variables – the slope <span class="math inline">\(m\)</span> and the intercept <span class="math inline">\(b\)</span> – and that the data points <span class="math inline">\(\{(x_i,y_i)\}\)</span> are constants for these purposes. Furthermore, it’s a quadratic function in those two variables. Since our goal is to find <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> that minimize the <span class="math inline">\(MSE\)</span>, we have a Calculus problem that we can solve by taking partial derivatives and setting them to zero.</p>
<p>To simplify the notation, let’s abbreviate <span class="math inline">\(MSE\)</span> by <span class="math inline">\(E\)</span>.</p>
<p><span class="math display">\[\begin{aligned} \frac{\partial E}{\partial m} &amp;=
\frac{1}{N}\sum_{1}^{N}-2x_i(y_i-mx_i-b) \\ \frac{\partial E}{\partial
b} &amp;= \frac{1}{N}\sum_{1}^{N}-2(y_i-mx_i-b) \\ \end{aligned} \]</span></p>
<p>We set these two partial derivatives to zero, so we can drop the <span class="math inline">\(-2\)</span> and regroup the sums to obtain two equations in two unknowns (we keep the <span class="math inline">\(\frac{1}{N}\)</span> because it is illuminating in the final result):</p>
<p><span class="math display">\[ \begin{aligned} \frac{1}{N}(\sum_{i=1}^{N} x_i^2)m &amp;+&amp;
\frac{1}{N}(\sum_{i=1}^{N} x_i)b &amp;=&amp; \frac{1}{N}\sum_{i=1}^{N} x_i y_i
\\ \frac{1}{N}(\sum_{i=1}^{N} x_i)m &amp;+&amp; b &amp;=&amp;
\frac{1}{N}\sum_{i=1}^{N} y_{i} \\ \end{aligned} \]</span>{#eq:LS}</p>
<p>In these equations, notice that <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N} x_i\)</span> is the average (or mean) value of the <span class="math inline">\(x_i\)</span>. Let’s call this <span class="math inline">\(\overline{x}\)</span>. Similarly, <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N} y_{i}\)</span> is the mean of the <span class="math inline">\(y_i\)</span>, and we’ll call it <span class="math inline">\(\overline{y}\)</span>. If we further simplify the notation and write <span class="math inline">\(S_{xx}\)</span> for <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N} x_i^2\)</span> and <span class="math inline">\(S_{xy}\)</span> for <span class="math inline">\(\frac{1}{N}\sum_{i=1}^{N}x_iy_i\)</span> then we can write down a solution to this system using Cramer’s rule:</p>
<p><span class="math display">\[ \begin{aligned} m &amp;=
\frac{S_{xy}-\overline{x}\overline{y}}{S_{xx}-\overline{x}^2} \\ b &amp;=
\frac{S_{xx}\overline{y}-S_{xy}\overline{x}}{S_{xx}-\overline{x}^2} \\
\end{aligned} \]</span>{#eq:LSAnswer}</p>
<p>where we must have <span class="math inline">\(S_{xx}-\overline{x}^2\not=0\)</span>.</p>
<h3 id="sec:CalcExercises">Exercises</h3>
<ol type="1">
<li><p>Verify that +<span class="citation" data-cites="eq:LSAnswer">@eq:LSAnswer</span> is in fact the solution to the system in +<span class="citation" data-cites="eq:LS">@eq:LS</span>.</p></li>
<li><p>Suppose that <span class="math inline">\(S_{xx}-\overline{x}^2=0\)</span>. What does that mean about the <span class="math inline">\(x_i\)</span>? Does it make sense that the problem of finding the “line of best fit” fails in this case?</p></li>
</ol>
<h2 id="sec:LinAlg">Least Squares (via Geometry)</h2>
<p>In our discussion above, we thought about our data as consisting of <span class="math inline">\(N\)</span> pairs <span class="math inline">\((x_i,y_i)\)</span> corresponding to <span class="math inline">\(n\)</span> points in the <span class="math inline">\(xy\)</span>-plane <span class="math inline">\(\mathbf{R}^2\)</span>. Now let’s turn that picture “on its side”, and instead think of our data as consisting of <em>two</em> points in <span class="math inline">\(\mathbf{R}^{n}\)</span>:</p>
<p><span class="math display">\[ X=\left[\begin{matrix} x_1\cr x_2\cr \vdots\cr
x_n\end{matrix}\right] \mathrm{\ and\ } Y = \left[\begin{matrix}
y_1\cr y_2\cr \vdots\cr y_n\end{matrix}\right] \]</span></p>
<p>Let’s also introduce one other vector</p>
<p><span class="math display">\[ E = \left[\begin{matrix} 1 \cr 1 \cr \vdots \cr
1\end{matrix}\right].  \]</span></p>
<p>First, let’s assume that <span class="math inline">\(E\)</span> and <span class="math inline">\(X\)</span> are linearly independent. If not, then <span class="math inline">\(X\)</span> is a constant vector (why?) which we already know is a problem from +<span class="citation" data-cites="sec:Calculus">@sec:Calculus</span>, Exercise 2. Therefore <span class="math inline">\(E\)</span> and <span class="math inline">\(X\)</span> span a plane in <span class="math inline">\(\mathbf{R}^{n}\)</span>.</p>
<figure>
<img src="img/distance-to-plane.png" id="fig:perp" style="width:50.0%" alt="Distance to A Plane" /><figcaption aria-hidden="true">Distance to A Plane</figcaption>
</figure>
<p>Now if our data points <span class="math inline">\((x_i,y_i)\)</span> all <em>did</em> lie on a line <span class="math inline">\(y=mx+b\)</span>, then the three vectors <span class="math inline">\(X\)</span>, <span class="math inline">\(Y\)</span>, and <span class="math inline">\(E\)</span> would be linearly dependent:</p>
<p><span class="math display">\[ Y = mX + bE.  \]</span></p>
<p>Since our data is only approximately linear, that’s not the case. So instead we look for an approximate solution. One way to phrase that is to ask:</p>
<p><em>What is the point <span class="math inline">\(\hat{Y}\)</span> in the plane <span class="math inline">\(H\)</span> spanned by <span class="math inline">\(X\)</span> and <span class="math inline">\(E\)</span> in <span class="math inline">\(\mathbf{R}^{n}\)</span> which is closest to <span class="math inline">\(Y\)</span>?</em></p>
<p>If we knew this point <span class="math inline">\(\hat{Y}\)</span>, then since it lies in <span class="math inline">\(H\)</span> we would have <span class="math inline">\(\hat{Y}=mX+bE\)</span> and the coefficients <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> would be a candidate for defining a line of best fit <span class="math inline">\(y=mx+b\)</span>. Finding the point in a plane closest to another point in <span class="math inline">\(\mathbf{R}^{n}\)</span> is a geometry problem that we can solve.</p>
<p><strong>Proposition:</strong> The point <span class="math inline">\(\hat{Y}\)</span> in the plane spanned by <span class="math inline">\(X\)</span> and <span class="math inline">\(E\)</span> is the point such that the vector <span class="math inline">\(Y-\hat{Y}\)</span> is perpendicular to <span class="math inline">\(H\)</span>.</p>
<p><strong>Proof:</strong> See +<span class="citation" data-cites="fig:perp">@fig:perp</span> for an illustration – perhaps you are already convinced by this, but let’s be careful. <span class="math inline">\(\hat{Y}=mX+bE\)</span> such that <span class="math display">\[ D = \|Y-\hat{Y}\|^2 = \|Y-mX-bE\|^2 \]</span> is minimal. Using some vector calculus, we have <span class="math display">\[ \frac{\partial D}{\partial m} =
\frac{\partial}{\partial m} (Y-mX-bE)\cdot (Y-mX-bE) =
-2(Y-mX-bE)\cdot X \]</span> and <span class="math display">\[ \frac{\partial D}{\partial b} =
\frac{\partial}{\partial b} (Y-mX-bE)\cdot (Y-mX-bE) =
-2(Y-mX-bE)\cdot E.  \]</span></p>
<p>So both derivatives are zero exactly when <span class="math inline">\(\hat{Y}=(Y-mX-bE)\)</span> is orthogonal to both <span class="math inline">\(X\)</span> and <span class="math inline">\(E\)</span>, and therefore every vector in <span class="math inline">\(H\)</span>.</p>
<p>We also obtain equations for <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> just as in our first look at this problem.</p>
<p><span class="math display">\[ \begin{aligned} m(X\cdot E) &amp;+ b(E\cdot E) &amp;= (Y\cdot E) \cr
m(X\cdot X) &amp;+ b(E\cdot X) &amp;= (Y\cdot X) \cr \end{aligned}
\]</span>{#eq:LSAnswer2}</p>
<p>We leave it is an exercise below to check that these are the same equations that we obtained in +<span class="citation" data-cites="eq:LSAnswer">@eq:LSAnswer</span>.</p>
<h3 id="exercises">Exercises</h3>
<ol type="1">
<li>Verify that +<span class="citation" data-cites="eq:LSAnswer">@eq:LSAnswer</span> and +<span class="citation" data-cites="eq:LSAnswer2">@eq:LSAnswer2</span> are equivalent.</li>
</ol>
<h2 id="sec:Multivariate-calculus">The Multivariate Case (Calculus)</h2>
<p>Having worked through the problem of finding a “line of best fit” from two points of view, let’s look at a more general problem. We looked above at a scatterplot showing the relationship between gas mileage and engine size (displacement). There are other factors that might contribute to gas mileage that we want to consider as well – for example: - a car that is heavy compared to its engine size may get worse mileage - a sports car with a drive train that gives fast acceleration as compared to a car with a transmission designed for long trips may have different mileage for the same engine size.</p>
<p>Suppose we wish to use engine displacement, vehicle weight, and acceleration all together to predict mileage. Instead of looking points <span class="math inline">\((x_i,y_i)\)</span> where <span class="math inline">\(x_i\)</span> is the displacement of the <span class="math inline">\(i^{th}\)</span> car model and we try to predict a value <span class="math inline">\(y\)</span> from a corresponding <span class="math inline">\(x\)</span> as <span class="math inline">\(y=mx+b\)</span> – let’s look at a situation in which our measured value <span class="math inline">\(y\)</span> depends on multiple variables – say displacement <span class="math inline">\(d\)</span>, weight <span class="math inline">\(w\)</span>, and acceleration <span class="math inline">\(a\)</span> with <span class="math inline">\(k=3\)</span> – and we are trying to find the best linear equation</p>
<p><span class="math display">\[ y=m_1 d + m_2 w + m_3 a +b \]</span>{#eq:multivariate}</p>
<p>But to handle this situation more generally we need to adopt a convention that will allow us to use indexed variables instead of <span class="math inline">\(d\)</span>, <span class="math inline">\(w\)</span>, and <span class="math inline">\(a\)</span>. We will use the <em>tidy</em> data convention.</p>
<p><strong>Tidy Data:</strong> A dataset is tidy if it consists of values <span class="math inline">\(x_{ij}\)</span> for <span class="math inline">\(i=1,\ldots,N\)</span> and <span class="math inline">\(j=1,\ldots, k\)</span> so that:</p>
<ul>
<li>the row index corresponds to a <em>sample</em> – a set of measurements from a single event or item;</li>
<li>the column index corresponds to a <em>feature</em> – a particular property measured for all of the events or items.</li>
</ul>
<p>In our case,</p>
<ul>
<li>the <em>samples</em> are the different types of car models,</li>
<li>the <em>features</em> are the properties of those car models.</li>
</ul>
<p>For us, <span class="math inline">\(N\)</span> is the number of different types of cars, and <span class="math inline">\(k\)</span> is the number of properties we are considering. Since we are looking at displacement, weight, and acceleration, we have <span class="math inline">\(k=3\)</span>.</p>
<p>So the “independent variables” for a set of data that consists of <span class="math inline">\(N\)</span> samples, and <span class="math inline">\(k\)</span> measurements for each sample, can be represented by a <span class="math inline">\(N\times k\)</span> matrix</p>
<p><span class="math display">\[ X = \left(\begin{matrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k} \\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k} \\ \vdots &amp; \vdots &amp; \ddots &amp; \vdots
\\ x_{N1} &amp; x_{k2} &amp; \cdots &amp; x_{Nk} \\ \end{matrix}\right) \]</span></p>
<p>and the measured dependent variables <span class="math inline">\(Y\)</span> are a column vector <span class="math display">\[ Y =
\left[\begin{matrix} y_1 \\ y_2 \\ \vdots \\ y_N\end{matrix}\right].
\]</span></p>
<p>If <span class="math inline">\(m_1,\ldots, m_k\)</span> are “slopes” associated with these properties in +<span class="citation" data-cites="eq:multivariate">@eq:multivariate</span>, and <span class="math inline">\(b\)</span> is the “intercept”, then the predicted value <span class="math inline">\(\hat{Y}\)</span> is given by a matrix equation</p>
<p><span class="math display">\[ \hat{Y} = X\left[\begin{matrix} m_1 \\ m_2 \\ \cdots \\
m_k\end{matrix}\right]+\left[\begin{matrix} 1 \\ 1 \\ \cdots \\
1\end{matrix}\right]b \]</span></p>
<p>and our goal is to choose these parameters <span class="math inline">\(m_i\)</span> and <span class="math inline">\(b\)</span> to make the mean squared error:</p>
<p><span class="math display">\[ MSE(m_1,\ldots, m_k,b) = \|Y-\hat{Y}\|^2 = \sum_{i=1}^{N} (y_i -
\sum_{j=1}^{k} x_{ij}m_j -b )^2.  \]</span></p>
<p>Here we are summing over the <span class="math inline">\(N\)</span> different car models, and for each model taking the squared difference between the true mileage <span class="math inline">\(y_i\)</span> and the “predicted” mileage <span class="math inline">\(\sum_{j=1}^{k} x_{ij}m_j +b\)</span>. We wish to minimize this MSE.</p>
<p>Let’s make one more simplification. The intercept variable <span class="math inline">\(b\)</span> is annoying because it requires separate treatment from the <span class="math inline">\(m_i\)</span>. But we can use a trick to eliminate the need for special treatment. Let’s add a new feature to our data matrix (a new column) that has the constant value <span class="math inline">\(1\)</span>.</p>
<p><span class="math display">\[ X = \left(\begin{matrix} x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1k} &amp; 1\\
x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2k} &amp; 1\\ \vdots &amp; \vdots &amp; \ddots &amp;
\vdots &amp; 1\\ x_{N1} &amp; x_{k2} &amp; \cdots &amp; x_{Nk} &amp; 1\\
\end{matrix}\right) \]</span></p>
<p>Now our data matrix <span class="math inline">\(X\)</span> is <span class="math inline">\(N\times(k+1)\)</span> and we can put our “intercept” <span class="math inline">\(b=m_{k+1}\)</span> into our vector of “slopes” <span class="math inline">\(m_1, \ldots, m_k,m_{k+1}\)</span>:</p>
<p><span class="math display">\[ \hat{Y} = X\left[\begin{matrix} m_1 \\ m_2 \\ \cdots \\ m_k \\
m_{k+1}\end{matrix}\right].  \]</span></p>
<p>and our MSE becomes</p>
<p><span class="math display">\[ MSE(M) = \|Y - XM\|^2 \]</span></p>
<p>where</p>
<p><span class="math display">\[ M=\left[\begin{matrix} m_1 \\ m_2 \\ \cdots \\ m_k \\
m_{k+1}\end{matrix}\right].  \]</span></p>
<p><strong>Remark:</strong> Later on (see {<span class="citation" data-cites="sec:centered">@sec:centered</span>}) we will see that if we “center” our features about their mean, by subtracting the average value of each column of <span class="math inline">\(X\)</span> from that column; and we also subtract the average value of <span class="math inline">\(Y\)</span> from the entries of <span class="math inline">\(Y\)</span>, then the <span class="math inline">\(b\)</span> that emerges from the least squares fit is zero. As a result, instead of adding a column of <span class="math inline">\(1\)</span>’s, you can change coordinates to center each feature about its mean, and keep your <span class="math inline">\(X\)</span> matrix <span class="math inline">\(N\times k\)</span>.</p>
<p>The Calculus approach to minimizing the <span class="math inline">\(MSE\)</span> is to take its partial derivatives with respect to the <span class="math inline">\(m_{i}\)</span> and set them to zero. Let’s first work out the derivatives in a nice form for later.</p>
<p><strong>Proposition:</strong> The gradient of <span class="math inline">\(MSE(M)=E\)</span> is given by</p>
<p><span class="math display">\[ \nabla E = \left[\begin{matrix} \df{M_1}E \\ \df{M_2}E \\ \vdots \\
\df{m_{M+1}}E\end{matrix}\right] = -2 X^{\intercal}Y + 2
X^{\intercal}XM \]</span>{#eq:gradient}</p>
<p>where <span class="math inline">\(X^{\intercal}\)</span> is the transpose of <span class="math inline">\(X\)</span>.</p>
<p><strong>Proof:</strong> First, remember that the <span class="math inline">\(ij\)</span> entry of <span class="math inline">\(X^{\intercal}\)</span> is the <span class="math inline">\(ji\)</span> entry of <span class="math inline">\(X\)</span>. Also, we will use the notation <span class="math inline">\(X[j,:]\)</span> to mean the <span class="math inline">\(j^{th}\)</span> row of <span class="math inline">\(X\)</span> and <span class="math inline">\(X[:,i]\)</span> to mean the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(X\)</span>. (This is copied from the Python programming language; the ‘:’ means that index runs over all possibilities).</p>
<p>Since <span class="math display">\[ E = \sum_{j=1}^{N} (Y_j-\sum_{s=1}^{k+1} X_{js}M_{s})^2 \]</span> we compute: <span class="math display">\[\begin{aligned} \df{M_t}E &amp;= -2\sum_{j=1}^{N}
X_{jt}(Y_{j}-\sum_{s=1}^{k+1} X_{js}M_{s}) \\ &amp;= -2(\sum_{j=1}^{N}
Y_{j}X_{jt} - \sum_{j=1}^{N}\sum_{s=1}^{k+1} X_{jt}X_{js}M_{s}) \\ &amp;=
-2(\sum_{j=1}^{N} X^{\intercal}_{tj}Y_{j}
-\sum_{j=1}^{N}\sum_{s=1}^{k+1} X^{\intercal}_{tj}X_{js}M_{s}) \\ &amp;=
-2(X^{\intercal}[t,:]Y - \sum_{s=1}^{k+1}\sum_{j=1}^{N}
X^{\intercal}_{tj}X_{js}M_{s}) \\ &amp;= -2(X^{\intercal}[t,:]Y -
\sum_{s=1}^{k+1} (X^{\intercal}X)_{ts}M_{s}) \\ &amp;=
-2(X^{\intercal}[t,:]Y - (X^{\intercal}X)[t,:]M)\\
\end{aligned}\]</span>{#eq:gradient2}</p>
<p>Stacking up the different rows to make <span class="math inline">\(E\)</span> yields the desired formula.</p>
<p><strong>Proposition:</strong> Assume that <span class="math inline">\(D=X^{\intercal}X\)</span> is invertible (notice that it is a <span class="math inline">\((k+1)\times(k+1)\)</span> square matrix so this makes sense). The solution <span class="math inline">\(M\)</span> to the multivariate least squares problem is <span class="math display">\[ M =
D^{-1}X^{\intercal}Y \]</span>{#eq:Msolution} and the “predicted value” <span class="math inline">\(\hat{Y}\)</span> for <span class="math inline">\(Y\)</span> is <span class="math display">\[ \hat{Y} = XD^{-1}X^{\intercal}Y.
\]</span>{#eq:projection}</p>
<h2 id="the-multivariate-case-geometry">The Multivariate Case (Geometry)</h2>
<p>Let’s look more closely at the equation obtained by setting the gradient of the error, +<span class="citation" data-cites="eq:gradient">@eq:gradient</span>, to zero. Remember that <span class="math inline">\(M\)</span> is the unknown vector in this equation, everything else is known:</p>
<p><span class="math display">\[ X^{\intercal}Y = X^{\intercal}XM \]</span></p>
<p>Here is how to think about this:</p>
<ol type="1">
<li><p>As <span class="math inline">\(M\)</span> varies, the <span class="math inline">\(N\times 1\)</span> matrix <span class="math inline">\(XM\)</span> varies over the space spanned by the columns of the matrix <span class="math inline">\(X\)</span>. So as <span class="math inline">\(M\)</span> varies <span class="math inline">\(XM\)</span> is a general element of the subspace <span class="math inline">\(H\)</span> of <span class="math inline">\(R^{N}\)</span> spanned by the <span class="math inline">\(k+1\)</span> columns of <span class="math inline">\(X\)</span>.</p></li>
<li><p>The product <span class="math inline">\(X^{\intercal}XM\)</span> is a <span class="math inline">\((k+1)\times 1\)</span> matrix. Each entry is the dot product of the general element of <span class="math inline">\(H\)</span> with one of the <span class="math inline">\(k+1\)</span> basis vectors of <span class="math inline">\(H\)</span>.</p></li>
<li><p>The product <span class="math inline">\(X^{\intercal}Y\)</span> is a <span class="math inline">\((k+1)\times 1\)</span> matrix whose entries are the dot product of the basis vectors of <span class="math inline">\(H\)</span> with <span class="math inline">\(Y\)</span>.</p></li>
</ol>
<p>Therefore, this equation asks for us to find <span class="math inline">\(M\)</span> so that the vector <span class="math inline">\(XM\)</span> in <span class="math inline">\(H\)</span> has the same dot products with the basis vectors of <span class="math inline">\(H\)</span> as <span class="math inline">\(Y\)</span> does. The condition</p>
<p><span class="math display">\[ X^{\intercal}\cdot (Y-XM)=0 \]</span></p>
<p>says that <span class="math inline">\(Y-XM\)</span> is orthogonal to <span class="math inline">\(H\)</span>. This argument establishes the following proposition.</p>
<p><strong>Proposition:</strong> Just as in the simple one-dimensional case, the predicted value <span class="math inline">\(\hat{Y}\)</span> of the least squares problem is the point in <span class="math inline">\(H\)</span> closest to <span class="math inline">\(Y\)</span> – or in other words the point <span class="math inline">\(\hat{Y}\)</span> in <span class="math inline">\(H\)</span> such that <span class="math inline">\(Y-\hat{Y}\)</span> is perpendicular to <span class="math inline">\(H\)</span>.</p>
<h3 id="orthogonal-projection">Orthogonal Projection</h3>
<p>Recall that we introduced the notation <span class="math inline">\(D=X^{\intercal}X\)</span>, and let’s assume, for now, that <span class="math inline">\(D\)</span> is an invertible matrix. We have the formula (see +<span class="citation" data-cites="eq:projection">@eq:projection</span>): <span class="math display">\[ \hat{Y} = XD^{-1}X^{\intercal}Y.  \]</span> <strong>Proposition:</strong> The matrix <span class="math inline">\(P=XD^{-1}X^{\intercal}\)</span> is an <span class="math inline">\(N\times N\)</span> matrix called the orthogonal projection operator onto the subspace <span class="math inline">\(H\)</span> spanned by the columns of <span class="math inline">\(X\)</span>. It has the following properties:</p>
<ul>
<li><span class="math inline">\(PY\)</span> belongs to the subspace <span class="math inline">\(H\)</span> for any <span class="math inline">\(Y\in\mathbf{R}^{N}\)</span>.</li>
<li><span class="math inline">\((Y-PY)\)</span> is orthogonal to <span class="math inline">\(H\)</span>.</li>
<li><span class="math inline">\(P*P = P\)</span>.</li>
</ul>
<p><strong>Proof:</strong> First of all, <span class="math inline">\(PY=XD^{-1}X^{\intercal}Y\)</span> so <span class="math inline">\(PY\)</span> is a linear combination of the columns of <span class="math inline">\(X\)</span> and is therefore an element of <span class="math inline">\(H\)</span>. Next, we can compute the dot product of <span class="math inline">\(PY\)</span> against a basis of <span class="math inline">\(H\)</span> by computing</p>
<p><span class="math display">\[ X^{\intercal}PY = X^{\intercal}XD^{-1}X^{\intercal}Y =
X^{\intercal}Y \]</span></p>
<p>since <span class="math inline">\(X^{\intercal}X=D\)</span>. This equation means that <span class="math inline">\(X^{\intercal}(Y-PY)=0\)</span> which tells us that <span class="math inline">\(Y-PY\)</span> has dot product zero with a basis for <span class="math inline">\(H\)</span>. Finally,</p>
<p><span class="math display">\[ PP = XD^{-1}X^{\intercal}XD^{-1}X^{\intercal} =
XD^{-1}X^{\intercal}=P.  \]</span></p>
<p>It should be clear from the above discussion that the matrix <span class="math inline">\(D=X^{\intercal}X\)</span> plays an important role in the study of this problem. In particular it must be invertible or our analysis above breaks down. In the next section we will look more closely at this matrix and what information it encodes about our data.</p>
<h2 id="sec:centered">Centered coordinates</h2>
<p>Recall from last section that the matrix <span class="math inline">\(D=X^{\intercal}X\)</span> is of central importance to the study of the multivariate least squares problem. Let’s look at it more closely.</p>
<p><strong>Lemma:</strong> The <span class="math inline">\(i,j\)</span> entry of <span class="math inline">\(D\)</span> is the dot product <span class="math display">\[
D_{ij}=X[:,i]\cdot X[:,j] \]</span> of the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> columns of <span class="math inline">\(X\)</span>.</p>
<p><strong>Proof:</strong> In the matrix multiplication <span class="math inline">\(X^{\intercal}X\)</span>, the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(X^{\intercal}\)</span> gets “dotted” with the <span class="math inline">\(j^{th}\)</span> column of <span class="math inline">\(X\)</span> to product the <span class="math inline">\(i,j\)</span> entry. But the <span class="math inline">\(i^{th}\)</span> row of <span class="math inline">\(X^{\intercal}\)</span> is the <span class="math inline">\(i^{th}\)</span> column of <span class="math inline">\(X\)</span>, as asserted in the statement of the lemma.</p>
<p>A crucial point in our construction above relied on the matrix <span class="math inline">\(D\)</span> being invertible. The following Lemma shows that <span class="math inline">\(D\)</span> fails to be invertible only when the different features (the columns of <span class="math inline">\(X\)</span>) are linearly dependent.</p>
<p><strong>Lemma:</strong> <span class="math inline">\(D\)</span> is not invertible if and only if the columns of <span class="math inline">\(X\)</span> are linearly dependent.</p>
<p><strong>Proof:</strong> If the columns of <span class="math inline">\(X\)</span> are linearly dependent, then there is a nonzero vector <span class="math inline">\(m\)</span> so that <span class="math inline">\(Xm=0\)</span>. In that case clearly <span class="math inline">\(Dm=X^{\intercal}Xm=0\)</span> so <span class="math inline">\(D\)</span> is not invertible. Suppose <span class="math inline">\(D\)</span> is not invertible. Then there is a nonzero vector <span class="math inline">\(m\)</span> with <span class="math inline">\(Dm=X^{\intercal}Xm=0\)</span>. This means that the vector <span class="math inline">\(Xm\)</span> is orthogonal to all of the columns of <span class="math inline">\(X\)</span>. Since <span class="math inline">\(Xm\)</span> belongs to the span <span class="math inline">\(H\)</span> of the columns of <span class="math inline">\(X\)</span>, if it is orthogonal to <span class="math inline">\(H\)</span> it must be zero.</p>
<p>In fact, the matrix <span class="math inline">\(D\)</span> captures some important statistical measures of our data, but to see this clearly we need to make a slight change of basis. First recall that <span class="math inline">\(X[:,k+1]\)</span> is our column of all <span class="math inline">\(1\)</span>, added to handle the intercept. As a result, the dot product <span class="math inline">\(X[:,i]\cdot X[:,k+1]\)</span> is the sum of the entries in the <span class="math inline">\(i^{th}\)</span> column, and so if we let <span class="math inline">\(\mu_{i}\)</span> denote the average value of the entries in column <span class="math inline">\(i\)</span>, we have <span class="math display">\[ \mu_{i} = \frac{1}{N}(X[:,i]\cdot
X[:,k+1]) \]</span></p>
<p>Now change the matrix <span class="math inline">\(X\)</span> by elementary column operations to obtain a new data matrix <span class="math inline">\(X_{0}\)</span> by setting <span class="math display">\[ X_{0}[:,i] =
X[:,i]-\frac{1}{N}(X[:,i]\cdot X[:,k+1])X[:,k+1] =
X[:,i]-\mu_{i}X[:,k+1] \]</span> for <span class="math inline">\(i=1,\ldots, k\)</span>.</p>
<p>In terms of the original data, we are changing the measurement scale of the data so that each feature has average value zero, and the subspace <span class="math inline">\(H\)</span> spanned by the columns of <span class="math inline">\(X_{0}\)</span> is the same as that spanned by the columns of <span class="math inline">\(X\)</span>. Using <span class="math inline">\(X_{0}\)</span> instead of <span class="math inline">\(X\)</span> for our least squares problem, we get</p>
<p><span class="math display">\[ \hat{Y} = X_{0}D_{0}^{-1}X_{0}^{\intercal}Y \]</span></p>
<p>and</p>
<p><span class="math display">\[ M_{0} = D_{0}^{-1}X_{0}^{\intercal}Y \]</span></p>
<p>where <span class="math inline">\(D_{0}=X_{0}^{\intercal}X_{0}.\)</span></p>
<p><strong>Proposition:</strong> The matrix <span class="math inline">\(D_{0}\)</span> has a block form. Its upper left block is a <span class="math inline">\(k\times k\)</span> symmetric block with entries <span class="math display">\[ (D_{0})_{ij} =
(X[:,i]-\mu_{i}X[:,k+1])\cdot(X[:,j]-\mu_{j}X[:,k+1]) \]</span> Its <span class="math inline">\((k+1)^{st}\)</span> row and column are all zero, except for the <span class="math inline">\((k+1),(k+1)\)</span> entry, which is <span class="math inline">\(N\)</span>.</p>
<p><strong>Proof:</strong> This follows from the fact that the last row and column entries are (for <span class="math inline">\(i\not=k+1\)</span>): <span class="math display">\[ (X[:,i]-\mu_{i}X[:,k+1])\cdot
X[:,k+1] = (X[:,i]\cdot X[:,k+1])-N\mu_{i} = 0 \]</span> and for <span class="math inline">\(i=k+1\)</span> we have <span class="math inline">\(X[:,k+1]\cdot X[:,k+1]=N\)</span> since that column is just <span class="math inline">\(N\)</span> <span class="math inline">\(1\)</span>’s.</p>
<p><strong>Proposition:</strong> If the <span class="math inline">\(x\)</span> coordinates (the features) are centered so that they have mean zero, then the intercept <span class="math inline">\(b\)</span> is <span class="math display">\[ \overline{Y} =
\frac{1}{N}\sum y_{i}.  \]</span></p>
<p><strong>Proof:</strong> By centering the coordinates, we replace the matrix <span class="math inline">\(X\)</span> by <span class="math inline">\(X_{0}\)</span> and <span class="math inline">\(D\)</span> by <span class="math inline">\(D_{0}\)</span>. and we are trying to minimize <span class="math inline">\(\|Y-X_{0}M_{0}\|^2\)</span>. Use the formula from +<span class="citation" data-cites="eq:Msolution">@eq:Msolution</span> to see that <span class="math display">\[ M_{0} = D_{0}^{-1}X_{0}^{\intercal}Y.  \]</span> The <span class="math inline">\(b\)</span> value we are interested in is the last entry <span class="math inline">\(m_{k+1}\)</span> in <span class="math inline">\(M_{0}\)</span>. From the block form of <span class="math inline">\(D_{0}\)</span>, we know that <span class="math inline">\(D_{0}^{-1}\)</span> has bottom row and last column zero except for <span class="math inline">\(1/N\)</span> in position <span class="math inline">\((k+1)\times(k+1)\)</span>. Also <span class="math inline">\(X_{0}^{\intercal}\)</span> has last row consisting entirely of <span class="math inline">\(1\)</span>. So the bottom entry of <span class="math inline">\(X_{0}^{\intercal}Y\)</span> is <span class="math inline">\(\sum_{i=1}^{N} y_{i}\)</span>, and the bottom entry <span class="math inline">\(b\)</span> of <span class="math inline">\(D_{0}^{-1}X_{0}^{\intercal}Y\)</span> is <span class="math display">\[ \mu_{Y} =
\frac{1}{N}\sum_{i=1}^{N} y_{i}.  \]</span> as claimed.</p>
<p><strong>Corollary:</strong> If we make a further change of coordinates to define <span class="math display">\[
Y_{0} = Y - \mu_{Y}\left[\begin{matrix} 1 \\ 1 \\ \vdots \\
1\end{matrix}\right] \]</span> then the associated <span class="math inline">\(b\)</span> is zero. As a result we can forget about the extra column of <span class="math inline">\(1&#39;s\)</span> that we added to <span class="math inline">\(X\)</span> to account for it and reduce the dimension of our entire problem by <span class="math inline">\(1\)</span>.</p>
<p>Just to recap, if we center our data so that <span class="math inline">\(\mu_{Y}=0\)</span> and <span class="math inline">\(\mu_{i}=0\)</span> for <span class="math inline">\(i=1,\ldots, k\)</span>, then the least squares problem reduces to minimizing <span class="math display">\[ E(M) = \|Y-XM\|^2 \]</span> where <span class="math inline">\(X\)</span> is the <span class="math inline">\(N\times k\)</span> matrix with <span class="math inline">\(j^{th}\)</span> row <span class="math inline">\((x_{j1},x_{j2},\ldots, x_{jk})\)</span> for <span class="math inline">\(j=1,\ldots, N\)</span> and the solutions are as given in +<span class="citation" data-cites="eq:Msolution">@eq:Msolution</span> and +<span class="citation" data-cites="eq:projection">@eq:projection</span>.</p>
<h2 id="caveats-about-linear-regression">Caveats about Linear Regression</h2>
<h3 id="basic-considerations">Basic considerations</h3>
<p>Reflecting on our long discussion up to this point, we should take note of some of the potential pitfalls that lurk in the use of linear regression.</p>
<ol type="1">
<li><p>When we apply linear regression, we are explicitly assuming that the variable <span class="math inline">\(Y\)</span> is associated to <span class="math inline">\(X\)</span> via linear equations. This is a big assumption!</p></li>
<li><p>When we use multilinear regression, we are assuming that changes in the different features have independent effects on the target variable <span class="math inline">\(y\)</span>. In other words, suppose that <span class="math inline">\(y=ax_1+bx_2\)</span>. Then an increase of <span class="math inline">\(x_1\)</span> by <span class="math inline">\(1\)</span> increases <span class="math inline">\(y\)</span> by <span class="math inline">\(a\)</span>, and an increase of <span class="math inline">\(x_2\)</span> by <span class="math inline">\(1\)</span> increases <span class="math inline">\(y\)</span> by <span class="math inline">\(b\)</span>. These effects are independent of one another and combine to yield an increase of <span class="math inline">\(a+b\)</span>.</p></li>
<li><p>We showed in our discussion above that linear regression problem has a solution when the matrix <span class="math inline">\(D=X^{\intercal}X\)</span> is invertible, and this happens when the columns of <span class="math inline">\(D\)</span> are linearly independent. When working with real data, which is messy, we could have a situation in which the features we are studying are, in fact, dependent – but because of measurement error, the samples that we collected aren’t. In this case, the matrix <span class="math inline">\(D\)</span> will be “close” to being non-invertible, although formally still invertible. In this case, computing <span class="math inline">\(D^{-1}\)</span> leads to numerical instability and the solution we obtain is very unreliable.</p></li>
</ol>
<h3 id="simpsons-effect">Simpson’s Effect</h3>
<p>Simpson’s effect is a famous phenomenon that illustrates that linear regression can be very misleading in some circumstances. It is often a product of “pooling” results from multiple experiments. Suppose, for example, that we are studying the relationship between a certain measure of blood chemistry and an individual’s weight gain or less on a particular diet. We do our experiments in three labs, the blue, green, and red labs. Each lab obtains similar results – higher levels of the blood marker correspond to greater weight gain, with a regression line of slope around 1. However, because of differences in the population that each lab is studying, some populations are more susceptible to weight gain and so the red lab sees a mean increase of almost 9 lbs while the blue lab sees a weight gain of only 3 lbs on average.</p>
<p>The three groups of scientists pool their results to get a larger sample size and do a new regression. Surprise! Now the regression line has slope <span class="math inline">\(-1.6\)</span> and increasing amounts of the marker seem to lead to <em>less</em> weight gain!</p>
<p>This is called Simpson’s effect, or Simpson’s paradox, and it shows that unknown factors (confounding factors) may cause linear regression to yield misleading results. This is particularly true when data from experiments conducted under different conditions is combined; in this case, the differences in experimental setting, called <em>batch effects</em>, can throw off the analysis very dramatically. See +<span class="citation" data-cites="fig:simpsons">@fig:simpsons</span> .</p>
<figure>
<img src="img/SimpsonsEffect.png" id="fig:simpsons" style="width:50.0%" alt="Simpson’s Effect" /><figcaption aria-hidden="true">Simpson’s Effect</figcaption>
</figure>
<h3 id="exercises-1">Exercises</h3>
<ol type="1">
<li>When proving that <span class="math inline">\(D\)</span> is invertible if and only if the columns of <span class="math inline">\(X\)</span> are linearly independent, we argued that if <span class="math inline">\(X^{\intercal}Xm=0\)</span> for a nonzero vector <span class="math inline">\(m\)</span>, then <span class="math inline">\(Xm\)</span> is orthogonal to the span of the columns of <span class="math inline">\(X\)</span>, and is also an element of that span, and is therefore zero. Provide the details: show that if <span class="math inline">\(H\)</span> is a subspace of <span class="math inline">\(\mathbf{R}^{N}\)</span>, and <span class="math inline">\(x\)</span> is a vector in <span class="math inline">\(H\)</span> such that <span class="math inline">\(x\cdot h=0\)</span> for all <span class="math inline">\(h\in H\)</span>, then <span class="math inline">\(x=0\)</span>. </li>
</ol>
<h1 id="the-naive-bayes-classification-method">The Naive Bayes classification method</h1>
<h2 id="introduction">Introduction</h2>
<p>In our discussion of Bayes Theorem, we looked at a situation in which we had a population consisting of people infected with COVID-19 and people not infected, and we had a test that we could apply to determine which class an individual belonged to. Because our test was not 100 percent reliable, a positive test result didn’t guarantee that a person was infected, and we used Bayes Theorem to evaluate how to interpret the positive test result. More specifically, our information about the test performance gave us the the conditional probabilities of positive and negative test results given infection status – so for example we were given <span class="math inline">\(P(+|\mathrm{infected})\)</span>, the chance of getting a positive test assuming the person is infected – and we used Bayes Theorem to determine <span class="math inline">\(P(\mathrm{infected}|+)\)</span>, the chance that a person was infected given a positive test result.</p>
<p>The Naive Bayes classification method is a generalization of this idea. We have data that belongs to one of two classes, and based on the results of a series of tests, we wish to decide which class a particular data point belongs to. For one example, we are given a collection of product reviews from a website and we wish to classify those reviews as either “positive” or “negative.” This type of problem is called “sentiment analysis.” For another, related example, we have a collection of emails or text messages and we wish to label those that are likely “spam” emails. In both of these examples, the “test” that we will apply is to look for the appearance or absence of certain key words that make the text more or less likely to belong to a certain class. For example, we might find that a movie review that contains the word “great” is more likely to be positive than negative, while a review that contains the word “boring” is more likely to be negative.</p>
<p>The reason for the word “naive” in the name of this method is that we will derive our probabilities from empirical data, rather than from any deeper theory. For example, to find the probability that a negative movie review contains the word “boring”, we will look at a bunch of reviews that our experts have said are negative, and compute the proportion of those that contain the word boring. Indeed, to develop our family of tests, we will rely on a training set of already classified data from which we can determine estimates of probabilities that we need.</p>
<h2 id="an-example-dataset">An example dataset</h2>
<p>To illustrate the Naive Bayes algorithm, we will work with the “Sentiment Labelled Sentences Data Set” (<span class="citation" data-cites="sentences">@sentences</span>). This dataset contains 3 files, each containing 1000 documents labelled <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> for “negative” or “positive” sentiment. There are 500 of each type of document in each file. One file contains reviews of products from amazon.com; one contains yelp restaurant reviews, and one contains movie reviews from imdb.com.</p>
<p>Let’s focus on the amazon reviews data. Here are some samples:</p>
<pre><code>So there is no way for me to plug it in here in the US unless I go by a converter.  0
Good case, Excellent value. 1
Great for the jawbone.  1
Tied to charger for conversations lasting more than 45 minutes.MAJOR PROBLEMS!! 0
The mic is great.   1
I have to jiggle the plug to get it to line up right to get decent volume.  0
If you have several dozen or several hundred contacts, then imagine the fun of sending each of them one by one. 0
If you are Razr owner...you must have this! 1
Needless to say, I wasted my money. 0
What a waste of money and time!.    0</code></pre>
<p>As you can see, each line consists of a product review followed by a <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>; in this file the review is separated from the text by a tab character.</p>
<h2 id="bernoulli-tests">Bernoulli tests</h2>
<p>We will describe the “Bernoulli” version of a Naive Bayes classifier for this data. The building block of this method is a test based on a single word. For example, let’s consider the word <strong>great</strong> among all of our amazon reviews. It turns out that <strong>great</strong> occurs <span class="math inline">\(5\)</span> times in negative reviews and <span class="math inline">\(92\)</span> times in positive reviews among our <span class="math inline">\(1000\)</span> examples. So it seems that seeing the word <strong>great</strong> in a review makes it more likely to be positive. The appearances of great are summarized in +<span class="citation" data-cites="tbl:great">@tbl:great</span> . We write ~<strong>great</strong> for the case where <strong>great</strong> does <em>not</em> appear.</p>
<table>
<caption>Ocurrences of <strong>great</strong> by type of review {#tbl:great}.</caption>
<thead>
<tr class="header">
<th></th>
<th>+</th>
<th>-</th>
<th>total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>great</strong></td>
<td>92</td>
<td>5</td>
<td>97</td>
</tr>
<tr class="even">
<td>~<strong>great</strong></td>
<td>408</td>
<td>495</td>
<td>903</td>
</tr>
<tr class="odd">
<td>total</td>
<td>500</td>
<td>500</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>In this data, positive and negative reviews are equally likely so <span class="math inline">\(P(+)=P(-)=.5\)</span> From this table, and Bayes Theorem, we obtain the empirical probabilities (or “naive” probabilities).</p>
<p><span class="math display">\[
P(\mathbf{great} | +) = \frac{92}{500} = .184
\]</span></p>
<p>and</p>
<p><span class="math display">\[
P(\mathbf{great}) = \frac{97}{1000} = .097
\]</span></p>
<p>Therefore</p>
<p><span class="math display">\[
P(+|\mathbf{great}) = \frac{.184}{.097}(.5) = 0.948.
\]</span></p>
<p>In other words, <em>if</em> you see the word <strong>great</strong> in a review, there’s a 95% chance that the review is positive.</p>
<p>What if you <em>do not</em> see the word <strong>great</strong>? A similar calculation from the table yields</p>
<p><span class="math display">\[
P(+|\sim\mathbf{great}) = \frac{408}{903} = .452
\]</span></p>
<p>In other words, <em>not</em> seeing the word great gives a little evidence that the review is negative (there’s a 55% chance it’s negative) but it’s not that conclusive.</p>
<p>The word <strong>waste</strong> is associated with negative reviews. It’s statistics are summarized in +<span class="citation" data-cites="tbl:waste">@tbl:waste</span>.</p>
<table>
<caption>Ocurrences of <strong>waste</strong> by type of review {#tbl:waste}.</caption>
<thead>
<tr class="header">
<th></th>
<th>+</th>
<th>-</th>
<th>total</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><strong>waste</strong></td>
<td>0</td>
<td>14</td>
<td>14</td>
</tr>
<tr class="even">
<td>~<strong>waste</strong></td>
<td>500</td>
<td>486</td>
<td>986</td>
</tr>
<tr class="odd">
<td>total</td>
<td>500</td>
<td>500</td>
<td>1000</td>
</tr>
</tbody>
</table>
<p>Based on this data, the “naive” probabilities we are interested in are:</p>
<p><span class="math display">\[\begin{align*}
P(+|\mathbf{waste}) &amp;= 0\\
P(+|\sim\mathbf{waste}) &amp;= .51
\end{align*}\]</span></p>
<p>In other words, if you see <strong>waste</strong> you definitely have a negative review, but if you don’t, you’re only slightly more likely to have a positive one.</p>
<p>What about combining these two tests? Or using even more words? We could analyze our data to count cases in which both <strong>great</strong> and <strong>waste</strong> occur, in which only one occurs, or in which neither occurs, within the two different categories of reviews, and then use those counts to estimate empirical probabilities of the joint events. But while this might be feasible with two words, if we want to use many words, the number of combinations quickly becomes huge. So instead, we make a basic, and probably false, assumption, but one that makes a simple analysis possible.</p>
<p><strong>Assumption:</strong> We assume that the presence or absence of the words <strong>great</strong> and <strong>waste</strong> in a particular review (positive or negative) are independent events. More generally, given a collection of words <span class="math inline">\(w_1,\ldots, w_k\)</span>, we assume that their occurences in a given review are independent events.</p>
<p>Independence means that we have <span class="math display">\[\begin{align*}
P(\mathbf{great},\mathbf{waste}|\pm) &amp;= P(\mathbf{great}|\pm)P(\mathbf{waste}|\pm)\\
P(\mathbf{great},\sim\mathbf{waste}|\pm) &amp;= P(\mathbf{great}|\pm)P(\sim\mathbf{waste}|\pm)\\
 &amp;\vdots \\
\end{align*}\]</span></p>
<p>So for example, if a document contains the word <strong>great</strong> and does <em>not</em> contain the word <strong>waste</strong>, then the probability of it being a positive review is: <span class="math display">\[
P(+|\mathbf{great},\sim\mathbf{waste}) = \frac{P(\mathbf{great}|+)P(\sim\mathbf{waste}|+)P(+)}{P(\mathbf{great},\sim\mathbf{waste})}
\]</span> while the probability of it being a negative review is <span class="math display">\[
P(-|\mathbf{great},\sim\mathbf{waste}) = \frac{P(\mathbf{great}|-)P(\sim\mathbf{waste}|-)P(-)}{P(\mathbf{great},\sim\mathbf{waste})}
\]</span> Rather than compute these probabilities (which involves working out the denominators), let’s just compare them. Since they have the same denominators, we just need to compare numerators, which we call <span class="math inline">\(L\)</span> for likelihood: Using the data from +<span class="citation" data-cites="tbl:great">@tbl:great</span> and +<span class="citation" data-cites="tbl:waste">@tbl:waste</span>, we obtain: <span class="math display">\[
L(+|\mathbf{great},\sim\mathbf{waste}) = (.184)(1)(.5) = .092
\]</span> and <span class="math display">\[
L(-|\mathbf{great},\sim\mathbf{waste}) = (.01)(.028)(.5) = .00014
\]</span> so our data suggests strongly that this is a positive review.</p>
<h2 id="feature-vectors">Feature vectors</h2>
<p>To generalize this, suppose that we have extracted keywords <span class="math inline">\(w_1,\ldots, w_k\)</span> from our data and we have computed the empirical values <span class="math inline">\(P(w_{i}|+)\)</span> and <span class="math inline">\(P(w_{i}|-)\)</span> by counting the fraction of positive and negative reviews that contain the word <span class="math inline">\(w_{i}\)</span>:</p>
<p><span class="math display">\[
P(w_{i}|\pm) = \frac{\hbox{\rm number of $\pm$ reviews that mention $w_{i}$}}{\hbox{\rm number of $\pm$ reviews total}}
\]</span></p>
<p>Notice that we only count <em>reviews</em>, not <em>ocurrences</em>, so that if a word occurs multiple times in a review it only contributes 1 to the count. That’s why this is called the <em>Bernoulli</em> Naive Bayes – we are thinking of each keyword as yielding a yes/no test on each review.</p>
<p>Given a review, we look to see whether each of our <span class="math inline">\(k\)</span> keywords appears or does not. We encode this information as a vector of length <span class="math inline">\(k\)</span> containing <span class="math inline">\(0\)</span>’s and <span class="math inline">\(1\)</span>’s indicating the absence or presence of the <span class="math inline">\(k\)</span>th keyword. Let’s call this vector the <em>feature vector</em> for the review.</p>
<p>For example, if our keywords are <span class="math inline">\(w_1=\mathbf{waste}\)</span>, <span class="math inline">\(w_2=\mathbf{great}\)</span>, and <span class="math inline">\(w_3=\mathbf{useless}\)</span>, and our review says</p>
<pre><code>This phone is useless, useless, useless!  What a waste!</code></pre>
<p>then the associated feature vector is <span class="math inline">\(f=(1,0,1)\)</span>.</p>
<p>For the purposes of classification of our reviews, we are going to forget entirely about the text of our reviews and work only with the feature vectors. From an abstract perspective, then, by choosing our <span class="math inline">\(k\)</span> keywords, our “training set” of <span class="math inline">\(N\)</span> labelled reviews can be replaced by an <span class="math inline">\(N\times k\)</span> matrix <span class="math inline">\(X=(x_{ij})\)</span> with entries <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, where <span class="math inline">\(x_{ij}=1\)</span> if and only if the <span class="math inline">\(j^{th}\)</span> keyword appears in the <span class="math inline">\(i^{th}\)</span> review.</p>
<p>The labels of <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> for unfavorable or favorable reviews can also be packaged up into a <span class="math inline">\(N\times 1\)</span> vector <span class="math inline">\(Y\)</span> that serves as our “target” variable.</p>
<p>Setting things up this way lets us express the computations of our probabilities <span class="math inline">\(P(w_{i}|\pm)\)</span> in vector form. In fact, <span class="math inline">\(Y^{\intercal}X\)</span> is the sum of the rows of <span class="math inline">\(X\)</span> corresponding to positive reviews, and therefore, letting <span class="math inline">\(N_{\pm}\)</span> denote the number of <span class="math inline">\(\pm\)</span> reviews, <span class="math display">\[
P_{+} = \frac{1}{N_{+}}Y^{\intercal}X = \left[\begin{array}{cccc} P(w_{1}|+)&amp; P(w_{2}|+) &amp; \cdots &amp;P(w_{k}|+)\end{array}\right].
\]</span> Similarly, since <span class="math inline">\(Y\)</span> and <span class="math inline">\(X\)</span> have zero and one entries only, if we write <span class="math inline">\(1-Y\)</span> and <span class="math inline">\(1-X\)</span> for the matrices obtained by replacing every entry <span class="math inline">\(z\)</span> by <span class="math inline">\(1-z\)</span> in each matrix, we have: <span class="math display">\[
P_{-} = \frac{1}{N_{-}}(1-Y)^{\intercal}X =  \left[\begin{array}{cccc} P(w_{1}|-)&amp; P(w_{2}|-) &amp; \cdots &amp;P(w_{k}|-)\end{array}\right].
\]</span></p>
<p>Note that the number of positive reviews is <span class="math inline">\(N_{+}=Y^{\intercal}Y\)</span> and the number of negative ones is <span class="math inline">\(N_{-}=N-N_{+}\)</span>. Since <span class="math inline">\(P(+)\)</span> is the fraction of positive reviews among all reviews, we can compute it as <span class="math inline">\(P(+)=\frac{1}{N}Y^{\intercal}Y\)</span>, and <span class="math inline">\(P(-)=1-P(+)\)</span>.</p>
<h2 id="likelihood">Likelihood</h2>
<p>If a review has an associated feature vector <span class="math inline">\(f=(f_1,\ldots, f_k)\)</span>, then by independence the probability of that feature vector ocurring within one of the <span class="math inline">\(\pm\)</span> classes is <span class="math display">\[
P(f|\pm) = \prod_{i: f_{i}=1} P(w_{i}|\pm)\prod_{i: f_{i}=0}(1-P(w_{i}|\pm))
\]</span> which we can also write <span class="math display">\[
P(f|\pm) = \prod_{i=1}^{k} P(w_{i}|\pm)^{f_{i}}(1-P(w_{i}|\pm))^{(1-f_{i})}.
\]</span>{#eq:likelihood}</p>
<p>These products aren’t practical to work with – they are often the product of many, many small numbers and are therefore really tiny. Therefore it’s much more practical to work with their logarithms. <span class="math display">\[
\log P(f|\pm) = \sum_{i=1}^{k} f_{i}\log P(w_{i}|\pm) + (1-f_{i})\log(1-P(w_{i}|\pm))
\]</span>{#eq:loglikelihood}</p>
<p>If we have a group of reviews <span class="math inline">\(N\)</span> organized in a matrix <span class="math inline">\(X\)</span>, where each row is the feature vector associated to the corresponding review, then we can compute all of this at once. We’ll write <span class="math inline">\(\log P_{\pm}=\log P(X|\pm)\)</span> as the row vector whose <span class="math inline">\(i^{th}\)</span> entry is <span class="math inline">\(\log P(f_{i}|\pm)\)</span>:</p>
<p><span class="math display">\[
\log P(X|\pm) = X(\log P_{\pm})^{\intercal}+(1-X)(\log (1-P_{\pm}))^{\intercal}.
\]</span>{#eq:matrixlikelihood}</p>
<p>By Bayes Theorem, we can express the chance that our review with feature vector <span class="math inline">\(f\)</span> is positive or negative by the formula: <span class="math display">\[
\log P(\pm|f) = \log P(f|\pm)+\log P(\pm) - \log P(f)
\]</span> where <span class="math display">\[
P(\pm) = \frac{\hbox{\rm the number of $\pm$ reviews}}{\hbox{\rm total number of reviews}}
\]</span> and <span class="math inline">\(P(f)\)</span> is the fraction of reviews with the given feature vector. (Note: in practice, some of these probabilities will be zero, and so the log will not be defined. A common practical approach to dealing with this is to introduce a “fake document” into both classes in which every vocabulary word appears – this guarantees that the frequency matrix will have no zeros in it).</p>
<p>A natural classification rule would be to say that a review is positive if <span class="math inline">\(\log P(+|f)&gt;\log P(-|f)\)</span>, and negative otherwise. In applying this, we can avoid computing <span class="math inline">\(P(f)\)</span> by just comparing <span class="math inline">\(\log P(f|+)+\log P(+)\)</span> and <span class="math inline">\(\log P(f|-)+\log P(-)\)</span> computed using +<span class="citation" data-cites="eq:loglikelihood">@eq:loglikelihood</span>. Then we say:</p>
<ul>
<li>a review is positive if <span class="math inline">\(\log P(f|+)+\log P(+)&gt;\log P(f|-)+\log P(-)\)</span> and negative otherwise.</li>
</ul>
<p>Again we can exploit the matrix structure to do this for a bunch of reviews at once. Using +<span class="citation" data-cites="eq:matrixlikelihood">@eq:matrixlikelihood</span> and the vectors <span class="math inline">\(P_{\pm}\)</span> we can compute column vectors corresponding to both sides of our decision inequality and subtract them. The positive entries indicate positive reviews, and the negative ones, negative reviews.</p>
<h2 id="the-bag-of-words">The Bag of Words</h2>
<p>In our analysis above, we thought of the presence or absence of certain key words as a set of independent tests that provided evidence of whether our review was positive or negative. This approach is suited to short pieces of text, but what about longer documents? In that case, we might want to consider not just the presence or absence of words, but the frequency with which they appear. Multinomial Naive Bayes, based on the “bag of words” model, is a classification method similar to Bernoulli Naive Bayes but which takes term frequency into account.</p>
<p>Let’s consider, as above, the problem of classifying documents into one of two classes. We assume that we have a set of keywords <span class="math inline">\(w_1,\ldots, w_k\)</span>. For each class <span class="math inline">\(\pm\)</span>, we have a set of probabilities <span class="math inline">\(P(w_i|\pm)\)</span> with the property that <span class="math display">\[
\sum_{i=1}^{k}P(w_{i}|\pm)=1.
\]</span></p>
<p>The “bag of words” model says that we construct a document of length <span class="math inline">\(N\)</span> in, say, the <span class="math inline">\(+\)</span> class by independently drawing a word <span class="math inline">\(N\)</span> times from the set <span class="math inline">\(w_1,\ldots, w_k\)</span> with probabilities <span class="math inline">\(P(w_{i}|+)\)</span>. The name “bag of words” comes from thinking of each class as having an associated bag containing the words <span class="math inline">\(w_1,\ldots, w_k\)</span> with relative frequencies given by the probabilities, and generating a document by repeatedly drawing a word from the bag.</p>
<p>In the Multinomial Naive Bayes method, we estimate the probabilities <span class="math inline">\(P(w_{i}|\pm)\)</span> by counting the number of times each word occurs in a document of the given class: <span class="math display">\[
P(w_{i}|\pm) = \frac{\hbox{\rm number of times word $i$ occurs in $\pm$ documents}}{\hbox{\rm total number of words in $\pm$ documents}}
\]</span> This is the “naive” part of the algorithm. Package up these probabilities in vectors: <span class="math display">\[
P_{\pm} = \left[\begin{array}{ccc} P(w_{1}|\pm) &amp; \cdots &amp; P(w_{k}|\pm)\end{array}\right].
\]</span></p>
<p>As in the Bernoulli case, we often add a fake document to each class where all of the words occur once, in order to avoid having zero frequencies when we take a logarithm later.</p>
<p>Now, given a document, we associate a feature vector <span class="math inline">\(\mathbf{f}\)</span> whose <span class="math inline">\(i^{th}\)</span> entry is the frequency with which word <span class="math inline">\(i\)</span> appears in that document. The probability of obtaining a particular document with feature vector <span class="math inline">\(\mathbf{f}=(f_1,\ldots, f_k)\)</span> from the bag of words associated with class <span class="math inline">\(\pm\)</span> is given by the “multinomial” distribution: <span class="math display">\[
P(\mathbf{f}|\pm)=\frac{N!}{f_1!f_2!\cdots f_k!} \prod_{i=1}^{k} P(w_{i}|\pm)^{f_{i}}
\]</span> which generalizes the binomial distribution to multiple choices. The constant will prove irrelevant, so let’s call the interesting part <span class="math inline">\(L_{\pm}\)</span>: <span class="math display">\[
L(\mathbf{f}|\pm)= \prod_{i=1}^{k} P(w_{i}|\pm)^{f_{i}}
\]</span></p>
<p>From Bayes Theorem, we have <span class="math display">\[
P(\pm|\mathbf{f}) = \frac{P(\mathbf{f}|\pm)P(\pm)}{P(\mathbf{f})}
\]</span> where <span class="math inline">\(P(\pm)\)</span> is estimated by the fraction of documents (total) in each class.</p>
<p>We classify our document by considering <span class="math inline">\(P(\pm|\mathbf{f})\)</span> and concluding:</p>
<ul>
<li>a document with feature vector <span class="math inline">\(\mathbf{f}\)</span> is in class <span class="math inline">\(+\)</span> if <span class="math inline">\(\log P(+|\mathbf{f})&gt;\log P(-|\mathbf{f})\)</span>.</li>
</ul>
<p>In this comparison, both the constant (the multinomial coefficient) and the denominator cancel out, so we only need to compare <span class="math inline">\(\log L(\mathbf{f}|+)+\log P(+)\)</span> with <span class="math inline">\(\log L(\mathbf{f}|-)+\log P(-)\)</span> We have <span class="math display">\[
\log L(\mathbf{f}|\pm) = \sum_{i=1}^{k} f_{i}\log P(w_{i}|\pm)
\]</span> or, in vector form, <span class="math display">\[
\log P(\mathbf{f}|\pm) = \mathbf{f}\log P_{\pm}^{\intercal}
\]</span></p>
<p>Therefore, just as in the Bernoulli case, we can package up our document <span class="math inline">\(i\)</span> as an <span class="math inline">\(N\times k\)</span> data matrix <span class="math inline">\(X\)</span>, where position <span class="math inline">\(ij\)</span> gives the number of times word <span class="math inline">\(j\)</span> occurs in document <span class="math inline">\(i\)</span>. Then we can compute the vector <span class="math display">\[
\hat{Y} = X\log P_{+}^{\intercal} + \log P(+)-X\log P_{-}^{\intercal} - \log P(-)
\]</span> and assign those documents where <span class="math inline">\(\hat{Y}&gt;0\)</span> to the <span class="math inline">\(+\)</span> class and the rest to the <span class="math inline">\(-\)</span> class.</p>
<h2 id="other-applications">Other applications</h2>
<p>We developed the Naive Bayes method for sentiment analysis, but once we chose a set of keywords our training data was reduced to an <span class="math inline">\(N\times k\)</span> matrix <span class="math inline">\(X\)</span> of <span class="math inline">\(0/1\)</span> entries, together with an <span class="math inline">\(N\times 1\)</span> target column vector <span class="math inline">\(Y\)</span>. Then our classification problem is to decide whether a given vector of <span class="math inline">\(k\)</span> entries, all <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span>, is more likely to carry a <span class="math inline">\(0\)</span> or <span class="math inline">\(1\)</span> label. All of the parameters we needed for Naive Bayes – the various probabilities – can be extracted from the matrix <span class="math inline">\(X\)</span>.</p>
<p>For example, suppose we have a collection of images represented as black/white pixels in a grid that belong to one of two classes. For example, we might have <span class="math inline">\(28x28\)</span> bitmaps of handwritten zeros and ones that are labelled, and we wish to construct a classifier that can decide whether a new <span class="math inline">\(28x28\)</span> bitmap is a zero or one. An example of such a bitmap is given in +<span class="citation" data-cites="fig:mnist0">@fig:mnist0</span>. We can view each <span class="math inline">\(28x28\)</span> bitmap as a vector of length <span class="math inline">\(784\)</span> with <span class="math inline">\(0/1\)</span> entries and apply the same approach outlined above. However, there are other methods that are more commonly used for this problem, such as logistic regression and neural networks.</p>
<figure>
<img src="img/mnist_data_10_0.png" id="fig:mnist0" style="width:2in" alt="Handwritten 0" /><figcaption aria-hidden="true">Handwritten 0</figcaption>
</figure>
<h1 id="probability-and-bayes-theorem">Probability and Bayes Theorem</h1>
<h2 id="introduction-1">Introduction</h2>
<p>Probability theory is one of the three central mathematical tools in machine learning, along with multivariable calculus and linear algebra. Tools from probability allow us to manage the uncertainty inherent in data collected from real world experiments, and to measure the reliability of predictions that we might make from that data. In these notes, we will review some of the basic terminology of probability and introduce Bayesian inference as a technique in machine learning problems.</p>
<p>This will only be a superficial introduction to ideas from probability. For a thorough treatment, see <a href="https://probability.oer.math.uconn.edu/3160-oer">this open-source introduction to probability.</a> For a more applied emphasis, I recommend the excellent online course <a href="https://ocw.mit.edu/courses/electrical-engineering-and-computer-science/6-041-probabilistic-systems-analysis-and-applied-probability-fall-2010/">Probabilistic Systems Analysis and Applied Probability</a> and its associated text <span class="citation" data-cites="Bertsekas">@Bertsekas</span>.</p>
<h2 id="probability-basics">Probability Basics</h2>
<p>The theory of probability begins with a set <span class="math inline">\(X\)</span> of possible events or outcomes, together with a “probability” function <span class="math inline">\(P\)</span> on (certain) subsets of <span class="math inline">\(X\)</span> that measures “how likely” that combination of events is to occur.</p>
<p>The set <span class="math inline">\(X\)</span> can be discrete or continuous. For example, when flipping a coin, our set of possible events would be the discrete set <span class="math inline">\(\{H,T\}\)</span> corresponding to the possible events of flipping heads or tails. When measuring the temperature using a thermometer, our set of possible outcomes might be the set of real numbers, or perhaps an interval in <span class="math inline">\(\mathbb{R}\)</span>. The thermometer’s measurement is random because it is affected by, say, electronic noise, and so its reading is the true temperature perturbed by a random amount.</p>
<p>The values of <span class="math inline">\(P\)</span> are between <span class="math inline">\(0\)</span>, meaning that the event <em>will not</em> happen, and <span class="math inline">\(1\)</span>, meaning that it is certain to occur. As part of our set up, we assume that the total chance of some event from <span class="math inline">\(X\)</span> occurring is <span class="math inline">\(1\)</span>, so that <span class="math inline">\(P(X)=1\)</span>; and the chance of “nothing” happening is zero, so <span class="math inline">\(P(\emptyset)=0\)</span>. And if <span class="math inline">\(U\subset X\)</span> is some collection, then <span class="math inline">\(P(U)\)</span> is the chance of an event from <span class="math inline">\(U\)</span> occurring.</p>
<p>The last ingredient of this picture of probability is additivity. Namely, we assume that if <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> are subsets of <span class="math inline">\(X\)</span> that are disjoint, then <span class="math display">\[
P(U\cup V)=P(U)+P(V).
\]</span> Even more generally, we assume that this holds for (countably) infinite collections of disjoint subsets <span class="math inline">\(U_1,U_2,\ldots\)</span>, where <span class="math display">\[
P(U_1\cup U_2\cup\cdots)=\sum_{i=1}^{\infty} P(U_i)
\]</span></p>
<p><strong>Definition:</strong> The combination of a set <span class="math inline">\(X\)</span> of possible outcomes and a probability function <span class="math inline">\(P\)</span> on subsets of <span class="math inline">\(X\)</span> that satisfies <span class="math inline">\(P(X)=1\)</span>, <span class="math inline">\(0\le P(U)\le 1\)</span> for all <span class="math inline">\(U\)</span>, and is additive on countable disjoint collections of subsets of <span class="math inline">\(X\)</span> is called a (naive) probability space. <span class="math inline">\(X\)</span> is called the <em>sample space</em> and the subsets of <span class="math inline">\(X\)</span> are called <em>events</em>.</p>
<p><strong>Warning:</strong> The reason for the term “naive” in the above definition is that, if <span class="math inline">\(X\)</span> is an uncountable set such as the real numbers <span class="math inline">\(\mathbb{R}\)</span>, then the conditions in the definition are self-contradictory. This is a deep and rather surprising fact. To make a sensible definition of a probability space, one has to restrict the domain of the probability function <span class="math inline">\(P\)</span> to certain subsets of <span class="math inline">\(X\)</span>. These ideas form the basis of the mathematical subject known as measure theory. In these notes we will work with explicit probability functions and simple subsets such as intervals that avoid these technicalities.</p>
<h3 id="discrete-probability-examples">Discrete probability examples</h3>
<p>The simplest probability space arises in the analysis of coin-flipping. As mentioned earlier, the set <span class="math inline">\(X\)</span> contains two elements <span class="math inline">\(\{H,T\}\)</span>. The probability function <span class="math inline">\(P\)</span> is determined by its value <span class="math inline">\(P(\{H\})=p\)</span>, where <span class="math inline">\(0\le p\le 1\)</span>, which is the chance of the coin yielding a “head”. Since <span class="math inline">\(P(X)=1\)</span>, we have <span class="math inline">\(P(\{T\})=1-p\)</span>.</p>
<p>Other examples of discrete probability spaces arise from dice-rolling and playing cards. For example, suppose we roll two six-sided dice. There are <span class="math inline">\(36\)</span> possible outcomes from this experiment, each equally likely. If instead we consider the sum of the two values on the dice, our outcomes range from <span class="math inline">\(2\)</span> to <span class="math inline">\(12\)</span> and the probabilities of these outcomes are given by</p>
<table>
<thead>
<tr class="header">
<th>2</th>
<th>3</th>
<th>4</th>
<th>5</th>
<th>6</th>
<th>7</th>
<th>8</th>
<th>9</th>
<th>10</th>
<th>11</th>
<th>12</th>
<th></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>1/36</td>
<td>1/18</td>
<td>1/12</td>
<td>1/9</td>
<td>5/36</td>
<td>1/6</td>
<td>5/36</td>
<td>1/9</td>
<td>1/12</td>
<td>1/18</td>
<td>1/36</td>
<td></td>
</tr>
</tbody>
</table>
<p>A traditional deck of <span class="math inline">\(52\)</span> playing cards contains <span class="math inline">\(4\)</span> aces. Assuming that the chance of drawing any card is the same (and is therefore equal to <span class="math inline">\(1/52\)</span>), the probability of drawing an ace is <span class="math inline">\(4/52=1/13\)</span> since <span class="math display">\[
P(\{A_{\clubsuit},A_{\spadesuit},A_{\heartsuit},A_{\diamondsuit}\}) = 4P(\{A_{\clubsuit}\})=4/52=1/13
\]</span></p>
<h3 id="continuous-probability-examples">Continuous probability examples</h3>
<p>When the set <span class="math inline">\(X\)</span> is continuous, such as in the temperature measurement, we measure <span class="math inline">\(P(U)\)</span>, where <span class="math inline">\(U\subset X\)</span>, by giving a “probability density function” <span class="math inline">\(f:X\to \mathbb{R}\)</span> and declaring that <span class="math display">\[
P(U) = \int_{U}f(x) dX.
\]</span> Notice that our function <span class="math inline">\(f(x)\)</span> has to satisfy the condition <span class="math display">\[
P(X)=\int_{X} f(x)dX = 1.
\]</span></p>
<p>For example, in our temperature measurement example, suppose the “true” outside temperature is <span class="math inline">\(t_0\)</span>, and our thermometer gives a reading <span class="math inline">\(t\)</span>. Then a good model for the random error is to assume that the error <span class="math inline">\(x=t-t_0\)</span> is governed by the density function <span class="math display">\[
f_\sigma(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-x^2/2\sigma^2}
\]</span> where <span class="math inline">\(\sigma\)</span> is a parameter. In a continuous situation such as this one, the probability of any particular outcome in <span class="math inline">\(X\)</span> is zero since <span class="math display">\[
P(\{t\})=\int_{t}^{t}f_{\sigma}(x)dx = 0
\]</span> Still, the shape of the density function does tell you where the values are concentrated – values where the density function is larger are more likely than those where it is smaller.</p>
<p>With this density function, and x=<span class="math inline">\(t-t_0\)</span>, the error in our measurement is given by <span class="math display">\[
P(|t-t_0|&lt;\delta)=\int_{-\delta}^{\delta} \frac{1}{\sigma\sqrt{2\pi}}e^{-x^2/2\sigma^2} dx
\]</span>{#eq:normal}</p>
<p>The parameter <span class="math inline">\(\sigma\)</span> (called the <em>standard deviation</em>) controls how tightly the thermometer’s measurement is clustered around the true value <span class="math inline">\(t_0\)</span>; when <span class="math inline">\(\sigma\)</span> is large, the measurements are scattered widely, when small, they are clustered tightly. See +<span class="citation" data-cites="fig:density">@fig:density</span>.</p>
<figure>
<img src="img/density.png" id="fig:density" alt="Normal Density" /><figcaption aria-hidden="true">Normal Density</figcaption>
</figure>
<h2 id="conditional-probability-and-bayes-theorem">Conditional Probability and Bayes Theorem</h2>
<p>The theory of conditional probability gives a way to study how partial information about an event informs us about the event as a whole. For example, suppose you draw a card at random from a deck. As we’ve seen earlier, the chance that card is an ace is <span class="math inline">\(1/13\)</span>. Now suppose that you learn that (somehow) that the card is definitely not a jack, king, or queen. Since there are 12 cards in the deck that are jacks, kings, or queens, the card you’ve drawn is one of the remaining 40 cards, which includes 4 aces. Thus the chance you are holding an ace is now <span class="math inline">\(4/40=1/10\)</span>.</p>
<p>In terms of notation, if <span class="math inline">\(A\)</span> is the event “my card is an ace” and <span class="math inline">\(B\)</span> is the event “my card is not a jack, queen, or king” then we say that <em>the probability of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span></em> is <span class="math inline">\(1/10\)</span>. The notation for this is <span class="math display">\[
P(A|B) = 1/10.
\]</span></p>
<p>More generally, if <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are events from a sample space <span class="math inline">\(X\)</span>, and <span class="math inline">\(P(B)&gt;0\)</span>, then <span class="math display">\[
P(A|B) = \frac{P(A\cap B)}{P(B)},
\]</span> so that <span class="math inline">\(P(A|B)\)</span> measures the chance that <span class="math inline">\(A\)</span> occurs among those situations in which <span class="math inline">\(B\)</span> occurs.</p>
<h3 id="bayes-theorem">Bayes Theorem</h3>
<p>Bayes theorem is a foundational result in probability.</p>
<p><strong>Theorem:</strong> Bayes Theorem says <span class="math display">\[
P(A|B) = \frac{P(B|A)P(A)}{P(B)}.
\]</span></p>
<p>If we use the definition of conditional probability given above, this is straightforward: <span class="math display">\[
\frac{P(B|A)P(A)}{P(B)} = \frac{P(B\cap A)}{P(B)} = P(A|B).
\]</span></p>
<h3 id="an-example">An example</h3>
<p>To illustrate conditional probability, let’s consider what happens when we administer the most reliable COVID-19 test, the PCR test, to an individual drawn from the population at large. There are two possible test results (positive and negative) and two possible true states of the person being tested (infected and not infected). Suppose I go to the doctor and get a COVID test which comes back positive. What is the probability that I actually have COVID?</p>
<p>Let’s let <span class="math inline">\(S\)</span> and <span class="math inline">\(W\)</span> stand for infected (sick) and not infected (well), and let <span class="math inline">\(+/-\)</span> stand for test positive or negative. Note that there are four possible outcomes of our experiment:</p>
<ul>
<li>test positive and infected (S+) – this is a <em>true positive</em>.</li>
<li>test positive and not infected (W+) – this is a <em>false positive</em>.</li>
<li>test negative and infected (S-) – this is a <em>false negative</em>.</li>
<li>test negative and not infected (W-) – this is a <em>true negative</em>.</li>
</ul>
<p>The <a href="https://www.icd10monitor.com/false-positives-in-pcr-tests-for-covid-19">CDC says</a> that the chance of a false positive – that is, the percentage of samples from well people that incorrectly yields a positive result – is about one-half of one percent, or 5 in 1000.</p>
<p>In other words, <span class="math display">\[
P(+|W) = P(W+)/P(W) = 5/1000=1/200
\]</span></p>
<p>On the other hand, the CDC tells us that chance of a false negative is 1 in 4, so <span class="math display">\[
P(-|S) = P(S-)/P(S) = .25.
\]</span> Since <span class="math inline">\(P(S-)+P(S+)=P(S).\)</span> since every test is either positive or negative, we have <span class="math display">\[
P(+|S) = .75.
\]</span></p>
<p>Suppose furthermore that the overall incidence of COVID-19 in the population is p. In other words, <span class="math inline">\(P(S)=p\)</span> so <span class="math inline">\(P(W)=1-p\)</span>. Then <span class="math display">\[P(S+)=P(S)P(+|S)=.75p\]</span> and <span class="math display">\[
P(W+)=P(W)P(+|W)=.005(1-p).
\]</span> Putting these together we get <span class="math inline">\(P(+)=.005+.745p\)</span></p>
<p>What I’m interested in is <span class="math inline">\(P(S|+)\)</span> – the chance that I’m sick, given that my test result was positive. By Bayes Theorem, <span class="math display">\[
P(S|+)=\frac{P(+|S)P(S)}{P(+)}=.75p/(.005+.745p)=\frac{750p}{5+745p}.
\]</span></p>
<p>As +<span class="citation" data-cites="fig:covidfn">@fig:covidfn</span> shows, if the population incidence is low then a positive test is far from conclusive. Indeed, if the overall incidence of COVID is one percent, then a positive test result only implies a 60 percent chance that I am in fact infected.</p>
<p>Just to fill out the picture, we have <span class="math display">\[
P(-) = P(S-)+P(W-)=(P(S)-P(S+))+(P(W)-P(W+))
\]</span> which yields <span class="math display">\[
P(-)=1-.005+.005p-.75p = .995-.745p.
\]</span> Using Bayes Theorem, we obtain <span class="math display">\[
P(S|-) = \frac{P(-|S)P(S)}{P(-)} = .25p/(.995-.745p) =\frac{250p}{995-745p}.
\]</span> In this case, even though the false negative rate is pretty high (25 percent) overall, if the population incidence is one percent, then the probability that you’re sick given a negative result is only about <span class="math inline">\(.25\)</span> percent. So negative results are very likely correct!</p>
<figure>
<img src="img/covidfn.png" id="fig:covidfn" alt="P(S|+) vs P(S)" /><figcaption aria-hidden="true">P(S|+) vs P(S)</figcaption>
</figure>
<h2 id="independence">Independence</h2>
<p>Independence is one of the fundamental concepts in probability theory. Conceptually, two events are independent if the occurrence of one has does not influence the likelihood of the occurrence of the other. For example, successive flips of a coin are independent events, since the result of the second flip doesn’t have anything to do with the result of the first. On the other hand, whether or not it rains today and tomorrow are not independent events, since the weather tomorrow depends (in a complicated way) on the weather today.</p>
<p>We can formalize this idea of independence using the following definition.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X\)</span> be a sample space and let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two events. Then <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are <em>independent</em> if <span class="math inline">\(P(A\cap B)=P(A)P(B)\)</span>. Equivalently, <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are independent if <span class="math inline">\(P(A|B)=P(A)\)</span> and <span class="math inline">\(P(B|A)=P(B)\)</span>.</p>
<h3 id="examples">Examples</h3>
<h4 id="coin-flipping">Coin Flipping</h4>
<p>Suppose our coin has a probability of heads given by a real number <span class="math inline">\(p\)</span> between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>, and we flip our coin <span class="math inline">\(N\)</span> times. What is the chance of gettting <span class="math inline">\(k\)</span> heads, where <span class="math inline">\(0\le k\le N\)</span>? Any particular sequence of heads and tails containing <span class="math inline">\(k\)</span> heads and <span class="math inline">\(N-k\)</span> tails has probability <span class="math display">\[
P(\hbox{\rm particular sequence of $k$ heads among $N$ flips}) = p^{k}(1-p)^{N-k}.
\]</span> In addition, there are <span class="math inline">\(\binom{N}{k}\)</span> sequences of heads and tails containing <span class="math inline">\(k\)</span> heads. Thus the probability <span class="math inline">\(P(k,N)\)</span> of <span class="math inline">\(k\)</span> heads among <span class="math inline">\(N\)</span> flips is <span class="math display">\[
P(k,N) = \binom{N}{k}p^{k}(1-p)^{N-k}.
\]</span>{#eq:binomial}</p>
<p>Notice that the binomial theorem gives us <span class="math inline">\(\sum_{k=0}^{N} P(k,N) =1\)</span> which is a reassuring check on our work.</p>
<p>The probability distribution on the set <span class="math inline">\(X=\{0,1,\ldots,N\}\)</span> given by <span class="math inline">\(P(k,N)\)</span> is called the <em>binomial distribution</em> with parameters <span class="math inline">\(N\)</span> and <span class="math inline">\(p\)</span>.</p>
<h4 id="a-simple-mixture">A simple ‘mixture’</h4>
<p>Now let’s look at an example of events that are not independent. Suppose that we have two coins, with probabilities of heads <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> respectively; and assume these probabilities are different. We play the a game in which we first choose one of the two coins (with equal chance) and then flip it twice. Is the result of the second flip independent of the first? In other words, is <span class="math inline">\(P(HH)=P(H)^2\)</span>?</p>
<p>This type of situation is called a ‘mixture distribution’ because the probability of a head is a “mixture” of the probability coming from the two different coins.</p>
<p>The chance that the first flip is a head is <span class="math inline">\((p_1+p_2)/2\)</span> because it’s the chance of picking the first coin, and then getting a head, plus the chance of picking the second, and then getting a head. The chance of getting two heads in a row is <span class="math inline">\((p_1^2+p_2^2)/2\)</span> because it’s the chance, having picked the first coin, of getting two heads, plus the chance, having picked the second, of getting two heads.</p>
<p>Since <span class="math display">\[
\frac{p_1^2+p_2^2}{2}\not=\left(\frac{p_1+p_2}{2}\right)^2
\]</span> we see these events are not independent.</p>
<p>In terms of conditional probabilities, the chance that the second flip is a head, given that the first flip is, is computed as: <span class="math display">\[
P(HH|H) = \frac{p_1^2+p_2^2}{p_1+p_2}.
\]</span> From the Cauchy-Schwartz inequality one can show that <span class="math display">\[
\frac{p_1^2+p_2^2}{p_1+p_2}&gt;\frac{p_1+p_2}{2}.
\]</span></p>
<p>Why should this be? Why should the chance of getting a head on the second flip go up given that the first flip was a head? One way to think of this is that the first coin flip contains a little bit of information about which coin we chose. If, for example <span class="math inline">\(p_1&gt;p_2\)</span>, and our first flip is heads, then it’s just a bit more likely that we chose the first coin. As a result, the chance of getting another head is just a bit more likely than if we didn’t have that information. We can make this precise by considering the conditional probability <span class="math inline">\(P(p=p_1|H)\)</span> that we’ve chosen the first coin given that we flipped a head. From Bayes’ theorem:</p>
<p><span class="math display">\[
P(p=p_1|H) = \frac{P(H|p=p_1)P(p=p_1)}{P(H)}=\frac{p_1}{p_1+p_2}=\frac{1}{1+(p_2/p_1)}&gt;\frac{1}{2}
\]</span> since <span class="math inline">\((1+(p_2/p_1))&lt;2\)</span>.</p>
<p><strong>Exercise:</strong> Push this argument a bit further. Let <span class="math inline">\(p_1=\max(p_1,p_2)\)</span> Let <span class="math inline">\(P_N\)</span> be the conditional probability of getting heads assuming that the first <span class="math inline">\(N\)</span> flips were heads. Show that <span class="math inline">\(P_N\to p_1\)</span> as <span class="math inline">\(N\to\infty\)</span>. All those heads piling up make it more and more likely that you’re flipping the first coin and so the chance of getting heads approaches <span class="math inline">\(p_1\)</span>.</p>
<h4 id="an-example-with-a-continuous-distribution">An example with a continuous distribution</h4>
<p>Suppose that we return to our example of a thermometer which measures the ambient temperature with an error that is distributed according to the normal distribution, as in +<span class="citation" data-cites="eq:normal">@eq:normal</span>. Suppose that we make 10 independent measurements <span class="math inline">\(t_1,\ldots, t_{10}\)</span> of the true temperature <span class="math inline">\(t_0\)</span>. What can we say about the distribution of these measurements?</p>
<p>In this case, independence means that <span class="math display">\[
P=P(|t_1-t_0|&lt;\delta,|t_2-t_0|&lt;\delta,\ldots) = P(|t_1-t_0|&lt;\delta)P(|t_2-t_0|&lt;\delta)\cdots P(|t_{10}-t_{0}|&lt;\delta)
\]</span> and therefore <span class="math display">\[
P = \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^{10}\int_{-\delta}^{\delta}\cdots\int_{-\delta}^{\delta}
e^{-(\sum_{i=1}^{10} x_i^2)/2\sigma^2} dx_1\cdots dx_{10}
\]</span></p>
<p>One way to look at this is that the vector <span class="math inline">\(\mathbf{e}\)</span> of errors <span class="math inline">\((|t_1-t_0|,\ldots,|t_{10}-t_0|)\)</span> is distributed according to a <em>multivariate gaussian distribution</em>: <span class="math display">\[
P(\mathbf{e}\in U) =\left(\frac{1}{\sigma\sqrt{2\pi}}\right)^{10}\int_{U} 
e^{-\|x\|^2/2\sigma^2} d\mathbf{x}
\]</span>{#eq:multivariategaussian}</p>
<p>where <span class="math inline">\(U\)</span> is a region in <span class="math inline">\(\mathbf{R}^{10}\)</span>.</p>
<p>The multivariate gaussian can also describe situations where independence does not hold. For simplicity, let’s work in two dimensions and consider the probability density on <span class="math inline">\(\mathbf{R}^{2}\)</span> given by <span class="math display">\[
P(\mathbf{e}\in U) = A\int_{U} e^{-(x_1^2-x_1x_2+x_2^2)/2\sigma^2} d\mathbf{x}.
\]</span> where the constant <span class="math inline">\(A\)</span> is chosen so that <span class="math display">\[
A\int_{\mathbf{R}^{2}}e^{-(x_1^2-x_1x_2+x_2^2)/2\sigma^2}d\mathbf{x} = 1.
\]</span></p>
<p>This density function as a “bump” concentrated near the origin in <span class="math inline">\(\mathbf{R}^{2}\)</span>, and its level curves are a family of ellipses centered at the origin. See +<span class="citation" data-cites="fig:multivariate">@fig:multivariate</span> for a plot of this function with <span class="math inline">\(\sigma=1\)</span>.</p>
<figure>
<img src="img/ellipse.png" id="fig:multivariate" alt="Multivariate Gaussian" /><figcaption aria-hidden="true">Multivariate Gaussian</figcaption>
</figure>
<p>In this situation we can look at the conditional probability of the first variable given the second, and see that the two variables are not independent. Indeed, if we fix <span class="math inline">\(x_2\)</span>, then the distribution of <span class="math inline">\(x_1\)</span> depends on our choice of <span class="math inline">\(x_2\)</span>. We could see this by a calculation, or we can just look at the graph: if <span class="math inline">\(x_2=0\)</span>, then the most likely values of <span class="math inline">\(x_1\)</span> cluster near zero, while if <span class="math inline">\(x_2=1\)</span>, then the most likely values of <span class="math inline">\(x_1\)</span> cluster somewhere above zero.</p>
<h2 id="random-variables-mean-and-variance">Random Variables, Mean, and Variance</h2>
<p>Typically, when we are studying a random process, we aren’t necessarily accessing the underlying events, but rather we are making measurements that provide us with some information about the underlying events. For example, suppose our sample space <span class="math inline">\(X\)</span> is the set of throws of a pair of dice, so <span class="math inline">\(X\)</span> contains the <span class="math inline">\(36\)</span> possible combinations that can arise from the throws. What we are actually interested is the sum of the values of the two dice – that’s our “measurement” of this system. This rather vague notion of a measurement of a random system is captured by the very general idea of a <em>random variable</em>.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X\)</span> be a sample space with probability function <span class="math inline">\(P\)</span>. A <em>random variable</em> on <span class="math inline">\(X\)</span> is a function <span class="math inline">\(f:X\to \mathbb{R}\)</span>.</p>
<p>Given a random variable <span class="math inline">\(f\)</span>, we can use the probability measure to decide how likely <span class="math inline">\(f\)</span> is to take a particular value, or values in a particular set by the formula <span class="math display">\[
P(f(x)\in U) = P(f^{-1}(U))
\]</span></p>
<p>In the dice rolling example, the random variable <span class="math inline">\(S\)</span> that assigns their sum to the pair of values obtained on two dice is a random variable. Those values lie between <span class="math inline">\(2\)</span> and <span class="math inline">\(12\)</span> and we have <span class="math display">\[
P(S=k) = P(S^{-1}(\{k\}))=P(\{(x,y): x+y=k\})
\]</span> where <span class="math inline">\((x,y)\)</span> runs through <span class="math inline">\(\{1,2,\ldots,6\}^{2}\)</span> representing the two values and <span class="math inline">\(P((x,y))=1/36\)</span> since all throws are equally likely.</p>
<p>Let’s look at a few more examples, starting with what is probably the most fundamental of all.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X\)</span> be a sample space with two elements, say <span class="math inline">\(H\)</span> and <span class="math inline">\(T\)</span>, and suppose that <span class="math inline">\(P(H)=p\)</span> for some <span class="math inline">\(0\le p\le 1\)</span>. Then the random variable that satisfies <span class="math inline">\(f(H)=1\)</span> and <span class="math inline">\(f(T)=0\)</span> is called a Bernoulli random variable with parameter <span class="math inline">\(p\)</span>.</p>
<p>In other words, a Bernoulli random variable gives the value <span class="math inline">\(1\)</span> when a coin flip is heads, and <span class="math inline">\(0\)</span> for tails.</p>
<p>Now let’s look at what we earlier called the binomial distribution.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X\)</span> be a sample space consisting of strings of <span class="math inline">\(H\)</span> and <span class="math inline">\(T\)</span> of length <span class="math inline">\(N\)</span>, with the probability of a <em>particular string</em> <span class="math inline">\(S\)</span> with <span class="math inline">\(k\)</span> heads and <span class="math inline">\(N-k\)</span> tails given by <span class="math display">\[
P(S)=p^{k}(1-p)^{N-k}
\]</span> for some <span class="math inline">\(0\le p\le 1\)</span>. In other words, <span class="math inline">\(X\)</span> is the sample space consisting of <span class="math inline">\(N\)</span> independent flips of a coin with probability of heads given by <span class="math inline">\(p\)</span>.</p>
<p>Let <span class="math inline">\(f:X\to \mathbb{R}\)</span> be the function which counts the number of <span class="math inline">\(H\)</span> in the string. We can express <span class="math inline">\(f\)</span> in terms of Bernoulli random variables; indeed, <span class="math display">\[
f=X_1+\ldots+X_N
\]</span> where each <span class="math inline">\(X_i\)</span> is a Bernoulli random variable with parameter <span class="math inline">\(p\)</span>.</p>
<p>Now <span class="math display">\[
P(f=k) = \binom{N}{k}p^{k}(1-p)^{N-k}
\]</span> since <span class="math inline">\(f^{-1}(\{k\})\)</span> is the number of elements in the subset of strings of <span class="math inline">\(H\)</span> and <span class="math inline">\(T\)</span> of length <span class="math inline">\(N\)</span> containing exactly <span class="math inline">\(k\)</span> <span class="math inline">\(H\)</span>’s. This is our old friend the binomial distribution. So <em>a binomial distribution is the distribution of the sum of <span class="math inline">\(N\)</span> independent Bernoulli random variables.</em></p>
<p>For an example with a continuous random variable, suppose our sample space is <span class="math inline">\(\mathbf{R}^{2}\)</span> and the probability density is the simple multivariate normal <span class="math display">\[
P(\mathbf{x}\in U) = \left(\frac{1}{\sqrt{2\pi}}\right)^2\int_{U} e^{-\|\mathbf{x}\|^2/2} d\mathbf{x}.
\]</span> Let <span class="math inline">\(f\)</span> be the random variable <span class="math inline">\(f(\mathbf{x})=\|\mathbf{x}\|\)</span>. The function <span class="math inline">\(f\)</span> measures the Euclidean distance of a randomly drawn point from the origin. The set <span class="math display">\[U=f^{-1}([0,r))\subseteq\mathbf{R}^{2}\]</span> is the circle of radius <span class="math inline">\(r\)</span> in <span class="math inline">\(\mathbf{R}^{2}\)</span>. The probability that a randomly drawn point lies in this circle is <span class="math display">\[
P(f&lt;r) = \left(\frac{1}{\sqrt{2\pi}}\right)^2\int_{U} e^{-\|\mathbf{x}\|^2/2} d\mathbf{x}.
\]</span></p>
<p>We can actually evaluate this integral in closed form by using polar coordinates. We obtain <span class="math display">\[
P(f&lt;r) = \left(\frac{1}{\sqrt{2\pi}}\right)^2\int_{\theta=0}^{2\pi}\int_{\rho=0}^{r} e^{-\rho^2/2}\rho d\rho d\theta.
\]</span> Since <span class="math display">\[
\frac{d}{d\rho}e^{-\rho^2/2}=-\rho e^{-\rho^2/2}
\]</span> we have <span class="math display">\[\begin{align*}
P(f&lt;r)&amp;=-\frac{1}{2\pi}\theta e^{-\rho^2/2}|_{\theta=0}^{2\pi}|_{\rho=0}^{r}\cr
&amp;=1-e^{-r^2/2}\cr
\end{align*}\]</span></p>
<p>The probability density associated with this random variable is the derivative of <span class="math inline">\(1-e^{-r^2/2}\)</span> <span class="math display">\[
P(f\in [a,b])=\int_{r=a}^{b} re^{-r^2/2} dr
\]</span> as you can see by the fundamental theorem of calculus. This density is drawn in +<span class="citation" data-cites="fig:maxwell">@fig:maxwell</span> where you can see that the points are clustered at a distance of <span class="math inline">\(1\)</span> from the origin.</p>
<figure>
<img src="img/maxwell.png" id="fig:maxwell" alt="Density of the Norm" /><figcaption aria-hidden="true">Density of the Norm</figcaption>
</figure>
<h3 id="independence-and-random-variables">Independence and Random Variables</h3>
<p>We can extend the notion of independence from events to random variables.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> be two random variables on a sample space <span class="math inline">\(X\)</span> with probability <span class="math inline">\(P\)</span>. Then <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> are independent if, for all intervals <span class="math inline">\(U\)</span> and <span class="math inline">\(V\)</span> in <span class="math inline">\(\mathbb{R}\)</span>, the events <span class="math inline">\(f^{-1}(U)\)</span> and <span class="math inline">\(g^{-1}(V)\)</span> are independent.</p>
<p>For discrete probability distributions, this means that, for all <span class="math inline">\(a,b\in\mathbb{R}\)</span>, <span class="math display">\[
P(f=a\hbox{\rm\ and\ }g=b)=P(f=a)P(g=b).
\]</span></p>
<p>For continous probability distributions given by a density function <span class="math inline">\(P(x)\)</span>, independence can be more complicated to figure out.</p>
<h3 id="expectation-mean-and-variance">Expectation, Mean and Variance</h3>
<p>The most fundamental tool in the study of random variables is the concept of “expectation”, which is a fancy version of average. The word “mean” is a synonym for expectation – the mean of a random variable is the same as its expectation or “expected value.”</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X\)</span> be a sample space with probability measure <span class="math inline">\(P\)</span>. Let <span class="math inline">\(f:X\to \mathbb{R}\)</span> be a random variable. Then the <em>expectation</em> or <em>expected value</em> <span class="math inline">\(E[f]\)</span> of <span class="math inline">\(f\)</span> is <span class="math display">\[
E[f] = \int_X f(x)dP.
\]</span> More specifically, if <span class="math inline">\(X\)</span> is discrete, then <span class="math display">\[
E[f] = \sum_{x\in X} f(x)P(x)
\]</span> while if <span class="math inline">\(X\)</span> is continuous with probability density function <span class="math inline">\(p(x)dx\)</span> then <span class="math display">\[
E[f] = \int_{X} f(x)p(x)dx.
\]</span></p>
<p>If <span class="math inline">\(f\)</span> is a Bernoulli random variable with parameter <span class="math inline">\(p\)</span>, then <span class="math display">\[
E[f] = 1\cdot p+0\cdot (1-p) = p
\]</span></p>
<p>If <span class="math inline">\(f\)</span> is a binomial random variable with parameters <span class="math inline">\(p\)</span> and <span class="math inline">\(N\)</span>, then <span class="math display">\[
E[f] = \sum_{i=0}^{N} i\binom{N}{i}p^{i}(1-p)^{N-i}
\]</span> One can evaluate this using some combinatorial tricks, but it’s easier to apply this basic fact about expectations.</p>
<p><strong>Proposition:</strong> Expectation is linear: <span class="math inline">\(E[aX+bY]=aE[X]+bE[Y]\)</span> for random variables <span class="math inline">\(X,Y\)</span> and constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>.</p>
<p>The proof is an easy consequence of the expression of <span class="math inline">\(E\)</span> as a sum (or integral).</p>
<p>Since a binomial random variable <span class="math inline">\(Z\)</span> with parameters <span class="math inline">\(N\)</span> and <span class="math inline">\(p\)</span> is the sum of <span class="math inline">\(N\)</span> Bernoulli random variables, its expectation is <span class="math display">\[
E[X_1+\cdots+X_N]=Np.
\]</span></p>
<p>A more sophisticated property of expectation is that it is multiplicative when the random variables are independent.</p>
<p><strong>Proposition:</strong> Let <span class="math inline">\(f\)</span> and <span class="math inline">\(g\)</span> be two independent random variables. Then <span class="math inline">\(E[fg]=E[f]E[g]\)</span>.</p>
<p><strong>Proof:</strong> Let’s suppose that the sample space <span class="math inline">\(X\)</span> is discrete. By definition, <span class="math display">\[
E[f]=\sum_{x\in X}f(x)P(x)
\]</span> and we can rewrite this as <span class="math display">\[
E[f]=\sum_{a\in\mathbf{R}} aP(\{x: f(x)=a\}).
\]</span> Let <span class="math inline">\(Z\subset\mathbb{R}\)</span> be the range of <span class="math inline">\(f\)</span>. Then <span class="math display">\[\begin{align*}
E[fg]&amp;=\sum_{a\in Z} aP(\{x: fg(x)=a\}) \\
&amp;=\sum_{a\in Z}\sum_{(u,v)\in\genfrac{}{}{0pt}{}{\mathbf{Z}^{2}}{uv=a}}aP(\{x:f(x)=u\hbox{\rm\ and\ }g(x)=v\}) \\
&amp;=\sum_{a\in Z}\sum_{\genfrac{}{}{0pt}{}{\mathbf{Z}^{2}}{uv=a}}uvP(\{x:f(x)=u\})P(\{x:g(x)=v\}) \\
&amp;=\sum_{u\in Z}uP(\{x:f(x)=u\})\sum_{v\in Z}vP(\{x:f(x)=v\}) \\
&amp;=E[f]E[g] 
\end{align*}\]</span></p>
<h4 id="variance">Variance</h4>
<p>The variance of a random variable is a measure of its dispersion around its mean.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(f\)</span> be a random variable. Then the variance is the expression <span class="math display">\[
\sigma^2(f) = E[(f-E[f])^2]=E[f^2]-(E[f])^2
\]</span> The square root of the variance is called the “standard deviation.”</p>
<p>The two formulae for the variance arise from the calculation <span class="math display">\[
E[(f-E[f])^2]=E[(f^2-2fE[f]+E[f]^2)]=E[f^2]-2E[f]^2+E[f]^2=E[f^2]-E[f]^2.
\]</span></p>
<p>To compute the variance of the Bernoulli random variable <span class="math inline">\(f\)</span> with parameter <span class="math inline">\(p\)</span>, we first compute <span class="math display">\[
E[f^2]=p(1)^2+(1-p)0^2=p.
\]</span> Since <span class="math inline">\(E[f]=p\)</span>, we have <span class="math display">\[
\sigma^2(f)=p-p^2=p(1-p).
\]</span></p>
<p>If <span class="math inline">\(f\)</span> is the binomial random variable with parameters <span class="math inline">\(N\)</span> and <span class="math inline">\(p\)</span>, we can again use the fact that <span class="math inline">\(f\)</span> is the sum of <span class="math inline">\(N\)</span> Bernoulli random variables <span class="math inline">\(X_1+\cdots+X_n\)</span> and compute</p>
<p><span class="math display">\[\begin{align*}
E[(\sum_{i}X_i)^2]-E[\sum_{i} X_{i}]^2 &amp;=E[\sum_{i} X_i^2+\sum_{i,j}X_{i}X_{j}]-N^2p^2\\
&amp;=Np+N(N-1)p^2-N^2p^2 \\
&amp;=Np(1-p)
\end{align*}\]</span></p>
<p>where we have used the fact that the square <span class="math inline">\(X^2\)</span> of a Bernoulli random variable is equal to <span class="math inline">\(X\)</span>.</p>
<p>For a continuous example, suppose that we consider a sample space <span class="math inline">\(\mathbb{R}\)</span> with the normal probability density <span class="math display">\[
P(x) = \frac{1}{\sigma\sqrt{2\pi}}e^{-x^2/2\sigma^2}dx.
\]</span></p>
<p>The mean of the random variable <span class="math inline">\(x\)</span> is <span class="math display">\[
E[x] =\frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty} xe^{-x^2/2\sigma^2}dx=0
\]</span></p>
<p>since the function being integrated is odd. The variance is</p>
<p><span class="math display">\[
E[x^2] = \frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty} x^2e^{-x^2/2\sigma^2}dx.
\]</span></p>
<p>The trick to evaluating this integral is to consider the derivative:</p>
<p><span class="math display">\[
\frac{d}{d\sigma}\left[\frac{1}{\sigma\sqrt{2\pi}}\int_{-\infty}^{\infty}e^{-x^2/(2\sigma^2)}dx\right]=0
\]</span></p>
<p>where the result is zero since the quantity being differentiated is a constant (namely <span class="math inline">\(1\)</span>). Sorting through the resulting equation leads to the fact that</p>
<p><span class="math display">\[
E[x^2]=\sigma^2
\]</span></p>
<p>so that the <span class="math inline">\(\sigma^2\)</span> parameter in the normal distribution really <em>is</em> the variance of the associated random variable.</p>
<h2 id="models-and-likelihood">Models and Likelihood</h2>
<p>A <em>statistical model</em> is a mathematical model that accounts for data via a process that incorporates random behavior in a structured way. We have seen several examples of such models in our discussion so far. For example, the Bernoulli process that describes the outcome of a series of coin flips as independent choices of heads or tails with probability <span class="math inline">\(p\)</span> is a simple statistical model; our more complicated mixture model in which we choose one of two coins at random and then flip that is a more complicated model.<br />
Our description of the variation in temperature measurements as arising from perturbations from the true temperature by a normally distributed amount is another example of a statistical model, this one involving a continuous random variable.</p>
<p>When we apply a mathematical model to understand data, we often have a variety of parameters in the model that we must adjust to get the model to best “fit” the observed data. For example, suppose that we observe the vibrations of a block attached to a spring. We know that the motion is governed by a second order linear differential equation, but the dynamics depend on the mass of the block, the spring constant, and the damping coefficient. By measuring the dynamics of the block over time, we can try to work backwards to figure out these parameters, after which we will be able to predict the block’s motion into the future.</p>
<h3 id="sec:mlcoin">Maximum Likelihood (Discrete Case)</h3>
<p>To see this process in a statistical setting, let’s return to the simple example of a coin flip. The only parameter in our model is the probability <span class="math inline">\(p\)</span> of getting heads on a particular flip. Suppose that we flip the coin <span class="math inline">\(100\)</span> times and get <span class="math inline">\(55\)</span> heads and <span class="math inline">\(45\)</span> tails. What can we say about <span class="math inline">\(p\)</span>?</p>
<p>We will approach this question via the “likelihood” function for our data. We ask: for a particular value of the parameter <span class="math inline">\(p\)</span>, how likely is this outcome? From +<span class="citation" data-cites="eq:binomial">@eq:binomial</span> we have <span class="math display">\[
P(55H,45T)=\binom{100}{55}p^{55}(1-p)^{45}.
\]</span></p>
<p>This function is plotted in +<span class="citation" data-cites="fig:beta">@fig:beta</span>. As you can see from that plot, it is extremely unlikely that we would have gotten <span class="math inline">\(55\)</span> heads if <span class="math inline">\(p\)</span> was smaller than <span class="math inline">\(.4\)</span> or greater than <span class="math inline">\(.7\)</span>, while the <em>most likely</em> value of <span class="math inline">\(p\)</span> occurs at the maximum value of this function, and a little calculus tells us that this point is where <span class="math inline">\(p=.55\)</span>. This <em>most likely</em> value of <span class="math inline">\(p\)</span> is called the <em>maximum likelihood estimate</em> for the parameter <span class="math inline">\(p\)</span>.</p>
<figure>
<img src="img/beta.png" id="fig:beta" alt="Likelihood Plot" /><figcaption aria-hidden="true">Likelihood Plot</figcaption>
</figure>
<h3 id="maximum-likelihood-continuous-case">Maximum Likelihood (Continuous Case)</h3>
<p>Now let’s look at our temperature measurements where the error is normally distributed with variance parameter <span class="math inline">\(\sigma^2\)</span>. As we have seen earlier, the probability density of errors <span class="math inline">\(\mathbf{x}=(x_1,\ldots,x_n)\)</span> of <span class="math inline">\(n\)</span> independent measurements is <span class="math display">\[
P(\mathbf{x}) = \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^{n}e^{-\|\mathbf{x}\|^2/(2\sigma^2)}d\mathbf{x}.
\]</span> (see +<span class="citation" data-cites="eq:multivariategaussian">@eq:multivariategaussian</span>). What should we use as the parameter <span class="math inline">\(\sigma\)</span>? We can ask which choice of <span class="math inline">\(\sigma\)</span> makes our data <em>most likely</em>. To calculate this, we think of the probability of a function of <span class="math inline">\(\sigma\)</span> and use Calculus to find the maximum. It’s easier to do this with the logarithm.</p>
<p><span class="math display">\[
\log P(\mathbf{x})=\frac{-\|\mathbf{x}\|^2}{2\sigma^2}-n\log{\sigma}+C
\]</span> where <span class="math inline">\(C\)</span> is a constant that we’ll ignore. Taking the derivative and setting it to zero, we obtain <span class="math display">\[
-\|\mathbf{x}\|^2\sigma^{-3}-n\sigma^{-1}=0
\]</span> which gives the formula <span class="math display">\[
\sigma^2=\frac{\|\mathbf{x}\|^2}{n}
\]</span></p>
<p>This should look familiar! The maximum likelihood estimate of the variance is the <em>mean-squared-error</em>.</p>
<h3 id="linear-regression-and-likelihood">Linear Regression and likelihood</h3>
<p>In our earlier lectures we discussed linear regression at length. Our introduction of ideas from probability give us new insight into this fundamental tool. Consider a statistical model in which certain measured values <span class="math inline">\(y\)</span> depend linearly on <span class="math inline">\(x\)</span> up to a normally distributed error: <span class="math display">\[
y=mx+b+\epsilon
\]</span> where <span class="math inline">\(\epsilon\)</span> is drawn from the normal distribution with variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p>The classic regression setting has us measuring a collection of <span class="math inline">\(N\)</span> points <span class="math inline">\((x_i,y_i)\)</span> and then asking for the “best” <span class="math inline">\(m\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(\sigma^2\)</span> to explain these measurements. Using the likelihood perspective, each value <span class="math inline">\(y_i-mx_i-b\)</span> is an independent draw from the normal distribution with variance <span class="math inline">\(\sigma^2\)</span>, exactly like our temperature measurements in the one variable case.</p>
<p>The likelihood (density) of those draws is therefore <span class="math display">\[
P = \left(\frac{1}{\sigma\sqrt{2\pi}}\right)^Ne^{-\sum_{i}(y_i-mx_i-b)^2/(2\sigma^2)}.
\]</span> What is the maximum likelihood estimate of the parameters <span class="math inline">\(m\)</span>, <span class="math inline">\(b\)</span>, and <span class="math inline">\(\sigma^2\)</span>?</p>
<p>To find this we look at the logarithm of <span class="math inline">\(P\)</span> and take derivatives. <span class="math display">\[
\log(P) = -N\log(\sigma) -\frac{1}{2\sigma^2}\sum_{i}(y_i-mx_i-b)^2.
\]</span></p>
<p>As far as <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> are concerned, the minimum comes from the derivatives with respect to <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> of <span class="math display">\[
\sum_{i}(y_i-mx_i-b)^2.
\]</span> In other words, the maximum likelihood estimate <span class="math inline">\(m_*\)</span> and <span class="math inline">\(b_*\)</span> for <span class="math inline">\(m\)</span> and <span class="math inline">\(b\)</span> are <em>exactly the ordinary least squares estimates.</em></p>
<p>As far as <span class="math inline">\(\sigma^2\)</span> is concerned, we find just as above that the maximum likelihood estimate <span class="math inline">\(\sigma^2_*\)</span> is the mean squared error <span class="math display">\[
\sigma^2_*=\frac{1}{N}\sum_{i}(y_i-m_*x_i-b_*)^2.
\]</span></p>
<h2 id="bayesian-inference">Bayesian Inference</h2>
<p>We conclude our review of ideas from probability by examining the Bayesian perspective on data.</p>
<p>Suppose that we wish to conduct an experiment to determine the temperature outside our house. We begin our experiment with a statistical model that is supposed to explain the variability in the results. The model depends on some parameters that we wish to estimate. For example, the parameters of our experiment might be the ‘true’ temperature <span class="math inline">\(t_*\)</span> and the variance <span class="math inline">\(\sigma^2\)</span> of the error.</p>
<p>From the Bayesian point of view, at the beginning of this experiment we have an initial sense of what the temperature is likely to be, expressed in the form of a probability distribution. This initial information is called the <em>prior</em> distribution.</p>
<p>For example, if we know that it’s December in Connecticut, our prior distribution might say that the temperature is more likely to be between 20 and 40 degrees Fahrenheit and is quite unlikely to be higher than 60 or lower than 0. So our prior distribution might look like +<span class="citation" data-cites="fig:tempprior">@fig:tempprior</span>.</p>
<figure>
<img src="img/prior.png" id="fig:tempprior" alt="Prior Distribution on Temperature" /><figcaption aria-hidden="true">Prior Distribution on Temperature</figcaption>
</figure>
<p>If we really have no opinion about the temperature other than its between say, <span class="math inline">\(-20\)</span> and <span class="math inline">\(100\)</span> degrees, our prior distribution might be uniform over that range, as in +<span class="citation" data-cites="fig:uniformprior">@fig:uniformprior</span>.</p>
<figure>
<img src="img/uniform.png" id="fig:uniformprior" alt="Uniform Prior" /><figcaption aria-hidden="true">Uniform Prior</figcaption>
</figure>
<p>The choice of a prior will guide the interpretation of our experiments in ways that we will see shortly.</p>
<p>The next step in our experiment is the collection of data. Suppose we let <span class="math inline">\(\mathbf{t}=(t_1,t_2,\ldots, t_n)\)</span> be a random variable representing <span class="math inline">\(n\)</span> independent measurements of the temperature. We consider the <em>joint distribution</em> of the parameters <span class="math inline">\(t_*\)</span> and <span class="math inline">\(\sigma^2\)</span> and the possible measurements <span class="math inline">\(\mathbf{t}\)</span>: <span class="math display">\[
P(\mathbf{t},t_*,\sigma^2)=\left(\frac{1}{\sigma\sqrt{2\pi}}\right)^{n}e^{-\|\mathbf{t}-t_*\mathbf{e}\|^2/(2\sigma^2)}
\]</span> where <span class="math inline">\(\mathbf{e}=(1,1,\ldots, 1)\)</span>.</p>
<p>The conditional probability <span class="math inline">\(P(t_{*},\sigma^2|\mathbf{t})\)</span> is the distribution of the values of <span class="math inline">\(t_*\)</span> and <span class="math inline">\(\sigma^2\)</span> <em>given</em> a value of the <span class="math inline">\(\mathbf{t}\)</span>. This is what we hope to learn by our experiment – namely, if we make a particular measurement, what does it tell us about <span class="math inline">\(t_*\)</span> and <span class="math inline">\(\sigma^2\)</span>?</p>
<p>Now suppose that we actually make some measurements, and so we obtain a specific set of values <span class="math inline">\(\mathbf{t}_0\)</span> for <span class="math inline">\(\mathbf{t}\)</span>.</p>
<p>By Bayes Theorem, <span class="math display">\[
P(t_{*},\sigma^2|\mathbf{t}=\mathbf{t}_0) = \frac{P(\mathbf{t}=\mathbf{t}_0|t_{*},\sigma^2)P(t_{*},\sigma^2)}{P(\mathbf{t}=\mathbf{t}_0)}
\]</span> We interpret this as follows:</p>
<ul>
<li>the left hand side <span class="math inline">\(P(t_{*},\sigma^2|\mathbf{t}=\mathbf{t}_0)\)</span> is called the <em>posterior distribution</em> and is the distribution of <span class="math inline">\(t_{*}\)</span> and <span class="math inline">\(\sigma^2\)</span> obtained by <em>updating our prior knowledge with the results of our experiment.</em></li>
<li>The probability <span class="math inline">\(P(\mathbf{t}=\mathbf{t}_{0}|t_{*},\sigma^2)\)</span> is the probability of obtaining the measurements we found for a particular value of the parameters <span class="math inline">\(t_{*}\)</span> and <span class="math inline">\(\sigma^2\)</span>.</li>
<li>The probability <span class="math inline">\(P(t_{*},\sigma^2)\)</span> is the <em>prior distribution</em> on the parameters that reflects our initial impression of the distribution of these parameters.</li>
<li>The denominator <span class="math inline">\(P(\mathbf{t}=\mathbf{t}_{0})\)</span> is the total probability of the results that we obtained, and is the integral over the distribution of the parameters weighted by their prior probability: <span class="math display">\[
P(\mathbf{t}=\mathbf{t}_{0})=\int_{t_{*},\sigma^2}P(\mathbf{t}=\mathbf{t}_{0}|t_{*},\sigma^2)P(t_{*},\sigma^2)
\]</span></li>
</ul>
<h3 id="bayesian-experiments-with-the-normal-distribution">Bayesian experiments with the normal distribution</h3>
<p>To illustrate these Bayesian ideas, we’ll consider the problem of measuring the temperature, but for simplicity let’s assume that we fix the variance in our error measurements at <span class="math inline">\(1\)</span> degree. Let’s use the prior distribution on the true temperature that I proposed in +<span class="citation" data-cites="fig:tempprior">@fig:tempprior</span>, which is a normal distribution with variance <span class="math inline">\(15\)</span> “shifted” to be centered at <span class="math inline">\(30\)</span>: <span class="math display">\[
P(t_*)=\left(\frac{1}{\sqrt{2\pi}}\right)e^{-(t_*-30)^2/30}.
\]</span> The expected value <span class="math inline">\(E[t]\)</span> – the mean of the this distribution – is <span class="math inline">\(30\)</span>.</p>
<p>Since the error in our measurements is normally distributed with variance <span class="math inline">\(1\)</span>, we have <span class="math display">\[
P(t-t_{*})=\left(\frac{1}{\sqrt{2\pi}}\right)e^{-(t-t_{*})^2/2}
\]</span> or as a function of the absolute temperature, we have <span class="math display">\[
P(t,t_{*}) = \left(\frac{1}{\sqrt{2\pi}}\right)e^{-(t-t_*)^2/2}.
\]</span></p>
<p>Now we make a bunch of measurements to obtain <span class="math inline">\(\mathbf{t}_0=(t_1,\ldots, t_n)\)</span>. We have <span class="math display">\[
P(\mathbf{t}=\mathbf{t}_0|t_{*}) = \left(\frac{1}{\sqrt{2\pi}}\right)^ne^{-\|\mathbf{t}-t_*\mathbf{e}\|^2/2}.
\]</span></p>
<p>The total probability <span class="math inline">\(T=P(\mathbf{t}=\mathbf{t_0})\)</span> is hard to calculate, so let’s table that for a while. The posterior probability is <span class="math display">\[
P(t_{*}|\mathbf{t}=\mathbf{t}_{0}) = \frac{1}{T} 
\left(\frac{1}{\sqrt{2\pi}}\right)^ne^{-\|\mathbf{t}-t_*\mathbf{e}\|^2/2}
\left(\frac{1}{\sqrt{2\pi}}\right)e^{-(t_*-30)^2/30}.
\]</span></p>
<p>Leaving aside the multiplicative constants for the moment, consider the exponential <span class="math display">\[
e^{-(\|\mathbf{t}-t_{*}\mathbf{e}\|^2/2+(t_{*}-30)^2)/30}.
\]</span> Since <span class="math inline">\(\mathbf{t}\)</span> is a vector of constants – it is a vector of our particular measurements – the exponent <span class="math display">\[
\|\mathbf{t}-t_{*}\mathbf{e}\|^2/2+(t_{*}-30)^2/30 = (t_{*}-30)^2/30+\sum_{i} (t_{i}-t_{*})^2/2
\]</span> is a quadratic polynomial in <span class="math inline">\(t_{*}\)</span> that simplifies: <span class="math display">\[
(t_{*}-30)^2/30+\sum_{i} (t_{i}-t_{*})^2/2 = At_{*}^2+Bt_{*}+C.
\]</span> Here <span class="math display">\[
A=(\frac{1}{30}+\frac{n}{2}),
\]</span> <span class="math display">\[
B=-2-\sum_{i} t_{i}
\]</span> <span class="math display">\[
C=30+\frac{1}{2}\sum_{i} t_{i}^2.
\]</span></p>
<p>We can complete the square to write <span class="math display">\[
At_{*}^2+Bt_{*}+C = (t_{*}-U)^2/2V +K
\]</span> where <span class="math display">\[
U=\frac{2+\sum_{i}t_{i}}{\frac{1}{15}+n}
\]</span> and <span class="math display">\[ 
V=\frac{1}{\frac{1}{15}+n}.
\]</span> So up to constants that don’t involve <span class="math inline">\(t_{*}\)</span>, the posterior density is of the form <span class="math display">\[
e^{(t_{*}-U)^2/2V}
\]</span> and since it is a probability density, the constants must work out to give total integral of <span class="math inline">\(1\)</span>. Therefore the posterior density is a normal distribution centered at <span class="math inline">\(U\)</span> and with variance <span class="math inline">\(V\)</span>. Here <span class="math inline">\(U\)</span> is called the <em>posterior mean</em> and <span class="math inline">\(V\)</span> the <em>posterior variance</em>.</p>
<p>To make this explicit, suppose <span class="math inline">\(n=5\)</span> and we measured the following temperatures: <span class="math display">\[
40, 41,39, 37, 44
\]</span> The mean of these observations is <span class="math inline">\(40.2\)</span> and the variance is <span class="math inline">\(5.4\)</span>.</p>
<p>A calculation shows that the posterior mean is <span class="math inline">\(40.1\)</span> and the posterior variance is <span class="math inline">\(0.2\)</span>. Comparing the prior with the posterior, we obtain the plot in +<span class="citation" data-cites="fig:comparison">@fig:comparison</span>. The posterior has a sharp peak at <span class="math inline">\(40.1\)</span> degrees. This value is just a bit smaller than the mean of the observed temperatures which is <span class="math inline">\(40.2\)</span> degrees. This difference is caused by the prior – our prior distribution said the temperature was likely to be around <span class="math inline">\(30\)</span> degrees, and so the prior pulls the observed mean a bit towards the prior mean taking into account past experience. Because the variance of the prior is large, it has a relatively small influence on the posterior.</p>
<p>The general version of the calculation above is summarized in this proposition.</p>
<p><strong>Proposition:</strong> Suppose that our statistical model for an experiment proposes that the measurements are normally distributed around an (unknown) mean value of <span class="math inline">\(\mu\)</span> with a (fixed) variance <span class="math inline">\(\sigma^2\)</span>. Suppose further that our prior distribution on the unknown mean <span class="math inline">\(\mu\)</span> is normal with mean <span class="math inline">\(\mu_0\)</span> and variance <span class="math inline">\(\tau^2\)</span>. Suppose we make measurements <span class="math display">\[
y_1,\ldots, y_n
\]</span> with mean <span class="math inline">\(\overline{y}\)</span>. Then the posterior distribution of <span class="math inline">\(\mu\)</span> is again normal, with posterior variance <span class="math display">\[
\tau&#39;^2 = \frac{1}{\frac{1}{\tau^2}+\frac{n}{\sigma^2}}
\]</span> and posterior mean <span class="math display">\[
\mu&#39; = \frac{\frac{\mu_0}{\tau^2}+\frac{n}{\sigma^2}\overline{y}}{\frac{1}{\frac{1}{\tau^2}+\frac{n}{\sigma^2}}}
\]</span></p>
<p>So the posterior mean is a sort of weighted average of the sample mean and the prior mean; and as <span class="math inline">\(n\to\infty\)</span>, the posterior mean approaches the sample mean – in other words, as you get more data, the prior has less and less influence on the results of the experiment.</p>
<figure>
<img src="img/priorposterior.png" id="fig:comparison" alt="Prior and Posterior" /><figcaption aria-hidden="true">Prior and Posterior</figcaption>
</figure>
<h3 id="bayesian-coin-flipping">Bayesian coin flipping</h3>
<p>For our final example in this fast overview of ideas from probability, we consider the problem of deciding whether a coin is fair. Our experiment consists of <span class="math inline">\(N\)</span> flips of a coin with unknown probability <span class="math inline">\(p\)</span> of heads, so the data consists of the number <span class="math inline">\(h\)</span> of heads out of the <span class="math inline">\(N\)</span> flips. To apply Bayesian reasoning, we need a prior distribution on <span class="math inline">\(p\)</span>. Let’s first assume that we have no reason to prefer one value of <span class="math inline">\(p\)</span> over another, and so we choose for our prior the uniform distribution on <span class="math inline">\(p\)</span> between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>.</p>
<p>We wish to analyze <span class="math inline">\(P(p|h)\)</span>, the probability distribution of <span class="math inline">\(p\)</span> given <span class="math inline">\(h\)</span> heads out of <span class="math inline">\(N\)</span> flips. Bayes Theorem gives us: <span class="math display">\[
P(p|h) = \frac{P(h|p)P(p)}{P(h)}
\]</span> where <span class="math display">\[
P(h|p) = \binom{N}{h}p^{h}(1-p)^{N-h}
\]</span> and <span class="math display">\[
P(h)=\int_{p=0}^{1} P(h|p)P(p) dp = \binom{N}{h}\int_{p=0}^{1} p^{h}(1-p)^{N-h}dp
\]</span> is a constant which insures that <span class="math display">\[\int_{p}P(p|h)dp=1.\]</span></p>
<p>We see that the posterior distribution <span class="math inline">\(P(p|h)\)</span> is proportional to the polynomial function <span class="math display">\[
P(p|h)\propto p^{h}(1-p)^{N-h}.
\]</span> As in +<span class="citation" data-cites="sec:mlcoin">@sec:mlcoin</span>, we see that this function peaks at <span class="math inline">\(h/N\)</span>. This is called the maximum <em>a posteriori estimate</em> for <span class="math inline">\(p\)</span>.</p>
<p>Another way to summarize the posterior distribution <span class="math inline">\(P(p|h)\)</span> is to look at the expected value of <span class="math inline">\(p\)</span>. This is called the <em>posterior mean</em> of <span class="math inline">\(p\)</span>. To compute it, we need to know the normalization constant in the expression for <span class="math inline">\(P(p|h)\)</span>, and for that we can take advantage of the properties of a special function <span class="math inline">\(B(a,b)\)</span> called the Beta-function: <span class="math display">\[
B(a,b) = \int_{p=0}^{1} p^{a-1}(1-p)^{b-1} dp.
\]</span></p>
<p><strong>Proposition:</strong> If <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> are integers, then <span class="math inline">\(B(a,b)=\frac{a+b}{ab}\frac{1}{\binom{a+b}{a}}\)</span>.</p>
<p><strong>Proof:</strong> Using integration by parts, one can show that <span class="math display">\[
B(a,b)=\frac{a-1}{b}B(a-1,b+1)
\]</span> and a simple calculation shows that <span class="math display">\[
B(1,b) = \frac{1}{b}.
\]</span> Let <span class="math display">\[
H(a,b)=\frac{a+b}{ab}\frac{1}{\binom{a+b}{a}} = \frac{(a-1)!(b-1)!}{(a+b-1)!}
\]</span> Then it’s easy to check that <span class="math inline">\(H\)</span> satsifies the same recurrences as <span class="math inline">\(B(a,b)\)</span>, and that <span class="math inline">\(H(1,b)=1/b\)</span>. So the two functions agree by induction.</p>
<p>Using this Proposition, we see that <span class="math display">\[
P(p|h) = \frac{p^{h}(1-p)^{N-h}}{B(h+1,N-h+1)}
\]</span> and <span class="math display">\[
E[p] = \frac{\int_{p=0}^{1} p^{h+1}(1-p)^{N-h}dp}{B(h+1,N-h+1)}=\frac{B(h+2,N-h+1)}{B(h+1,N-h+1)}.
\]</span> Sorting through this using the formula for <span class="math inline">\(B(a,b)\)</span> we obtain <span class="math display">\[
E[p]=\frac{h+1}{N+2}.
\]</span></p>
<p>So if we obtained <span class="math inline">\(55\)</span> heads out of <span class="math inline">\(100\)</span> flips, the maximum a posteriori estimate for <span class="math inline">\(p\)</span> is <span class="math inline">\(.55\)</span>, while the posterior mean is <span class="math inline">\(56/102=.549\)</span> – just a bit less.</p>
<p>Now suppose that we had some reason to believe that our coin was fair. Then we can choose a prior probability distribution that expresses this. For example, we can choose <span class="math display">\[
P(p) = \frac{1}{B(5,5)}p^{4}(1-p)^{4}.
\]</span> Here we use the Beta function to guarantee that <span class="math inline">\(\int_{0}^{1}P(p)dp=1\)</span>. We show this prior distribution in +<span class="citation" data-cites="fig:betaprior">@fig:betaprior</span>.</p>
<figure>
<img src="img/betaprior.png" id="fig:betaprior" alt="Beta(5,5) Prior" /><figcaption aria-hidden="true">Beta(5,5) Prior</figcaption>
</figure>
<p>If we redo our Bayes theorem calculation, we find that our posterior distribution is <span class="math display">\[
P(p|h) \propto p^{h+4}(1-p)^{N-h+4}
\]</span> and relying again on the Beta function for normalization we have <span class="math display">\[
P(p|h) = \frac{1}{B(h+5,N-h+5)}p^{h+4}(1-p)^{N-h+4}
\]</span> Here the maximum a posterior estimate for <span class="math inline">\(p\)</span> is <span class="math inline">\(h+4/N+8\)</span> while our posterior mean is <span class="math display">\[
\frac{B(h+6,N-h+5)}{B(h+5,N-h+5)} = \frac{h+5}{N+10}.
\]</span></p>
<p>In the situation of <span class="math inline">\(55\)</span> heads out of <span class="math inline">\(100\)</span>, the maximum a posteriori estimate is <span class="math inline">\(.546\)</span> and the posterior mean is <span class="math inline">\(.545\)</span>. These numbers have been pulled just a bit towards <span class="math inline">\(.5\)</span> because our prior knowledge makes us a little bit biased towards <span class="math inline">\(p=.5\)</span>.</p>
<h1 id="principal-component-analysis">Principal Component Analysis</h1>
<h2 id="introduction-2">Introduction</h2>
<p>Suppose that, as usual, we begin with a collection of measurements of different features for a group of samples. Some of these measurements will tell us quite a bit about the difference among our samples, while others may contain relatively little information. For example, if we are analyzing the effect of a certain weight loss regimen on a group of people, the age and weight of the subjects may have a great deal of influence on how successful the regimen is, while their blood pressure might not. One way to help identify which features are more significant is to ask whether or not the feature varies a lot among the different samples. If nearly all the measurements of a feature are the same, it can’t have much power in distinguishing the samples, while if the measurements vary a great deal then that feature has a chance to contain useful information.</p>
<p>In this section we will discuss a way to measure the variability of measurements and then introduce principal component analysis (PCA). PCA is a method for finding which linear combinations of measurements have the greatest variability and therefore might contain the most information. It also allows us to identify combinations of measurements that don’t vary much at all. Combining this information, we can sometimes replace our original system of features with a smaller set that still captures most of the interesting information in our data, and thereby find hidden characteristics of the data and simplify our analysis a great deal.</p>
<h2 id="variance-and-covariance">Variance and Covariance</h2>
<h3 id="variance-1">Variance</h3>
<p>Suppose that we have a collection of measurements <span class="math inline">\((x_1,\ldots, x_n)\)</span> of a particular feature <span class="math inline">\(X\)</span>. For example, <span class="math inline">\(x_i\)</span> might be the initial weight of the <span class="math inline">\(ith\)</span> participant in our weight loss study. The mean of the values <span class="math inline">\((x_1,\ldots, x_n)\)</span> is</p>
<p><span class="math display">\[
\mu_{X} = \frac{1}{n}\sum_{i=1}^{n} x_{i}.
\]</span></p>
<p>The simplest measure of the variability of the data is called its <em>variance.</em></p>
<p><strong>Definition:</strong> The (sample) variance of the data <span class="math inline">\(x_1,\ldots, x_n\)</span> is</p>
<p><span class="math display">\[
\sigma_{X}^2 = \frac{1}{n}\sum_{i=1}^{n} \left(x_{i}-\mu_{X}\right)^2 = \frac{1}{n}\left(\sum_{i=1}^{n} x_{i}^2\right)- \mu_{X}^2
\]</span>{#eq:variance}</p>
<p>The square root of the variance is called the <em>standard deviation.</em></p>
<p>As we see from the formula, the variance is a measure of how ‘spread out’ the data is from the mean.</p>
<p>Recall that in our discussion of linear regression we thought of our set of measurements <span class="math inline">\(x_1,\ldots, x_n\)</span> as a vector – it’s one of the columns of our data matrix. From that point of view, the variance has a geometric interpretation – it is <span class="math inline">\(\frac{1}{N}\)</span> times the square of the distance from the point <span class="math inline">\(X=(x_1,\ldots, x_n)\)</span> to the point <span class="math inline">\(\mu_{X}(1,1,\ldots,1)=\mu_{X}E\)</span>:</p>
<p><span class="math display">\[
\sigma_{X}^2 = \frac{1}{n}(X-\mu_{X}E)\cdot(X-\mu_{X}E)  = \frac{1}{n}\|X-\mu_{X}E\|^2.
\]</span>{#eq:variancedot}</p>
<h3 id="covariance">Covariance</h3>
<p>The variance measures the dispersion of measures of a single feature. Often, we have measurements of multiple features and we might want to know something about how two features are related. The <em>covariance</em> is a measure of whether two features tend to be related, in the sense that when one increases, the other one increases; or when one increases, the other one decreases.</p>
<p><strong>Definition:</strong> Given measurements <span class="math inline">\((x_1,\ldots, x_n)\)</span> and <span class="math inline">\((y_1,\ldots, y_n)\)</span> of two features <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>, the covariance of <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> is</p>
<p><span class="math display">\[
\sigma_{XY} = \frac{1}{N}\sum_{i=1}^{N} (x_i-\mu_{X})(y_i-\mu_{Y})
\]</span>{#eq:covariancedot}</p>
<p>There is a nice geometric interpretation of this, as well, in terms of the dot product. If <span class="math inline">\(X=(x_1,\ldots, x_n)\)</span> and <span class="math inline">\(Y=(y_1\ldots,y_n)\)</span> then</p>
<p><span class="math display">\[
\sigma_{XY} = \frac{1}{N} ((X-\mu_{X}E)\cdot (Y-\mu_{Y}E)).
\]</span></p>
<p>From this point of view, we can see that <span class="math inline">\(\sigma_{XY}\)</span> is positive if the <span class="math inline">\(X-\mu_{X}E\)</span> and <span class="math inline">\(Y-\mu_{Y}E\)</span> vectors “point roughly in the same direction” and its negative if they “point roughly in the opposite direction.”</p>
<h3 id="correlation">Correlation</h3>
<p>One problem with interpreting the variance and covariance is that we don’t have a scale – for example, if <span class="math inline">\(\sigma_{XY}\)</span> is large and positive, then we’d like to say that <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span> are closely related, but it could be just that the entries of <span class="math inline">\(X-\mu_{X}E\)</span> and <span class="math inline">\(Y-\mu_{Y}E\)</span> are large. Here, though, we can really take advantage of the geometric interpretation. Recall that the dot product of two vectors satisfies the formula</p>
<p><span class="math display">\[
a \cdot b = \|a\|\|b\|\cos(\theta)
\]</span></p>
<p>where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span>. So</p>
<p><span class="math display">\[
\cos(\theta) = \frac{a\cdot b}{\|a\|\|b\|}.
\]</span></p>
<p>Let’s apply this to the variance and covariance, by noticing that</p>
<p><span class="math display">\[
\frac{(X-\mu_{X}E)\cdot (Y-\mu_{Y}E)}{\|(X-\mu_{X}E)\|\|(Y-\mu_{Y}E)\|} = \frac{\sigma_{XY}}{\sigma_{XX}\sigma_{YY}}
\]</span></p>
<p>so the quantity</p>
<p><span class="math display">\[
r_{XY} = \frac{\sigma_{XY}}{\sigma_{X}\sigma_{Y}}
\]</span>{#eq:rxy}</p>
<p>measures the cosine of the angle between the vectors <span class="math inline">\(X-\mu_{X}E\)</span> and <span class="math inline">\(Y-\mu_{Y}E\)</span>.</p>
<p><strong>Definition:</strong> The quantity <span class="math inline">\(r_{XY}\)</span> defined in +<span class="citation" data-cites="eq:rxy">@eq:rxy</span> is called the (sample) <em>correlation coefficient</em> between <span class="math inline">\(X\)</span> and <span class="math inline">\(Y\)</span>. We have <span class="math inline">\(0\le |r_{XY}|\le 1\)</span> with <span class="math inline">\(r_{XY}=\pm 1\)</span> if and only if the two vectors <span class="math inline">\(X-\mu_{X}\)</span> and <span class="math inline">\(Y-\mu_{Y}\)</span> are collinear in <span class="math inline">\(\mathbf{R}^{n}\)</span>.</p>
<p>*<span class="citation" data-cites="fig:corrfig">@fig:corrfig</span> illustrates data with different values of the correlation coefficient.</p>
<figure>
<img src="img/correlation.png" id="fig:corrfig" style="width:50.0%" alt="Correlation" /><figcaption aria-hidden="true">Correlation</figcaption>
</figure>
<h3 id="sec:covarmat">The covariance matrix</h3>
<p>In a typical situation we have many features for each of our (many) samples, that we organize into a data matrix <span class="math inline">\(X\)</span>. To recall, each column of <span class="math inline">\(X\)</span> corresponds to a feature that we measure, and each row corresponds to a sample. For example, each row of our matrix might correspond to a person enrolled in a study, and the columns correspond to height (cm), weight (kg), systolic blood pressure, and age (in years):</p>
<table>
<caption>A sample data matrix <span class="math inline">\(X\)</span> {#tbl:data}</caption>
<thead>
<tr class="header">
<th>sample</th>
<th style="text-align: right;">Ht</th>
<th style="text-align: right;">Wgt</th>
<th style="text-align: right;">Bp</th>
<th style="text-align: right;">Age</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>A</td>
<td style="text-align: right;">180</td>
<td style="text-align: right;">75</td>
<td style="text-align: right;">110</td>
<td style="text-align: right;">35</td>
</tr>
<tr class="even">
<td>B</td>
<td style="text-align: right;">193</td>
<td style="text-align: right;">80</td>
<td style="text-align: right;">130</td>
<td style="text-align: right;">40</td>
</tr>
<tr class="odd">
<td>…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
<td style="text-align: right;">…</td>
</tr>
<tr class="even">
<td>U</td>
<td style="text-align: right;">150</td>
<td style="text-align: right;">92</td>
<td style="text-align: right;">105</td>
<td style="text-align: right;">55</td>
</tr>
</tbody>
</table>
<p>If we have multiple features, as in this example, we might be interested in the variance of each feature and all of their mutual covariances. This “package” of information can be obtained “all at once” by taking advantage of some matrix algebra.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X\)</span> be a <span class="math inline">\(N\times k\)</span> data matrix, where the <span class="math inline">\(k\)</span> columns of <span class="math inline">\(X\)</span> correspond to different features and the <span class="math inline">\(N\)</span> rows to different samples. Let <span class="math inline">\(X_{0}\)</span> be the centered version of this data matrix, obtained by subtracting the mean <span class="math inline">\(\mu_{i}\)</span> of column <span class="math inline">\(i\)</span> from all the entries <span class="math inline">\(x_{si}\)</span> in that column. Then the <span class="math inline">\(k\times k\)</span> symmetric matrix</p>
<p><span class="math display">\[
D_{0} = \frac{1}{N}X_{0}^{\intercal}X_{0}
\]</span></p>
<p>is called the (sample) covariance matrix for the data.</p>
<p><strong>Proposition:</strong> The diagonal entries <span class="math inline">\(d_{ii}\)</span> of <span class="math inline">\(D_{0}\)</span> are the variances of the columns of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
d_{ii} = \sigma_{i}^2 = \frac{1}{N}\sum_{s=1}^{N}(x_{si}-\mu_i)^2
\]</span></p>
<p>and the off-diagonal entries <span class="math inline">\(d_{ij} = d_{ji}\)</span> are the covariances of the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> columns of <span class="math inline">\(X\)</span>:</p>
<p><span class="math display">\[
d_{ij} = \sigma_{ij} = \frac{1}{N}\sum_{s=1}^{N}(x_{si}-\mu_{i})(x_{sj}-\mu_{j})
\]</span></p>
<p>The sum of the diagonal entries, the trace of <span class="math inline">\(D_{0}\)</span> is the <strong>total</strong> variance of the data.</p>
<p><strong>Proof:</strong> This follows from the definitions, but it’s worth checking the details, which we leave as an exercise.</p>
<h3 id="sec:visualizecovar">Visualizing the covariance matrix</h3>
<p>If the number of features in the data is not too large, a density matrix plot provides a tool for visualizing the covariance matrix of the data. A density matrix plot is an <span class="math inline">\(k\times k\)</span> grid of plots (where <span class="math inline">\(k\)</span> is the number of features). The entry with <span class="math inline">\((i,j)\)</span> coordinates in the grid is a scatter plot of the <span class="math inline">\(i^{th}\)</span> feature against the <span class="math inline">\(j^{th}\)</span> one if <span class="math inline">\(i\not=j\)</span>, and is a histogram of the <span class="math inline">\(i^{th}\)</span> variable if <span class="math inline">\(i=j\)</span>.</p>
<p>*<span class="citation" data-cites="fig:density0">@fig:density0</span> is an example of a density matrix plot for a dataset with <span class="math inline">\(50\)</span> samples and <span class="math inline">\(2\)</span> features. This data has been centered, so it can be represented in a <span class="math inline">\(50\times 2\)</span> data matrix <span class="math inline">\(X_{0}\)</span>. The upper left and lower right graphs are scatter plots of the two columns, while the lower left and upper right are the histograms of the columns.</p>
<figure>
<img src="img/density2x2.png" id="fig:density0" style="width:50.0%" alt="Density Matrix Plot" /><figcaption aria-hidden="true">Density Matrix Plot</figcaption>
</figure>
<h3 id="linear-combinations-of-features-scores">Linear Combinations of Features (Scores)</h3>
<p>Sometimes useful information about our data can be revealed if we combine different measurements together to obtain a “hybrid” measure that captures something interesting. For example, in the Auto MPG dataset that we studied in the section on Linear Regression, we looked at the influence of both vehicle weight <span class="math inline">\(w\)</span> and engine displacement <span class="math inline">\(e\)</span> on gas mileage; perhaps their is some value in considering a hybrid “score” defined as <span class="math display">\[
S = aw + be
\]</span> for some constants <span class="math inline">\(a\)</span> and <span class="math inline">\(b\)</span> – maybe by choosing a good combination we could find a better predictor of gas mileage than using one or the other of the features individually.</p>
<p>As another example, suppose we are interested in the impact of the nutritional content of food on weight gain in a study. We know that both calorie content and the level dietary fiber contribute to the weight gain of participants eating this particular food; maybe there is some kind of combined “calorie/fiber” score we could introduce that captures the impact of that food better.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(X_{0}\)</span> be a (centered) <span class="math inline">\(N\times k\)</span> data matrix giving information about <span class="math inline">\(k\)</span> features for each of <span class="math inline">\(N\)</span> samples. A linear synthetic feature, or a linear score, is a linear combination of the <span class="math inline">\(k\)</span> features. The linear score is defined by constants <span class="math inline">\(a_{1},\ldots, a_{k}\)</span> so that If <span class="math inline">\(y_{1},\ldots, y_{k}\)</span> are the values of the features for a particular sample, then the linear score for that sample is</p>
<p><span class="math display">\[
S = a_{1}y_{1}+a_{2}y_{2}+\cdots+a_{k}y_{k}
\]</span></p>
<p><strong>Lemma:</strong> The values of the linear score for each of the <span class="math inline">\(N\)</span> samples can be calculated as</p>
<p><span class="math display">\[
\left[\begin{matrix} S_{1} \\ \vdots \\ S_{N}\\ \end{matrix}\right] =
X_{0}\left[
\begin{matrix} a_{1} \\ \vdots \\ a_{k}\end{matrix}\right].
\]</span>{#eq:linearscore}</p>
<p><strong>Proof:</strong> Multiplying a matrix by a column vector computes a linear combination of the columns – that’s what this lemma says. Exercise 3 asks you to write out the indices and make sure you believe this.</p>
<h3 id="mean-and-variance-of-scores">Mean and variance of scores</h3>
<p>When we combine features to make a hybrid score, we assume that the features were centered to begin with, so that each features has mean zero. As a result, the mean of the hybrid features is again zero.</p>
<p><strong>Lemma:</strong> A linear combination of features with mean zero again has mean zero.</p>
<p><strong>Proof:</strong> Let <span class="math inline">\(S_{i}\)</span> be the score for the <span class="math inline">\(i^{th}\)</span> sample, so <span class="math display">\[
S_{i} = \sum_{j=1}^{k} x_{ij}a_{j}.
\]</span> where <span class="math inline">\(X_{0}\)</span> has entries <span class="math inline">\(x_{ij}\)</span>. Then the mean value of the score is <span class="math display">\[
\mu_{S} = \frac{1}{k}\sum_{i=1}^{N} S_{i} = \frac{1}{N}\sum_{i=1}^{N}\sum_{j=1}^{k} x_{ij}a_{j}.
\]</span> Reversing the order of the sum yields <span class="math display">\[
\mu_{S} = \frac{1}{N}\sum_{j=1}^{k}\sum_{i=1}^{N} x_{ij}a_{j} = \sum_{j=1}^{k} a_{j}\frac{1}{N}(\sum_{i=1}^{N} x_{ij})=
\sum_{j=1}^{k}a_{j}\mu_{j}=0
\]</span> where <span class="math inline">\(\mu_{j}=0\)</span> is the mean of the <span class="math inline">\(j^{th}\)</span> feature (column) of <span class="math inline">\(X_{0}\)</span>.</p>
<p>The variance is more interesting, and gives us an opportunity to put the covariance matrix to work. Remember from <span class="citation" data-cites="eq:variancedot">@eq:variancedot</span> that, since a score <span class="math inline">\(S\)</span> has mean zero, it’s variance is <span class="math inline">\(\sigma_{S}^2=\frac{1}{N}S\cdot S\)</span> – where here the score <span class="math inline">\(S\)</span> is represented by the column vector with entries <span class="math inline">\(S_{1},\ldots S_{k}\)</span> as in +<span class="citation" data-cites="eq:linearscore">@eq:linearscore</span>.</p>
<p><strong>Lemma:</strong> The variance of the score <span class="math inline">\(S\)</span> with weights <span class="math inline">\(a_1,\ldots a_k\)</span> is <span class="math display">\[
\sigma_{S}^2 = a^{\intercal}D_{0}a = \left[\begin{matrix}a_{1} &amp; \cdots &amp; a_{k}\end{matrix}\right]D_{0}
\left[\begin{matrix} a_{1} \\ \vdots \\ a_{k}\end{matrix}\right]
\]</span>{#eq:ada} More generally, if <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(S_{2}\)</span> are scores with weights <span class="math inline">\(a_1,\ldots, a_k\)</span> and <span class="math inline">\(b_1,\ldots, b_k\)</span> respectively, then the covariance <span class="math inline">\(\sigma_{S_{1}S_{2}}\)</span> is <span class="math display">\[
\sigma_{S_{1}S_{2}} = a^{\intercal}D_{0}b.
\]</span></p>
<p><strong>Proof:</strong> From +<span class="citation" data-cites="eq:variancedot">@eq:variancedot</span> and <span class="citation" data-cites="eq:linearscore">@eq:linearscore</span> we know that <span class="math display">\[
\sigma_{S}^2 = \frac{1}{N}S\cdot S
\]</span> and <span class="math display">\[
S = X_{0}a.
\]</span> Since <span class="math inline">\(\frac{1}{N}S\cdot S = \frac{1}{N}S^{\intercal}S\)</span>, this gives us <span class="math display">\[
\frac{1}{N}\sigma_{S}^2 = \frac{1}{N}(X_{0}a)^{\intercal}(X_{0}a) = \frac{1}{N}a^{\intercal}X_{0}^{\intercal}X_{0}a = a^{\intercal}D_{0}a
\]</span> as claimed.</p>
<p>For the covariance, use a similar argument with +<span class="citation" data-cites="eq:covariancedot">@eq:covariancedot</span> and +<span class="citation" data-cites="eq:linearscore">@eq:linearscore</span>. writing <span class="math inline">\(\sigma_{S_{1}S_{2}}=\frac{1}{N}S_{1}\cdot S_{2}\)</span> and the fact that <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(S_{2}\)</span> can be written as <span class="math inline">\(X_{0}a\)</span> and <span class="math inline">\(X_{0}b\)</span>.</p>
<p>The point of this lemma is that the covariance matrix contains not just the variances and covariances of the original features, but also enough information to construct the variances and covariances for <em>any linear combination of features.</em></p>
<p>In the next section we will see how to exploit this idea to reveal hidden structure in our data.</p>
<h3 id="geometry-of-scores">Geometry of Scores</h3>
<p>Let’s return to the dataset that we looked at in +<span class="citation" data-cites="sec:visualizecovar">@sec:visualizecovar</span>. We simplify the density matrix plot in +<span class="citation" data-cites="fig:pcasimfig">@fig:pcasimfig</span>, which shows one of the scatter plots and the two histograms.</p>
<p>The scatter plot shows that the data points are arranged in a more or less elliptical cloud oriented at an angle to the <span class="math inline">\(xy\)</span>-axes which represent the two given features. The two individual histograms show the distribution of the two features – each has mean zero, with the <span class="math inline">\(x\)</span>-features distributed between <span class="math inline">\(-2\)</span> and <span class="math inline">\(2\)</span> and the <span class="math inline">\(y\)</span> feature between <span class="math inline">\(-4\)</span> and <span class="math inline">\(4\)</span>. Looking just at the two features individually, meaning only at the two histograms, we can’t see the overall elliptical structure.</p>
<figure>
<img src="img/PCAsimulated-1.png" id="fig:pcasimfig" style="width:50.0%" alt="Simulated Data with Two Features" /><figcaption aria-hidden="true">Simulated Data with Two Features</figcaption>
</figure>
<p>How can we get a better grip on our data in this situation? We can try to find a “direction” in our data that better illuminates the variation of the data. For example, suppose that we pick a unit vector at the origin pointing in a particular direction in our data. See +<span class="citation" data-cites="fig:pcasimfig-1">@fig:pcasimfig-1</span>.</p>
<figure>
<img src="img/PCAsimulated-2.png" id="fig:pcasimfig-1" style="width:50.0%" alt="A direction in the data" /><figcaption aria-hidden="true">A direction in the data</figcaption>
</figure>
<p>Now we can orthogonally project the datapoints onto the line defined by this vector, as shown in +<span class="citation" data-cites="fig:pcasimfig-2">@fig:pcasimfig-2</span>.</p>
<figure>
<img src="img/PCAsimulated-3.png" id="fig:pcasimfig-2" style="width:50.0%" alt="Projecting the datapoints" /><figcaption aria-hidden="true">Projecting the datapoints</figcaption>
</figure>
<p>Recall that if the unit vector is defined by coordinates <span class="math inline">\(u=[u_0,u_1]\)</span>, then the orthogonal projection of the point <span class="math inline">\(x\)</span> with coordinates <span class="math inline">\((x_0,x_1)\)</span> is <span class="math inline">\((x\cdot u)u\)</span>. Now <span class="math display">\[
x\cdot u = u_0 x_0 + u_1 x_1
\]</span> so the coordinates of the points along the line defined by <span class="math inline">\(u\)</span> are the values of the score <span class="math inline">\(Z\)</span> defined by <span class="math inline">\(u=[u_0,u_1]\)</span>. Using our work in the previous section, we see that we can find all of these coordinates by matrix multiplication: <span class="math display">\[
Z = X_0 u
\]</span> where <span class="math inline">\(X_0\)</span> is our data matrix. Now let’s add a histogram of the values of <span class="math inline">\(Z\)</span> to our picture:</p>
<figure>
<img src="img/PCAsimulated-4.png" id="fig:pcasimfig-3" style="width:50.0%" alt="Distribution of Z" /><figcaption aria-hidden="true">Distribution of Z</figcaption>
</figure>
<p>This histogram shows the distribution of the values of <span class="math inline">\(Z\)</span> along the tilted line defined by the unit vector <span class="math inline">\(u\)</span>.</p>
<p>Finally, using our work on the covariance matrix, we see that the variance of <span class="math inline">\(Z\)</span> is given by <span class="math display">\[
\sigma_{Z}^2 = \frac{1}{50}u^{\intercal}X_{0}^{\intercal}X_{0}u = u^{\intercal}D_{0}u
\]</span> where <span class="math inline">\(D_{0}\)</span> is the covariance matrix of the data <span class="math inline">\(X_{0}\)</span>.</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(X_{0}\)</span> be a <span class="math inline">\(N\times k\)</span> centered data matrix, and let <span class="math inline">\(D_{0}=\frac{1}{N}X_{0}^{\intercal}X_{0}\)</span> be the associated covariance matrix. Let <span class="math inline">\(u\)</span> be a unit vector in “feature space” <span class="math inline">\(\mathbf{R}^{k}\)</span>. Then the score <span class="math inline">\(S=X_{0}u\)</span> can be interpreted as the coordinates of the points of <span class="math inline">\(X_{0}\)</span> projected onto the line generated by <span class="math inline">\(u\)</span>. The variance of this score is <span class="math display">\[
\sigma^{2}_{S} = u^{\intercal}D_{0}u = \sum_{i=1}^{N} s_{i}^2
\]</span> where <span class="math inline">\(s_{i} = X_{0}[i,:]u\)</span> is the dot product of the <span class="math inline">\(i^{th}\)</span> row <span class="math inline">\(X_{0}[i,:]\)</span> with <span class="math inline">\(u\)</span>. It measures the variability in the data “in the direction of the unit vector <span class="math inline">\(u\)</span>”.</p>
<h2 id="principal-components">Principal Components</h2>
<h3 id="change-of-variance-with-direction">Change of variance with direction</h3>
<p>As we’ve seen in the previous section, if we choose a unit vector <span class="math inline">\(u\)</span> in the feature space and find the projection <span class="math inline">\(X_{0}u\)</span> of our data onto the line through <span class="math inline">\(u\)</span>, we get a “score” that we can use to measure the variance of the data in the direction of <span class="math inline">\(u\)</span>. What happens as we vary <span class="math inline">\(u\)</span>?</p>
<p>To study this question, let’s continue with our simulated data from the previous section, and introduce a unit vector <span class="math display">\[
u(\theta) = \left[\begin{matrix} \cos(\theta) &amp; \sin(\theta)\end{matrix}\right].
\]</span> This is in fact a unit vector, since <span class="math inline">\(\sin^2(\theta)+\cos^2(\theta)=1\)</span>, and it is oriented at an angle <span class="math inline">\(\theta\)</span> from the <span class="math inline">\(x\)</span>-axis.</p>
<p>The variance of the data in the direction of <span class="math inline">\(u(\theta)\)</span> is given by <span class="math display">\[
\sigma_{\theta}^2 = u(\theta)^{\intercal}D_{0}u(\theta).
\]</span></p>
<p>A plot of this function for the data we have been considering is in +<span class="citation" data-cites="fig:pcatheta">@fig:pcatheta</span>. As you can see, the variance goes through two full periods with the angle, and it reaches a maximum and minimum value at intervals of <span class="math inline">\(\pi/2\)</span> – so the two angles where the variance are maximum and minimum are orthogonal to one another.</p>
<figure>
<img src="img/PCAtheta.png" id="fig:pcatheta" style="width:25.0%" alt="Change of variance with angle theta" /><figcaption aria-hidden="true">Change of variance with angle theta</figcaption>
</figure>
<p>The two directions where the variance is maximum and minimum are drawn on the original data scatter plot in +<span class="citation" data-cites="fig:pcaprincipal">@fig:pcaprincipal</span> .</p>
<figure>
<img src="img/PCAprincipal.png" id="fig:pcaprincipal" style="width:25.0%" alt="Data with principal directions" /><figcaption aria-hidden="true">Data with principal directions</figcaption>
</figure>
<p>Let’s try to understand why this is happening.</p>
<h3 id="sec:extremalvariance">Directions of extremal variance</h3>
<p>Given our centered, <span class="math inline">\(N\times i\)</span> data matrix <span class="math inline">\(X_{0}\)</span>, with its associated covariance matrix <span class="math inline">\(D_{0}=\frac{1}{N}X_{0}^{\intercal}X_{0}\)</span>, we would like to find unit vectors <span class="math inline">\(u\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span> so that <span class="math display">\[
\sigma_{u}^{2} = u^{\intercal}D_{0}u
\]</span> reaches its maximum and its minimum. Here <span class="math inline">\(\sigma_{u}^2\)</span> is the variance of the “linear score” <span class="math inline">\(X_{0}u\)</span> and it represents how dispersed the data is in the “u direction” in <span class="math inline">\(\mathbf{R}^{k}\)</span>.</p>
<p>In this problem, remember that the coordinates of <span class="math inline">\(u=(u_1,\ldots, u_{k})\)</span> are the variables and the symmetric matrix <span class="math inline">\(D_{0}\)</span> is given. As usual, we to find the maximum and minimum values of <span class="math inline">\(\sigma_{u}^{2}\)</span>, we should look at the partial derivatives of <span class="math inline">\(\sigma_{u}^{2}\)</span> with respect to the variables <span class="math inline">\(u_{i}\)</span> and set them to zero. Here, however, there is a catch – we want to restrict <span class="math inline">\(u\)</span> to being a unit vector, with <span class="math inline">\(u\cdot u =\sum u_{i}^2=1\)</span>.</p>
<p>So this is a <em>constrained optimization problem</em>:</p>
<ul>
<li>Find extreme values of the function <span class="math display">\[
\sigma_{u}^{2} = u^{\intercal}D_{0}u
\]</span></li>
<li>Subject to the constraint <span class="math inline">\(\|u\|^2 = u\cdot u=1\)</span> (or <span class="math inline">\(u\cdot u-1=0\)</span>)</li>
</ul>
<p>We will use the technique of <em>Lagrange Multipliers</em> to solve such a problem.</p>
<p>To apply this method, we introduce the function</p>
<p><span class="math display">\[
S(u, \lambda) = u^{\intercal}D_{0}u - \lambda(u\cdot u -1)
\]</span>{#eq:lagrange}</p>
<p>Then we compute the gradient</p>
<p><span class="math display">\[
\nabla S = \left[\begin{matrix} \frac{\partial S}{\partial u_{1}} \\ \vdots \\ \frac{\partial S}{\partial u_{k}} \\ \frac{\partial S}{\partial \lambda}\end{matrix}\right]
\]</span>{#eq:lagrangegradient}</p>
<p>and solve the system of equations <span class="math inline">\(\nabla S=0\)</span>. Here we have written the gradient as a column vector for reasons that will become clearer shortly.</p>
<p>Computing all of these partial derivatives looks messy, but actually if we take advantage of matrix algebra it’s not too bad. The following two lemmas explain how to do this.</p>
<p><strong>Lemma</strong>: Let <span class="math inline">\(M\)</span> be a <span class="math inline">\(N\times k\)</span> matrix with constant coefficients and let <span class="math inline">\(u\)</span> be a <span class="math inline">\(k\times 1\)</span> column vector whose entries are <span class="math inline">\(u_1,\ldots u_{k}\)</span>. The function <span class="math inline">\(F(u) = Mu\)</span> is a linear map from <span class="math inline">\(\mathbf{R}^{k}\to\mathbf{R}^{N}\)</span>. Its (total) derivative is a linear map between the same vector spaces, and satisfies <span class="math display">\[
D(F)(v) = Mv
\]</span> for any <span class="math inline">\(k\times 1\)</span> vector <span class="math inline">\(v\)</span>. If <span class="math inline">\(u\)</span> is a <span class="math inline">\(1\times N\)</span> matrix, and <span class="math inline">\(G(u) = uM\)</span>, then <span class="math display">\[
D(G)(v) = vM
\]</span></p>
<p>for any <span class="math inline">\(1\times N\)</span> vector <span class="math inline">\(v\)</span>. (This is the matrix version of the derivative rule that <span class="math inline">\(\frac{d}{dx}(ax)=a\)</span> for a constant <span class="math inline">\(a\)</span>.)</p>
<p><strong>Proof:</strong> Since <span class="math inline">\(F:\mathbf{R}^{k}\to\mathbf{R}^{N}\)</span>, we can write out <span class="math inline">\(F\)</span> in more traditional function notation as <span class="math display">\[
F(u) = (F_{1}(u_1,\ldots, u_k), \ldots, F_{N}(u_1,\ldots, u_{k})
\]</span> where <span class="math display">\[
F_{i}(u_1,\ldots u_k) = \sum_{j=1}^{k} m_{ij}u_{j}.
\]</span> Thus <span class="math inline">\(\frac{\partial F_{i}}{\partial u_{j}} = m_{ij}\)</span>. The total derivative <span class="math inline">\(D(F)\)</span> is the linear map with matrix <span class="math display">\[
D(F)_{ij} = \frac{\partial F_{i}}{\partial u_{j}} = m_{ij}
\]</span> and so <span class="math inline">\(D(F)=M\)</span>.</p>
<p>The other result is proved the same way.</p>
<p><strong>Lemma</strong>: Let <span class="math inline">\(D\)</span> be a symmetric <span class="math inline">\(k\times k\)</span> matrix with constant entries and let <span class="math inline">\(u\)</span> be an <span class="math inline">\(k\times 1\)</span> column vector of variables <span class="math inline">\(u_{1},\ldots, u_{k}\)</span>. Let <span class="math inline">\(F:\mathbf{R}^{k}\to R\)</span> be the function <span class="math inline">\(F(u) = u^{\intercal}Du\)</span>. Then the gradient <span class="math inline">\(\nabla_{u} F\)</span> is a vector field – that is, a vector-valued function of <span class="math inline">\(u\)</span>, and is given by the formula <span class="math display">\[
\nabla_{u} F = 2Du
\]</span></p>
<p><strong>Proof:</strong> Let <span class="math inline">\(d_{ij}\)</span> be the <span class="math inline">\(i,j\)</span> entry of <span class="math inline">\(D\)</span>. We can write out the function <span class="math inline">\(F\)</span> to obtain <span class="math display">\[
F(u_1,\ldots, u_{k}) = \sum_{i=1}^{k} \sum_{j=1}^{k} u_i d_{ij} u_j.
\]</span> Now <span class="math inline">\(\frac{\partial F}{\partial u_{i}}\)</span> is going to pick out only terms where <span class="math inline">\(u_{i}\)</span> appears, yielding: <span class="math display">\[
\frac{\partial F}{\partial u_{i}} = \sum_{j=1}^{k} d_{ij}u_{j} + \sum_{j=1}^{k} u_{j}d_{ji}
\]</span> Here the first sum catches all of the terms where the first “u” is <span class="math inline">\(u_{i}\)</span>; and the second sum catches all the terms where the second “u” is <span class="math inline">\(u_{i}\)</span>. The diagonal terms <span class="math inline">\(u_{i}^2d_{ii}\)</span> contribute once to each sum, which is consistent with the rule that the derivative of <span class="math inline">\(u_{i}^2d_{ii} = 2u_{i}d_{ii}\)</span>. To finish the proof, notice that <span class="math display">\[
\sum_{j=1}^{k} u_{j}d_{ji} = \sum_{j=1}^{k} d_{ij}u_{j} 
\]</span> since <span class="math inline">\(D\)</span> is symmetric, so in fact the two terms are the same Thus <span class="math display">\[
\df{u_{i}}F = 2\sum_{j=1}^{k} d_{ij}u_{j}
\]</span> But the right hand side of this equation is twice the <span class="math inline">\(i^{th}\)</span> entry of <span class="math inline">\(Du\)</span>, so putting the results together we get <span class="math display">\[
\nabla_{u}F = \left[\begin{matrix} \frac{\partial F}{\partial u_{1}} \\ \vdots \\ \frac{\partial F}{\partial u_{k}}\end{matrix}\right] = 2Du.
\]</span></p>
<p>The following theorem puts all of this work together to reduce our questions about how variance changes with direction.</p>
<h3 id="sec:critvals">Critical values of the variance</h3>
<p><strong>Theorem:</strong> The critical values of the variance <span class="math inline">\(\sigma_{u}^2\)</span>, as <span class="math inline">\(u\)</span> varies over unit vectors in <span class="math inline">\(\mathbf{R}^{N}\)</span>, are the eigenvalues <span class="math inline">\(\lambda_{1},\ldots,\lambda_{k}\)</span> of the covariance matrix <span class="math inline">\(D\)</span>, and if <span class="math inline">\(e_{i}\)</span> is a unit eigenvector corresponding to <span class="math inline">\(\lambda_{i}\)</span>, then <span class="math inline">\(\sigma_{e_{i}}^2 = \lambda_{i}\)</span>.</p>
<p><strong>Proof:</strong> Recall that we introduced the Lagrange function <span class="math inline">\(S(u,\lambda)\)</span>, whose critical points give us the solutions to our constrained optimization problem. As we said in +<span class="citation" data-cites="eq:lagrange">@eq:lagrange</span>: <span class="math display">\[
S(u,\lambda) = u^{\intercal}D_{0}u - \lambda(u\cdot u - 1) = u^{\intercal}D_{0}u -\lambda(u\cdot u) + \lambda
\]</span> Now apply our Matrix calculus lemmas. First, let’s treat <span class="math inline">\(\lambda\)</span> as a constant and focus on the <span class="math inline">\(u\)</span> variables. We can write <span class="math inline">\(u\cdot u = u^{\intercal} I_{N} u\)</span> where <span class="math inline">\(I_{N}\)</span> is the identity matrix to compute: <span class="math display">\[
\nabla_{u} S = 2D_{0}u -2\lambda u
\]</span> For <span class="math inline">\(\lambda\)</span> we have <span class="math display">\[
\df{\lambda}S = -u\cdot u +1.
\]</span> The critical points occur when <span class="math display">\[
\nabla_{u} S = 2(D_{0}-\lambda)u = 0
\]</span> and <span class="math display">\[
\df{\lambda}S = 1-u\cdot u = 0
\]</span> The first equation says that <span class="math inline">\(\lambda\)</span> must be an eigenvalue, and <span class="math inline">\(u\)</span> an eigenvector: <span class="math display">\[
D_{0}u = \lambda u
\]</span> while the second says <span class="math inline">\(u\)</span> must be a unit vector <span class="math inline">\(u\cdot u=\|u\|^2=1\)</span>. The second part of the result follows from the fact that if <span class="math inline">\(e_{i}\)</span> is a unit eigenvector with eigenvalue <span class="math inline">\(\lambda_{i}\)</span> then <span class="math display">\[
\sigma_{e_{i}}^2 = e_{i}^{\intercal}D_{0}e_{i} = \lambda_{i}\|e_{i}\|^2=\lambda_{i}.
\]</span></p>
<p>To really make this result pay off, we need to recall some key facts about the eigenvalues and eigenvectors of symmetric matrices. Because these facts are so central to this result, and to other applications throughout machine learning and mathematics generally, we provide proofs in +<span class="citation" data-cites="sec:spectraltheorem">@sec:spectraltheorem</span>.</p>
<hr />
<table>
<caption>Properties of Eigenvalues of Real Symmetric Matrices {#tbl:symmmat}</caption>
<colgroup>
<col style="width: 100%" />
</colgroup>
<thead>
<tr class="header">
<th style="text-align: left;">Summary</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td style="text-align: left;">1. All of the eigenvalues <span class="math inline">\(\lambda_{1},\ldots, \lambda_{l}\)</span> of <span class="math inline">\(D\)</span> are real. If <span class="math inline">\(u^{\intercal}Du\ge 0\)</span> for all <span class="math inline">\(u\in\mathbf{R}^{k}\)</span>, then all eigenvalues <span class="math inline">\(\lambda_{i}\)</span> are non-negative. In the latter case we say that <span class="math inline">\(D\)</span> is <em>positive semi-definite.</em></td>
</tr>
<tr class="even">
<td style="text-align: left;">2. If <span class="math inline">\(v\)</span> is an eigenvector for <span class="math inline">\(D\)</span> with eigenvalue <span class="math inline">\(\lambda\)</span>, and <span class="math inline">\(w\)</span> is an eigenvector with a different eigenvalue <span class="math inline">\(\lambda&#39;\)</span>, then <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> are orthogonal: <span class="math inline">\(v\cdot w = 0\)</span>.</td>
</tr>
<tr class="odd">
<td style="text-align: left;">3. There is an orthonormal basis <span class="math inline">\(u_{1},\ldots, u_{k}\)</span> of <span class="math inline">\(\mathbf{R}^{k}\)</span> made up of eigenvectors of <span class="math inline">\(D\)</span> corresponding to the eigenvalues <span class="math inline">\(\lambda_{i}\)</span>.</td>
</tr>
<tr class="even">
<td style="text-align: left;">4. Let <span class="math inline">\(\Lambda\)</span> be the diagonal matrix with entries <span class="math inline">\(\lambda_{1},\ldots, \lambda_{N}\)</span> and let <span class="math inline">\(P\)</span> be the matrix whose columns are made up of the vectors <span class="math inline">\(u_{i}\)</span>. Then <span class="math inline">\(D = P\Lambda P^{\intercal}.\)</span></td>
</tr>
</tbody>
</table>
<hr />
<p>If we combine this theorem with the facts summarized in +<span class="citation" data-cites="tbl:symmmat">@tbl:symmmat</span> then we get a complete picture. Let <span class="math inline">\(D_{0}\)</span> be the covariance matrix of our data. Since <span class="math display">\[
\sigma_{u}^2 = u^{\intercal}D_{0}u\ge 0 \hbox{(it&#39;s a sum of squares)}
\]</span> we know that the eigenvalues <span class="math inline">\(\lambda_{1}\ge\lambda_{2}\ge \cdots \ge \lambda_{k}\ge 0\)</span> are all nonnegative. Choose a corresponding sequence <span class="math inline">\(u_{1},\ldots u_{k}\)</span> of orthogonal eigenvectors where all <span class="math inline">\(\|u_{i}\|^2=1\)</span>. Since the <span class="math inline">\(u_{i}\)</span> form a basis of <span class="math inline">\(\mathbf{R}^{N}\)</span>, any score is a linear combination of the <span class="math inline">\(u_{i}\)</span>: <span class="math display">\[
S = \sum_{i=1}^{k} a_{i}u_{i}.
\]</span> Since <span class="math inline">\(u_{i}^{\intercal}D_{0}u_{j} = \lambda_{j}u_{i}^{\intercal}u_{j} = 0\)</span> unless <span class="math inline">\(i=j\)</span>, in which case it is <span class="math inline">\(\lambda_{i}\)</span>, we can compute <span class="math display">\[
\sigma_{S}^2 = \sum_{i=1}^{k} \lambda_{i}a_{i}^2,
\]</span> and <span class="math inline">\(\|S\|^2=\sum_{i=1}^{k} a_{i}^2\)</span> since the <span class="math inline">\(u_{i}\)</span> are an orthonormal set. So in these coordinates, our optimization problem is:</p>
<ul>
<li>maximize <span class="math inline">\(\sum \lambda_{i}a_{i}^2\)</span></li>
<li>subject to the constraint <span class="math inline">\(\sum a_{i}^2 = 1\)</span>.</li>
</ul>
<p>We don’t need any fancy math to see that the maximum happens when <span class="math inline">\(a_{1}=1\)</span> and the other <span class="math inline">\(a_{j}=0\)</span>, and in that case, the maximum is <span class="math inline">\(\lambda_{1}\)</span>. (If <span class="math inline">\(\lambda_{1}\)</span> occurs more than once, there may be a whole subspace of directions where the variance is maximal). Similarly, the minimum value is <span class="math inline">\(\lambda_{k}\)</span> and occurs when <span class="math inline">\(a_{k}=1\)</span> and the others are zero.</p>
<h3 id="sec:subspaces">Subspaces of extremal variance</h3>
<p>We can generalize the idea of the variance of our data in a particular direction to a higher dimensional version of <em>total variance</em> in a subspace. Suppose that <span class="math inline">\(E\)</span> is a subspace of <span class="math inline">\(\mathbf{R}^{k}\)</span> and <span class="math inline">\(U\)</span> is a matrix whose columns span <span class="math inline">\(E\)</span> – the columns of <span class="math inline">\(U\)</span> are the weights of a family of scores that span <span class="math inline">\(E\)</span>. The values of these scores are <span class="math inline">\(XU\)</span> and the covariance matrix of this projected data is <span class="math display">\[\frac{1}{N}U^{\intercal}X^{\intercal}XU=U^{\intercal}D_{0}U.\]</span>.</p>
<p>Finally, the <em>total variance</em> <span class="math inline">\(\sigma_{E}^2\)</span> of the data projected into <span class="math inline">\(E\)</span> is the sum of the diagonal entries of the matrix</p>
<p><span class="math display">\[
\sigma^2_{E} = \mathop{trace}(U^{\intercal}D_{0}U)
\]</span></p>
<p>Just as the variance in a given direction <span class="math inline">\(u\)</span> depends on the scaling of <span class="math inline">\(u\)</span>, the variance in a subspace depends on the scaling of the columns of <span class="math inline">\(U\)</span>. To normalize this scaling, we assume that the columns of <span class="math inline">\(U\)</span> are an orthonormal basis of the subspace <span class="math inline">\(E\)</span>.</p>
<p>Now we can generalize the question asked in +<span class="citation" data-cites="sec:extremalvariance">@sec:extremalvariance</span> by seeking, not just a vector <span class="math inline">\(u\)</span> pointing in the direction of the extremal variance, but instead the <em>subspace</em> <span class="math inline">\(U_{s}\)</span> of dimension <span class="math inline">\(s\)</span> with the property that the total variance of the projection of the data into <span class="math inline">\(U_{s}\)</span> is maximal compared to its projection into other subspaces of that dimension. This is called a <em>subspace of extremal variance.</em></p>
<p>To make this concrete, suppose we consider a subspace <span class="math inline">\(E\)</span> of <span class="math inline">\(\mathbf{R}^{k}\)</span> of dimension <span class="math inline">\(t\)</span> with basis <span class="math inline">\(w_{1},\ldots, w_{t}\)</span>. Complete this to a basis <span class="math inline">\(w_{1},\ldots, w_{t},w_{t+1},\ldots, w_{k}\)</span> of <span class="math inline">\(\mathbf{R}^{k}\)</span> and then apply the Gram Schmidt Process (see +<span class="citation" data-cites="sec:gsprocess">@sec:gsprocess</span>) to find an orthonormal basis <span class="math inline">\(w&#39;_{1},\ldots,w&#39;_{s},w&#39;_{s+1},\ldots, w&#39;_{k}\)</span> where the <span class="math inline">\(w&#39;_{1},\ldots, w&#39;_{t}\)</span> are an orthonormal basis for <span class="math inline">\(E\)</span>. Let <span class="math inline">\(W\)</span> be the <span class="math inline">\(k\times t\)</span> matrix whose columns are the <span class="math inline">\(w&#39;_{i}\)</span> for <span class="math inline">\(i=1,\ldots,t\)</span>. The rows of the matrix <span class="math inline">\(X_{0}W\)</span> given the coordinates of the projection of each sample into the subspace <span class="math inline">\(E\)</span> expressed in terms of the scores corresponding to these vectors <span class="math inline">\(w&#39;_{i}\)</span>. The total variance of these projections is</p>
<p><span class="math display">\[
\sigma_{E}^2 = \sum_{i=1}^{t} \|X_{0}w&#39;_{i}\|^2 = \sum_{i=1}^{t} (w&#39;_{i})^{\intercal}X_{0}^{\intercal}X_{0}w&#39;_{i}  = \sum_{i=1}^{t} (w&#39;_{i})^{\intercal}D_{0}w&#39;_{i}
\]</span></p>
<p>If we want to maximize this, we have the constrained optimization problem of finding <span class="math inline">\(w&#39;_{1},\ldots, w&#39;_{t}\)</span> so that</p>
<ul>
<li><span class="math inline">\(\sum_{i=1}^{t} (w&#39;_{i})^{\intercal}D_{0}w&#39;_{i}\)</span> is maximal</li>
<li>subject to the constraint that each <span class="math inline">\(w_{i}\)</span> has <span class="math inline">\(\|w&#39;_{i}\|^2=1\)</span>,</li>
<li>and that the <span class="math inline">\(w&#39;_{i}\)</span> are orthogonal, meaning <span class="math inline">\(w&#39;_{i}\cdot w&#39;_{j}=0\)</span> for <span class="math inline">\(i\not=j\)</span>,</li>
<li>and that the <span class="math inline">\(w&#39;_{i}\)</span> are linearly independent.</li>
</ul>
<p>Then the span <span class="math inline">\(E\)</span> of these <span class="math inline">\(w&#39;_{i}\)</span> is subspace of extremal variance.</p>
<p><strong>Theorem:</strong> A <span class="math inline">\(t\)</span>-dimensional subspace <span class="math inline">\(E\)</span> is a subspace of extremal variance if and only if it is spanned by <span class="math inline">\(t\)</span> orthonormal eigenvectors of the matrix <span class="math inline">\(D_{0}\)</span> corresponding to the <span class="math inline">\(t\)</span> largest eigenvalues for <span class="math inline">\(D_{0}\)</span>.</p>
<p><strong>Proof:</strong> We can approach this problem using Lagrange multipliers and matrix calculus if we are careful. Our unknown is <span class="math inline">\(k\times t\)</span> matrix <span class="math inline">\(W\)</span> whose columns are the <span class="math inline">\(t\)</span> (unknown) vectors <span class="math inline">\(w&#39;_{i}\)</span>. The objective function that we are seeking to maximize is <span class="math display">\[
F = \mathop{trace}(W^{\intercal}D_{0}W) = \sum_{i=1}^{t} (w&#39;_{i})^{\intercal}D_{0}w_{i}.
\]</span> The constraints are the requirements that <span class="math inline">\(\|w&#39;_{i}\|^2=1\)</span> and <span class="math inline">\(w&#39;_{i}\cdot w&#39;_{j}=0\)</span> if <span class="math inline">\(i\not=j\)</span>. If we introduction a matrix of lagrange multipliers <span class="math inline">\(\Lambda=(\lambda_{ij})\)</span>, where <span class="math inline">\(\lambda_{ij}\)</span> is the multiplier that goes with the the first of these constraints when <span class="math inline">\(i=j\)</span>, and the second when <span class="math inline">\(i\not=j\)</span>, we can express our Lagrange function as: <span class="math display">\[
S(W,\Lambda) = \mathop{trace}(W^{\intercal}D_{0}W) - (W^{\intercal}W-I)\Lambda
\]</span> where <span class="math inline">\(I\)</span> is the <span class="math inline">\(t\times t\)</span> identity matrix.</p>
<p>Taking the derivatives with respect to the entries of <span class="math inline">\(W\)</span> and of <span class="math inline">\(\Lambda\)</span> yields the following two equations: <span class="math display">\[\begin{align*}
D_{0}W &amp;= W\Lambda \\
W^{\intercal}W &amp;= I \\
\end{align*}\]</span></p>
<p>The first of these equations says that the space <span class="math inline">\(E\)</span> spanned by the columns of <span class="math inline">\(W\)</span> is <em>invariant</em> under <span class="math inline">\(D_{0}\)</span>, while the second says that the columns of <span class="math inline">\(W\)</span> form an orthonormal basis.</p>
<p>Let’s assume for the moment that we have a matrix <span class="math inline">\(W\)</span> that satisfies these conditions.<br />
Then it must be the case that <span class="math inline">\(\Lambda\)</span> is a symmetric, real valued <span class="math inline">\(t\times t\)</span> matrix, since <span class="math display">\[
W^{\intercal}D_{0}W = W^{\intercal}W\Lambda = \Lambda.
\]</span> and the matrix on the left is symmetric.</p>
<p>By the properties of real symmetric matrices (the spectral theorem), there are orthonormal vectors <span class="math inline">\(q_{1},\ldots q_{t}\)</span> that are eigenvectors of <span class="math inline">\(\Lambda\)</span> with corresponding eigenvalues <span class="math inline">\(\tau_{i}\)</span>. If we let <span class="math inline">\(Q\)</span> be the matrix whose columns are the vectors <span class="math inline">\(q_{i}\)</span> and let <span class="math inline">\(T\)</span> be the diagonal <span class="math inline">\(t\times t\)</span> matrix whose entries are the <span class="math inline">\(\tau_{i}\)</span>, we have <span class="math display">\[
\Lambda Q = QT.
\]</span></p>
<p>If we go back to our original equations, we see that if <span class="math inline">\(W\)</span> exists such that <span class="math inline">\(DW=W\Lambda\)</span>, then there is a matrix <span class="math inline">\(Q\)</span> with orthonormal columns and a diagonal matrix <span class="math inline">\(T\)</span> such that <span class="math display">\[
D_{0}WQ = W\Lambda Q = W Q T.
\]</span> In other words, <span class="math inline">\(WQ\)</span> is a matrix whose columns are eigenvectors of <span class="math inline">\(D_{0}\)</span> with eigenvalues <span class="math inline">\(\tau_{i}\)</span> for <span class="math inline">\(i=1,\ldots, t\)</span>.</p>
<p>Thus we see how to construct an invariant subspace <span class="math inline">\(E\)</span> and a solution matrix <span class="math inline">\(W\)</span>. Such an <span class="math inline">\(E\)</span> is spanned by <span class="math inline">\(t\)</span> orthonormal eigenvectors <span class="math inline">\(q_{i}\)</span> with eigenvalues <span class="math inline">\(\tau_{i}\)</span> of <span class="math inline">\(D_{0}\)</span>; and <span class="math inline">\(W\)</span> is is the matrix whose columns are the <span class="math inline">\(q_{i}\)</span>. Further, in that case, the total variance associated to <span class="math inline">\(E\)</span> is the sum of the eigenvalues <span class="math inline">\(\tau_{i}\)</span>; to make this as large as possible, we should choose our eigenvectors to correspond to <span class="math inline">\(t\)</span> of the largest eigenvalues of <span class="math inline">\(D_{0}\)</span>. This concludes the proof.</p>
<h3 id="definition-of-principal-components">Definition of Principal Components</h3>
<p><strong>Definition:</strong> The orthonormal unit eigenvectors <span class="math inline">\(u_{i}\)</span> for <span class="math inline">\(D_{0}\)</span> are the <em>principal directions</em> or <em>principal components</em> for the data <span class="math inline">\(X_{0}\)</span>.</p>
<p><strong>Theorem:</strong> The maximum variance occurs in the principal direction(s) associated to the largest eigenvalue, and the minimum variance in the principal direction(s) associated with the smallest one. The covariance between scores in principal directions associatedwith different eigenvalues is zero.</p>
<p>At this point, the picture in +<span class="citation" data-cites="fig:pcaprincipal">@fig:pcaprincipal</span> makes sense – the red and green dashed lines are the principal directions, they are orthogonal to one another, and the point in the directions where the data is most (and least) “spread out.”</p>
<p><strong>Proof:</strong> The statement about the largest and smallest eigenvalues is proved at the very end of the last section. The covariance of two scores corresponding to different eigenvectors <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(u_{j}\)</span> is <span class="math display">\[u_{i}^{\intercal}D_{0}u_{j} = \lambda_{j}(u_{i}\cdot u_{j}) = 0\]</span> since the <span class="math inline">\(u_{i}\)</span> and <span class="math inline">\(u_{j}\)</span> are orthogonal.</p>
<p>Sometimes the results above are presented in a slightly different form, and may be referred to, in part, as Rayleigh’s theorem.</p>
<p><strong>Corollary:</strong> (Rayleigh’s Theorem) Let <span class="math inline">\(D\)</span> be a real symmetric matrix and let <span class="math display">\[
H(v) = \max_{v\not = 0}\frac{v^{\intercal}Dv}{v^{\intercal}v}.
\]</span> Then <span class="math inline">\(H(v)\)</span> is the largest eigenvalue of <span class="math inline">\(D\)</span>. (Similarly, if we replace <span class="math inline">\(\max\)</span> by <span class="math inline">\(\min\)</span>, then the minimum is the least eigenvalue).</p>
<p><strong>Proof:</strong> The maximum of the function <span class="math inline">\(H(v)\)</span> is the solution to the same optimization problem that we considered above.</p>
<p><strong>Exercises.</strong></p>
<ol type="1">
<li><p>Prove that the two expressions for <span class="math inline">\(\sigma_{X}^2\)</span> given in +<span class="citation" data-cites="eq:variance">@eq:variance</span> are the same.</p></li>
<li><p>Prove that the covariance matrix is as described in the proposition in <span class="citation" data-cites="sec:covarmat">@sec:covarmat</span>.</p></li>
<li><p>Let <span class="math inline">\(X_{0}\)</span> be a <span class="math inline">\(k\times N\)</span> matrix with entries <span class="math inline">\(x_{ij}\)</span> for <span class="math inline">\(1\le i\le k\)</span> and <span class="math inline">\(1\le j\le N\)</span>. If a linear score is defined by the constants <span class="math inline">\(a_{1},\ldots a_{N}\)</span>, check that equation +<span class="citation" data-cites="eq:linearscore">@eq:linearscore</span> holds as claimed.</p></li>
<li><p>Why is it important to use a unit vector when computing the variance of <span class="math inline">\(X_{0}\)</span> in the direction of <span class="math inline">\(u\)</span>? Suppose <span class="math inline">\(v=\lambda u\)</span> where <span class="math inline">\(u\)</span> is a unit vector and <span class="math inline">\(\lambda&gt;0\)</span> is a constant. Let <span class="math inline">\(S&#39;\)</span> be the score <span class="math inline">\(X_{0}v\)</span>. How is the variance of <span class="math inline">\(S&#39;\)</span> related to that of <span class="math inline">\(S=X_{0}u\)</span>?</p></li>
</ol>
<h2 id="dimensionality-reduction-via-principal-components">Dimensionality Reduction via Principal Components</h2>
<p>The principal components associated with a dataset separate out directions in the feature space in which the data is most (or least) variable. One of the main applications of this information is to enable us to take data with a great many features – a set of points in a high dimensional space – and, by focusing our attention on the scores corresponding to the principal directions, capture most of the information in the data in a much lower dimensional setting.</p>
<p>To illustrate how this is done, let <span class="math inline">\(X\)</span> be a <span class="math inline">\(N\times k\)</span> data matrix, let <span class="math inline">\(X_{0}\)</span> be its centered version, and let <span class="math inline">\(D_{0} = \frac{1}{N}X_{0}^{\intercal}X\)</span> be the associated covariance matrix.</p>
<p>Apply the spectral theorem (described in +<span class="citation" data-cites="tbl:symmmat">@tbl:symmmat</span>) and proved in +<span class="citation" data-cites="sec:spectraltheorem">@sec:spectraltheorem</span> to the covariance matrix to obtain eigenvalues <span class="math inline">\(\lambda_{1}\ge \lambda_{2}\ge\cdots \lambda_{k}\ge 0\)</span> and associated eigenvectors <span class="math inline">\(u_{1},\ldots, u_{k}\)</span>. The scores <span class="math inline">\(S_{i}=X_{0}u_{i}\)</span> give the values of the data in the principal directions. The variance of <span class="math inline">\(S_{i}\)</span> is <span class="math inline">\(\lambda_{i}\)</span>.</p>
<p>Now choose a number <span class="math inline">\(t&lt;k\)</span> and consider the vectors <span class="math inline">\(S_{1},\ldots, S_{t}\)</span>. The <span class="math inline">\(j^{th}\)</span> entry in <span class="math inline">\(S_{i}\)</span> is the value of the score <span class="math inline">\(S_{i}\)</span> for the <span class="math inline">\(j^{th}\)</span> data point. Because <span class="math inline">\(S_{1},\ldots, S_{t}\)</span> capture the most significant variability in the original data, we can learn a lot about our data by considering just these <span class="math inline">\(t\)</span> features of the data, instead of needing all <span class="math inline">\(N\)</span>.</p>
<p>To illustrate, let’s look at an example. We begin with a synthetic dataset <span class="math inline">\(X_{0}\)</span> which has <span class="math inline">\(200\)</span> samples and <span class="math inline">\(15\)</span> features. The data (some of it) for some of the samples is shown in +<span class="citation" data-cites="tbl:rawdata">@tbl:rawdata</span>.</p>
<table>
<caption>Simulated Data for PCA Analysis {#tbl:rawdata}</caption>
<thead>
<tr class="header">
<th></th>
<th style="text-align: left;">f-0</th>
<th style="text-align: left;">f-1</th>
<th style="text-align: left;">f-2</th>
<th style="text-align: left;">f-3</th>
<th style="text-align: left;">f-4</th>
<th>...</th>
<th style="text-align: left;">f-10</th>
<th style="text-align: left;">f-11</th>
<th style="text-align: left;">f-12</th>
<th style="text-align: left;">f-13</th>
<th style="text-align: left;">f-14</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>s-0</td>
<td style="text-align: left;">1.18</td>
<td style="text-align: left;">-0.41</td>
<td style="text-align: left;">2.02</td>
<td style="text-align: left;">0.44</td>
<td style="text-align: left;">2.24</td>
<td>...</td>
<td style="text-align: left;">0.32</td>
<td style="text-align: left;">0.95</td>
<td style="text-align: left;">0.88</td>
<td style="text-align: left;">1.10</td>
<td style="text-align: left;">0.89</td>
</tr>
<tr class="even">
<td>s-1</td>
<td style="text-align: left;">0.74</td>
<td style="text-align: left;">0.58</td>
<td style="text-align: left;">1.54</td>
<td style="text-align: left;">0.23</td>
<td style="text-align: left;">2.05</td>
<td>...</td>
<td style="text-align: left;">0.99</td>
<td style="text-align: left;">1.14</td>
<td style="text-align: left;">1.56</td>
<td style="text-align: left;">0.99</td>
<td style="text-align: left;">0.59</td>
</tr>
<tr class="odd">
<td>...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td>...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
<td style="text-align: left;">...</td>
</tr>
<tr class="even">
<td>s-198</td>
<td style="text-align: left;">1.04</td>
<td style="text-align: left;">2.02</td>
<td style="text-align: left;">1.44</td>
<td style="text-align: left;">0.40</td>
<td style="text-align: left;">1.33</td>
<td>...</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">0.62</td>
<td style="text-align: left;">0.54</td>
<td style="text-align: left;">1.96</td>
<td style="text-align: left;">0.04</td>
</tr>
<tr class="odd">
<td>s-199</td>
<td style="text-align: left;">0.92</td>
<td style="text-align: left;">2.09</td>
<td style="text-align: left;">1.58</td>
<td style="text-align: left;">1.19</td>
<td style="text-align: left;">1.17</td>
<td>...</td>
<td style="text-align: left;">0.42</td>
<td style="text-align: left;">0.85</td>
<td style="text-align: left;">0.83</td>
<td style="text-align: left;">2.22</td>
<td style="text-align: left;">0.90</td>
</tr>
</tbody>
</table>
<p>The full dataset is a <span class="math inline">\(200\times 15\)</span> matrix; it has <span class="math inline">\(3000\)</span> numbers in it and we’re not really equipped to make sense of it. We could try some graphing – for example, +<span class="citation" data-cites="fig:features">@fig:features</span> shows a scatter plot of two of the features plotted against each other.</p>
<figure>
<img src="img/features.png" id="fig:features" style="width:50.0%" alt="Scatter Plot of Two Features" /><figcaption aria-hidden="true">Scatter Plot of Two Features</figcaption>
</figure>
<p>Unfortunately there’s not much to see in +<span class="citation" data-cites="fig:features">@fig:features</span> – just a blob – because the individual features of the data don’t tell us much in isolation, whatever structure there is in this data arises out of the relationship between different features.</p>
<p>In +<span class="citation" data-cites="fig:densitygrid">@fig:densitygrid</span> we show a “density grid” plot of the data. The graph in position <span class="math inline">\(i,j\)</span> shows a scatter plot of the <span class="math inline">\(i^{th}\)</span> and <span class="math inline">\(j^{th}\)</span> columns of the data, except in the diagonal positions, where in position <span class="math inline">\(i,i\)</span> we plot a histogram of column <span class="math inline">\(i\)</span>. There’s not much structure visible; it is a lot of blobs.</p>
<figure>
<img src="img/density.png" id="fig:densitygrid" style="width:50.0%" alt="Density Grid Plot of All Features" /><figcaption aria-hidden="true">Density Grid Plot of All Features</figcaption>
</figure>
<p>So let’s apply the theory of principal components. We use a software package to compute the eigenvalues and eigenvectors of the matrix <span class="math inline">\(D_{0}\)</span>. The <span class="math inline">\(15\)</span> eigenvalues <span class="math inline">\(\lambda_{1}\ge \cdots \ge \lambda_{15}\)</span> are plotted, in descending order, in +<span class="citation" data-cites="fig:eigenvalues">@fig:eigenvalues</span> .</p>
<figure>
<img src="img/eigenvalues.png" id="fig:eigenvalues" style="width:50.0%" alt="Eigenvalues of the Covariance Matrix" /><figcaption aria-hidden="true">Eigenvalues of the Covariance Matrix</figcaption>
</figure>
<p>This plot shows that the first <span class="math inline">\(4\)</span> eigenvalues are relatively large, while the remaining <span class="math inline">\(11\)</span> are smaller and not much different from each other. We interpret this as saying that <em>most of the variation in the data is accounted for by the first four principal components.</em> We can even make this quantitative. The <em>total variance</em> of the data is the sum of the eigenvalues of the covariance matrix – the trace of <span class="math inline">\(D_{0}\)</span> – and in this example that sum is around <span class="math inline">\(5\)</span>. The sum of the first <span class="math inline">\(4\)</span> eigenvalues is about <span class="math inline">\(4\)</span>, so the first four eignvalues account for about <span class="math inline">\(4/5\)</span> of the total variance, or about <span class="math inline">\(80\%\)</span> of the variation of the data.</p>
<p>Now let’s focus in on the two largest eigenvalues <span class="math inline">\(\lambda_{1}\)</span> and <span class="math inline">\(\lambda_{2}\)</span> and their corresponding eigenvectors <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span>. The <span class="math inline">\(200\times 1\)</span> column vectors <span class="math inline">\(S_{1}=X_{0}u_{1}\)</span> and <span class="math inline">\(S_{2}=X_{0}u_{2}\)</span> are the values of the scores associated with these two eigenvectors. So for each data point (each row of <span class="math inline">\(X_{0}\)</span>) we have two values (the corresponding entries of <span class="math inline">\(S_{1}\)</span> and <span class="math inline">\(S_{2}\)</span>.) In +<span class="citation" data-cites="fig:principalvalues">@fig:principalvalues</span> we show a scatter plot of these scores.</p>
<figure>
<img src="img/pcadimred.png" id="fig:principalvalues" style="width:50.0%" alt="Scatter Plot of Scores in the First Two Principal Directions" /><figcaption aria-hidden="true">Scatter Plot of Scores in the First Two Principal Directions</figcaption>
</figure>
<p>Notice that suddenly some structure emerges in our data! We can see that the 200 points are separated into five clusters, distinguished by the values of their scores! This ability to find hidden structure in complicated data, is one of the most important applications of principal components.</p>
<p>If we were dealing with real data, we would now want to investigate the different groups of points to see if we can understand what characteristics the principal components have identified.</p>
<h3 id="loadings">Loadings</h3>
<p>There’s one last piece of the PCA puzzle that we are going to investigate. In +<span class="citation" data-cites="fig:principalvalues">@fig:principalvalues</span>, we plotted our data points in the coordinates given by the first two principal components. In geometric terms, we took the cloud of <span class="math inline">\(200\)</span> points in <span class="math inline">\(\mathbf{R}^{15}\)</span> given by the rows of <span class="math inline">\(X_{0}\)</span> and projected those points into the two dimensional plane spanned by the eigenvectors <span class="math inline">\(u_{1}\)</span> and <span class="math inline">\(u_{2}\)</span>, and then plotted the distribution of the points in that plane.</p>
<p>More generally, suppose we take our dataset <span class="math inline">\(X_{0}\)</span> and consider the first <span class="math inline">\(t\)</span> principal components corresponding to the eigenvectors <span class="math inline">\(u_{1},\ldots, u_{t}\)</span>. The projection of the data into the space spanned by these eigenvectors is the represented by the <span class="math inline">\(S = k\times t\)</span> matrix <span class="math inline">\(X_{0}U\)</span> where <span class="math inline">\(U\)</span> is the <span class="math inline">\(k\times t\)</span> matrix whose columns are the eigenvectors <span class="math inline">\(u_{i}\)</span>. Each row of <span class="math inline">\(S\)</span> gives the values of the score arising from <span class="math inline">\(u_{i}\)</span> in the <span class="math inline">\(i^{th}\)</span> column for <span class="math inline">\(i=1,\ldots, t\)</span>.</p>
<p>The remaining question that we wish to consider is: how can we see some evidence of the original features in subspace? We can answer this by imagining that we had an artificial sample <span class="math inline">\(x\)</span> that has a measurement of <span class="math inline">\(1\)</span> for the <span class="math inline">\(i^{th}\)</span> feature and a measurement of zero for all the other features. The corresponding point is represented by a <span class="math inline">\(1\times k\)</span> row vector with a <span class="math inline">\(1\)</span> in position <span class="math inline">\(i\)</span>. The projection of this synthetic sample into the span of the first <span class="math inline">\(t\)</span> principal components is the <span class="math inline">\(1\times t\)</span> vector <span class="math inline">\(xU\)</span>. Notice, however, that <span class="math inline">\(xU\)</span> is just the <span class="math inline">\(i^{th}\)</span> row of the matrix <span class="math inline">\(U\)</span>. This vector in the space spanned by the <span class="math inline">\(u_{i}\)</span> is called the “loading” of the <span class="math inline">\(i^{th}\)</span> feature in the principal components.</p>
<p>This is illustrated in +<span class="citation" data-cites="fig:loadings">@fig:loadings</span>, which shows a line along the direction of the loading corresponding to the each feature added to the scatter plot of the data in the plane spanned by the first two principal components. One observation one can make is that some of the features are more “left to right”, like features <span class="math inline">\(7\)</span> and <span class="math inline">\(8\)</span>, while others are more “top to bottom”, like <span class="math inline">\(6\)</span>. So points that lie on the left side of the plot have smaller values of features <span class="math inline">\(7\)</span> and <span class="math inline">\(8\)</span>, while those at the top of the plot have larger values of feature <span class="math inline">\(6\)</span>.</p>
<figure>
<img src="img/loading.png" id="fig:loadings" style="width:50.0%" alt="Loadings in the Principal Component Plane" /><figcaption aria-hidden="true">Loadings in the Principal Component Plane</figcaption>
</figure>
<p>In the next, and last, section, of this discussion of Principal Component Analysis, we will give proofs of the key mathematical ideas summarized earlier in +<span class="citation" data-cites="tbl:symmmat">@tbl:symmmat</span>, which have been central to this analysis.</p>
<h3 id="sec:svd">The singular value decomposition</h3>
<p>The singular value decomposition is a slightly different way of looking at principal components. Let <span class="math inline">\(\Lambda\)</span> be the diagonal matrix of eigenvalues of <span class="math inline">\(D_{0}\)</span>; we know that the entries of <span class="math inline">\(D_{0}\)</span> are non-negative. Let’s drop the eigenvectors corresponding to the zero eigenvalue. Let’s say that there are <span class="math inline">\(s\)</span> non-zero eigenvalues, and <span class="math inline">\(s\)</span> corresponding eigenvectors.</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(P&#39;\)</span> be the <span class="math inline">\(N\times s\)</span> matrix whose columns are the eigenvectors with non-zero eigenvalues, and let <span class="math inline">\(\Lambda_{+}\)</span> be the <span class="math inline">\(s\times s\)</span> diagonal matrix whose entries are the non-zero eigenvalues. Then <span class="math inline">\(P&#39;\Lambda_{+} P&#39;^{\intercal} = P\Lambda P^{\intercal} = D_{0}\)</span>.</p>
<p><strong>Proof:</strong> First observe that <span class="math inline">\(P&#39;\Lambda_{+}P&#39;^{\intercal}\)</span> is in fact an <span class="math inline">\(N\times N\)</span> matrix. Then look at the block structure to verify the result.</p>
<p>The matrix <span class="math inline">\(\Lambda_{+}\)</span> is diagonal, invertible, and, since the eigenvalues are positive, it makes sense to consider the real matrix <span class="math inline">\(\Lambda_{+}^{1/2}\)</span> whose entries are the square roots of the eigenvalues.</p>
<p>Let <span class="math inline">\(U = X_{0}P&#39;\Lambda_{+}^{-1/2}\)</span>. Note that <span class="math inline">\(U\)</span> is a <span class="math inline">\(k\times s\)</span> dimensional matrix.</p>
<p><strong>Lemma:</strong> The columns of <span class="math inline">\(U\)</span> are orthonormal.</p>
<p><strong>Proof:</strong> Compute the <span class="math inline">\(s\times s\)</span> matrix <span class="math inline">\(U^{\intercal}U\)</span>, whose entries are all of the dot products of the columns of <span class="math inline">\(U\)</span>: <span class="math display">\[
\begin{aligned}
U^{\intercal}U &amp;=&amp; \Lambda_{+}^{-1/2}P&#39;^{\intercal}X_{0}^{\intercal}X_{0}P&#39;\Lambda_{+}^{-1/2} \\
&amp;=&amp; \Lambda_{+}^{-1/2}P&#39;^{\intercal}P&#39;\Lambda_{+}P&#39;^{\intercal}P&#39;\Lambda_{+}^{-1/2} \\
&amp;=&amp; I_{s} \\
\end{aligned}
\]</span> by the previous lemma and the fact that <span class="math inline">\(P&#39;P&#39;^{\intercal}\)</span> is the <span class="math inline">\(s\times s\)</span> identity matrix.</p>
<p>Rearranging this yields the singular value decomposition.</p>
<p><strong>Theorem:</strong> (The singular value decomposition) The matrix <span class="math inline">\(X_{0}\)</span> has a decomposition: <span class="math display">\[
X_{0} = U\Lambda_{+}^{-1/2}P&#39;^{\intercal}
\]</span> where <span class="math inline">\(U\)</span> (of dimension <span class="math inline">\(k\times s\)</span>) and <span class="math inline">\(P&#39;\)</span> (of dimension <span class="math inline">\(N\times s\)</span>) are orthogonal, and <span class="math inline">\(\Lambda_{+}\)</span> (of dimension <span class="math inline">\(s\times s\)</span>) is diagonal with positive entries. Furthermore, the entries of <span class="math inline">\(\Lambda_{+}\)</span> are the non-negative eigenvalues of <span class="math inline">\(D_{0}=X_{0}^{\intercal}X_{0}\)</span>, and <span class="math inline">\(U\)</span> and <span class="math inline">\(P&#39;\)</span> are uniquely determined by <span class="math inline">\(X_{0}\)</span>.</p>
<p><strong>Proof:</strong> We won’t work through all of this, as it is a reinterpretation of our work on principal components.</p>
<p><strong>Remark:</strong> The entries of <span class="math inline">\(\Lambda_{+}^{-1/2}\)</span> are called the singular values of <span class="math inline">\(X_{0}\)</span>. They can be found directly by considering the optimization problem implicitly equivalent to the problem we solved in +<span class="citation" data-cites="sec:subspaces">@sec:subspaces</span>.</p>
<h2 id="sec:spectraltheorem">Eigenvalues and Eigenvectors of Real Symmetric Matrices (The Spectral Theorem)</h2>
<p>Now that we’ve shown how to apply the theory of eigenvalues and eigenvectors of symmetric matrices to extract principal directions from data, and to use those principal directions to find structure, we will give a proof of the properties that we summarized in +<span class="citation" data-cites="tbl:symmmat">@tbl:symmmat</span>.</p>
<p>A key tool in the proof is the Gram-Schmidt orthogonalization process.</p>
<h3 id="sec:gsprocess">Gram-Schmidt</h3>
<p><strong>Proposition (Gram-Schmidt Process):</strong> Let <span class="math inline">\(w_{1},\ldots, w_{k}\)</span> be a collection of linearly independent vectors in <span class="math inline">\(\mathbf{R}^{N}\)</span> and let <span class="math inline">\(W\)</span> be the span of the <span class="math inline">\(w_{i}\)</span>. Let <span class="math inline">\(u_{1} = w_{1}\)</span> and let <span class="math display">\[
u_{i} = w_{i} - \sum_{j=1}^{i-1} \frac{w_{i}\cdot u_{j}}{u_{j}\cdot u_{j}}u_{j}
\]</span> for <span class="math inline">\(i=2,\ldots, k\)</span>. Then</p>
<ul>
<li>The vectors <span class="math inline">\(u_{i}\)</span> are orthogonal: <span class="math inline">\(u_{i}\cdot u_{j}=0\)</span> unless <span class="math inline">\(i=j\)</span>.</li>
<li>The vectors <span class="math inline">\(u_{i}\)</span> span <span class="math inline">\(W\)</span>.</li>
<li>Each <span class="math inline">\(u_{i}\)</span> is orthogonal to the all of <span class="math inline">\(w_{1},\ldots, w_{i-1}\)</span>.</li>
<li>The vectors <span class="math inline">\(u&#39;_{i} = u_{i}/\|u_{i}\|\)</span> are orthonormal.</li>
</ul>
<p><strong>Proof:</strong> This is an inductive exercise, and we leave it to you to work out the details.</p>
<h3 id="the-spectral-theorem">The spectral theorem</h3>
<p><strong>Theorem:</strong> Let <span class="math inline">\(D\)</span> be a real symmetric <span class="math inline">\(N\times N\)</span> matrix. Then:</p>
<ol type="1">
<li>All of the <span class="math inline">\(N\)</span> eigenvalues <span class="math inline">\(\lambda_1\ge \lambda_2\ge \cdots \ge \lambda_{N}\)</span> are real. If <span class="math inline">\(u^{\intercal}Du\ge 0\)</span> for all <span class="math inline">\(u\in\mathbf{R}^{N}\)</span>, then all eigenvalues <span class="math inline">\(\lambda_{i}\ge 0\)</span>.</li>
<li>The matrix <span class="math inline">\(D\)</span> is diagonalizable – that is, it has <span class="math inline">\(N\)</span> linearly independent eigenvectors.</li>
<li>If <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> are eigenvectors corresponding to eigenvalues <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\lambda&#39;\)</span>, with <span class="math inline">\(\lambda\not=\lambda&#39;\)</span>, then <span class="math inline">\(v\)</span> and <span class="math inline">\(w\)</span> are orthogonal: <span class="math inline">\(v\cdot w=0\)</span>.</li>
<li>There is an orthonormal basis <span class="math inline">\(u_{1},\ldots, u_{N}\)</span> of <span class="math inline">\(\mathbf{R}^{N}\)</span> made up of eigenvectors for the eigenvalues <span class="math inline">\(\lambda_{i}\)</span>.</li>
<li>Let <span class="math inline">\(\Lambda\)</span> be the diagonal matrix with entries <span class="math inline">\(\lambda_{1},\ldots, \lambda_{N}\)</span> and let <span class="math inline">\(P\)</span> be the matrix whose columns are made up of the eigenvectors <span class="math inline">\(u_{i}\)</span>. Then <span class="math inline">\(D=P\Lambda P^{\intercal}\)</span>.</li>
</ol>
<p><strong>Proof:</strong> Start with <span class="math inline">\(1\)</span>. Suppose that <span class="math inline">\(\lambda\)</span> is an eigenvalue of <span class="math inline">\(D\)</span>. Let <span class="math inline">\(u\)</span> be a corresponding nonzero eigenvector. Then <span class="math inline">\(Du=\lambda u\)</span> and <span class="math inline">\(D\overline{u}=\overline{\lambda}\overline{u}\)</span>, where <span class="math inline">\(\overline{u}\)</span> is the vector whose entries are the conjugates of the entries of <span class="math inline">\(u\)</span> (and <span class="math inline">\(\overline{D}=D\)</span> since <span class="math inline">\(D\)</span> is real). Now we have <span class="math display">\[
\overline{u}^{\intercal}Du = \lambda \overline{u}\cdot u = \lambda\|u\|^2
\]</span> and <span class="math display">\[
u^{\intercal}D\overline{u} = \overline{\lambda}u\cdot \overline{u} = \overline{\lambda}\|u\|^2.
\]</span> But the left hand side of both of these equations are the same (take the transpose and use the symmetry of <span class="math inline">\(D\)</span>) so we must have <span class="math inline">\(\lambda\|u\|^2 = \overline{\lambda}\|u\|^2\)</span> so <span class="math inline">\(\lambda=\overline{\lambda}\)</span>, meaning <span class="math inline">\(\lambda\)</span> is real.</p>
<p>If we have the additional property that <span class="math inline">\(u^{\intercal}Du\ge 0\)</span> for all <span class="math inline">\(u\)</span>, then in particular <span class="math inline">\(u_{i}^{\intercal}Du_{i} = \lambda\|u\|^2\ge 0\)</span>, and since <span class="math inline">\(\|u\|^2&gt; 0\)</span> we must have <span class="math inline">\(\lambda\ge 0\)</span>.</p>
<p>Property <span class="math inline">\(2\)</span> is in some ways the most critical fact. We know from the general theory of the characteristic polynomial, and the fundamental theorem of algebra, that <span class="math inline">\(D\)</span> has <span class="math inline">\(N\)</span> complex eigenvalues, although some may be repeated. However, it may not be the case that <span class="math inline">\(D\)</span> has <span class="math inline">\(N\)</span> linearly independent eigenvectors – it may not be <em>diagonalizable</em>. So we will establish that now.</p>
<p>A one-by-one matrix is automatically symmetric and diagonalizable. In the <span class="math inline">\(N\)</span>-dimensional case, we know, at least, that <span class="math inline">\(D\)</span> has at least one eigenvector, and real one at that by part <span class="math inline">\(1\)</span>, and this gives us a place to begin an inductive argument.</p>
<p>Let <span class="math inline">\(v_{N}\not=0\)</span> be an eigenvector with eigenvalue <span class="math inline">\(\lambda\)</span> and normalized so that <span class="math inline">\(\|v_{N}\|^2=1\)</span>,<br />
and extend this to a basis <span class="math inline">\(v_{1},\ldots v_{N}\)</span> of <span class="math inline">\(\mathbf{R}^{N}\)</span>. Apply the Gram-Schmidt process to construct an orthonormal basis of <span class="math inline">\(\mathbf{R}^{N}\)</span> <span class="math inline">\(u_{1},\ldots, u_{N}\)</span> so that <span class="math inline">\(u_{N}=v_{N}\)</span>.</p>
<p>Any vector <span class="math inline">\(v\in\mathbf{R}^{N}\)</span> is a linear combination <span class="math display">\[
v = \sum_{i=1}^{N} a_{i}u_{i}
\]</span> and, since the <span class="math inline">\(u_{i}\)</span> are orthonormal, the coefficients can be calculated as <span class="math inline">\(a_{i}=(u_{i}\cdot v)\)</span>.</p>
<p>Using this, we can find the matrix <span class="math inline">\(D&#39;\)</span> of the linear map defined by our original matrix <span class="math inline">\(D\)</span> in this new basis. By definition, if <span class="math inline">\(d&#39;_{ij}\)</span> are the entries of <span class="math inline">\(D&#39;\)</span>, then</p>
<p><span class="math display">\[
Du_{i} = \sum_{j=1}^{N} d&#39;_{ij} u_{j}
\]</span></p>
<p>and so</p>
<p><span class="math display">\[
d&#39;_{ij} = u_{j}\cdot Du_{i} = u_{j}^{\intercal}Du_{i}.
\]</span></p>
<p>Since <span class="math inline">\(D\)</span> is symmetric, <span class="math inline">\(u_{j}^{\intercal}Du_{i} =u_{i}^{\intercal}Du_{j}\)</span> and so <span class="math inline">\(d&#39;_{ij}=d&#39;_{ji}\)</span>. In other words, the matrix <span class="math inline">\(D&#39;\)</span> is still symmetric. Furthermore,</p>
<p><span class="math display">\[
d&#39;_{Ni} = u_{i}\cdot Du_{N} = u_{i}\cdot \lambda u_{N} = \lambda (u_{i}\cdot u_{N})
\]</span></p>
<p>since <span class="math inline">\(u_{N}=v_{N}\)</span>. Since the <span class="math inline">\(u_{i}\)</span> are an orthonormal basis, we see that <span class="math inline">\(d&#39;_{iN}=0\)</span> unless <span class="math inline">\(i=N\)</span>, and <span class="math inline">\(d&#39;_{NN}=\lambda\)</span>.</p>
<p>In other words, the matrix <span class="math inline">\(D&#39;\)</span> has a block form: <span class="math display">\[
D&#39; = \left(\begin{matrix} * &amp; * &amp; \cdots &amp;*  &amp; 0 \\ \vdots &amp; \vdots &amp; \ddots   &amp; \vdots &amp; \vdots \\
* &amp; * &amp; \cdots &amp;*  &amp; 0 \\
0 &amp; 0 &amp; \cdots &amp;0 &amp;\lambda \end{matrix}\right)
\]</span> and the block denoted by <span class="math inline">\(*\)</span>’s is symmetric. If we call that block <span class="math inline">\(D_{*}\)</span>, the inductive hypothesis tells us that the symmetric matrix <span class="math inline">\(D_{*}\)</span> is diagonalizable, so it has a basis of eigenvectors <span class="math inline">\(u&#39;_{1},\ldots, u&#39;_{N-1}\)</span> with eigenvalues <span class="math inline">\(\lambda_{1},\ldots, \lambda_{N-1}\)</span>; this gives us a basis for the subspace of <span class="math inline">\(\mathbf{R}^{N}\)</span> spanned by <span class="math inline">\(u_{1},\ldots, u_{N-1}\)</span> which, together with <span class="math inline">\(u_{N}\)</span> gives us a basis of <span class="math inline">\(\mathbf{R}^{N}\)</span> consisting of eigenvectors of <span class="math inline">\(D\)</span>.</p>
<p>This finishes the proof of Property <span class="math inline">\(2\)</span>.</p>
<p>For property <span class="math inline">\(3\)</span>, compute <span class="math display">\[
v^{\intercal}Dw = \lambda&#39;(v\cdot w)=w^{\intercal}Dv = \lambda (w\cdot v).
\]</span> Since <span class="math inline">\(\lambda\not=\lambda&#39;\)</span>, we must have <span class="math inline">\(v\cdot w=0\)</span>.</p>
<p>For property <span class="math inline">\(4\)</span>, if the eigenvalues are all distinct, this is a consequence of property <span class="math inline">\(2\)</span> – you have <span class="math inline">\(N\)</span> eigenvectors, scaled to length <span class="math inline">\(1\)</span>, for different eigenvalues, and by <span class="math inline">\(2\)</span> they are orthogonal. So the only complication is the case where some eigenvalues are repeated. If <span class="math inline">\(\lambda\)</span> occurs <span class="math inline">\(r\)</span> times, then you have <span class="math inline">\(r\)</span> linearly independent vectors <span class="math inline">\(u_{1},\ldots, u_{r}\)</span> that span the <span class="math inline">\(\lambda\)</span> eigenspace. The Gram-Schmidt process allows you to construct an orthonormal set that spans this eigenspace, and while this orthonormal set isn’t unique, any one of them will do.</p>
<p>For property <span class="math inline">\(5\)</span>, let <span class="math inline">\(e_{i}\)</span> be the column vector that is zero except for a <span class="math inline">\(1\)</span> in position <span class="math inline">\(i\)</span>. The product <span class="math inline">\(e_{j}^{\intercal}De_{i}=d_{ij}\)</span>. Let’s write <span class="math inline">\(e_{i}\)</span> and <span class="math inline">\(e_{j}\)</span> in terms of the orthonormal basis <span class="math inline">\(u_{1},\ldots u_{N}\)</span>: <span class="math display">\[
e_{i} = \sum_{k=1}^{N} (e_{i}\cdot u_{k})u_k \hbox{ and } e_{j} = \sum_{k=1}^{N}(e_{j}\cdot u_{k})u_{k}.
\]</span> Using this expansion, we compute <span class="math inline">\(e_{j}^{\intercal}De_{i}\)</span> in a more complicated way: <span class="math display">\[
e_{j}^{\intercal}De_{i} = \sum_{r=1}^{N}\sum_{s=1}^{N} (e_{j}\cdot u_{r})(e_{i}\cdot u_{s})(u_{r}^{\intercal}Du_{s}).
\]</span> But <span class="math inline">\(u_{r}^{\intercal}Du_{s}=\lambda_{s}(u_{r}\cdot u_{s})=0\)</span> unless <span class="math inline">\(r=s\)</span>, in which case it equals <span class="math inline">\(\lambda_{r}\)</span>, so <span class="math display">\[
e_{j}^{\intercal}De_{i} = \sum_{r=1}^{N} \lambda_{r}(e_{j}\cdot u_{r})(e_{i}\cdot u_{r}).
\]</span> On the other hand, <span class="math display">\[
P^{\intercal}e_{i} = \left[\begin{matrix} (e_{i}\cdot u_{1})\\ (e_{i}\cdot u_{2})\\ \vdots \\(e_{i}\cdot u_{N})\end{matrix}\right]
\]</span> and <span class="math display">\[
\Lambda P^{\intercal}e_{i} = \left[\begin{matrix} \lambda_{1}(e_{i}\cdot u_{i})\\ \lambda_{2}(e_{i}\cdot u_{2})\\ \vdots \\ \lambda_{N}(e_{i}\cdot u_{N})\end{matrix}\right]
\]</span> Therefore the <span class="math inline">\(i,j\)</span> entry of <span class="math inline">\(P\Lambda P^{\intercal}\)</span> is <span class="math display">\[
(e_{j}^{\intercal}P)\Lambda (P^{\intercal}e_{j}) = \sum_{r=1}^{N} \lambda_{r}(e_{i}\cdot u_{r})(e_{j}\cdot u_{r}) = d_{ij}
\]</span> so the two matrices <span class="math inline">\(D\)</span> and <span class="math inline">\(P\Lambda P^{\intercal}\)</span> are in fact equal.</p>
<p><strong>Exercises:</strong></p>
<ol type="1">
<li><p>Prove the rest of the first lemma in +<span class="citation" data-cites="sec:svd">@sec:svd</span>.</p></li>
<li><p>Prove the Gram-Schmidt Process has the claimed properties in +<span class="citation" data-cites="sec:gsprocess">@sec:gsprocess</span>.</p></li>
</ol>
<h1 id="support-vector-machines">Support Vector Machines</h1>
<h2 id="introduction-3">Introduction</h2>
<p>Suppose that we are given a collection of data made up of samples from two different classes, and we would like to develop an algorithm that can distinguish between the two classes. For example, given a picture that is either a dog or a cat, we’d like to be able to say which of the pictures are dogs, and which are cats. For another example, we might want to be able to distinguish “real” emails from “spam.” This type of problem is called a <em>classification</em> problem.</p>
<p>Typically, one approaches a classification problem by beginning with a large set of data for which you know the classes, and you use that data to <em>train</em> an algorithm to correctly distinguish the classes for the test cases where you already know the answer. For example, you start with a few thousand pictures labelled “dog” and “cat” and you build your algorithm so that it does a good job distinguishing the dogs from the cats in this initial set of <em>training data</em>. Then you apply your algorithm to pictures that aren’t labelled and rely on the predictions you get, hoping that whatever let your algorithm distinguish between the particular examples will generalize to allow it to correctly classify images that aren’t pre-labelled.</p>
<p>Because classification is such a central problem, there are many approaches to it. We will see several of them through the course of these lectures. We will begin with a particular classification algorithm called “Support Vector Machines” (SVM) that is based on linear algebra. The SVM algorithm is widely used in practice and has a beautiful geometric interpretation, so it will serve as a good beginning for later discussion of more complicated classification algorithms.</p>
<p>Incidentally, I’m not sure why this algorithm is called a “machine”; the algorithm was introduced in the paper <span class="citation" data-cites="vapnik92">@vapnik92</span> where it is called the “Optimal Margin Classifier” and as we shall see that is a much better name for it.</p>
<p>My presentation of this material was heavily influenced by the beautiful paper <span class="citation" data-cites="bennettDuality">@bennettDuality</span>.</p>
<h2 id="a-simple-example">A simple example</h2>
<p>Let us begin our discussion with a very simple dataset (see <span class="citation" data-cites="penguins">@penguins</span> and <span class="citation" data-cites="penguindata">@penguindata</span>). This data consists of various measurements of physical characteristics of 344 penguins of 3 different species: Gentoo, Adelie, and Chinstrap. If we focus our attention for the moment on the Adelie and Gentoo species, and plot their body mass against their culmen depth, we obtain the following scatterplot.</p>
<figure>
<img src="img/penguins.png" id="fig:penguins" style="width:50.0%" alt="Penguin Scatterplot" /><figcaption aria-hidden="true">Penguin Scatterplot</figcaption>
</figure>
<p>Incidentally, a bird’s <em>culmen</em> is the upper ridge of their beak, and the <em>culmen depth</em> is a measure of the thickness of the beak. There’s a nice picture at <span class="citation" data-cites="penguindata">@penguindata</span> for the penguin enthusiasts.</p>
<p>A striking feature of this scatter plot is that there is a clear separation between the clusters of Adelie and Gentoo penguins. Adelie penguins have deeper culmens and less body mass than Gentoo penguins. These characteristics seem like they should provide a way to classify a penguin between these two species based on these two measurements.</p>
<p>One way to express the separation between these two clusters is to observe that one can draw a line on the graph with the property that all of the Adelie penguins lie on one side of that line and all of the Gentoo penguins lie on the other. In +<span class="citation" data-cites="fig:penguinsline">@fig:penguinsline</span> I’ve drawn in such a line (which I found by eyeballing the picture in +<span class="citation" data-cites="fig:penguins">@fig:penguins</span>). The line has the equation <span class="math display">\[
Y = 1.25X+2.
\]</span></p>
<figure>
<img src="img/penguins_with_line.png" id="fig:penguinsline" style="width:50.0%" alt="Penguins with Separating Line" /><figcaption aria-hidden="true">Penguins with Separating Line</figcaption>
</figure>
<p>The fact that all of the Gentoo penguins lie above this line means that, for the Gentoo penguins, their body mass in grams is at least <span class="math inline">\(400\)</span> more than <span class="math inline">\(250\)</span> times their culmen depth in mm. (Note that the <span class="math inline">\(y\)</span> axis of the graph is scaled by <span class="math inline">\(200\)</span> grams).</p>
<p><span class="math display">\[
\mathrm{Gentoo\ mass}&gt; 250(\mathrm{Gentoo\ culmen\ depth})+400
\]</span></p>
<p>while</p>
<p><span class="math display">\[
\mathrm{Adelie\ mass}&lt;250(\mathrm{Adelie\ culmen\ depth})+400.
\]</span></p>
<p>Now, if we measure a penguin caught in the wild, we can compute <span class="math inline">\(250(\mathrm{culmen\ depth})+400\)</span> for that penguin and if this number is greater than the penguin’s mass, we say it’s an Adelie; otherwise, a Gentoo. Based on the experimental data we’ve collected – the <em>training</em> data – this seems likely to work pretty well.</p>
<h2 id="the-general-case">The general case</h2>
<p>To generalize this approach, let’s imagine now that we have <span class="math inline">\(n\)</span> samples and <span class="math inline">\(k\)</span> features (or measurements) for each sample. As before, we can represent this data as an <span class="math inline">\(n\times k\)</span> data matrix <span class="math inline">\(X\)</span>. In the penguin example, our data matrix would be <span class="math inline">\(344\times 2\)</span>, with one row for each penguin and the columns representing the mass and the culmen depth. In addition to this numerical data, we have a classification that assigns each row to one of two classes. Let’s represent the classes by a <span class="math inline">\(n\times 1\)</span> vector <span class="math inline">\(Y\)</span>, where <span class="math inline">\(y_{i}=+1\)</span> if the <span class="math inline">\(i^{th}\)</span> sample is in one class, and <span class="math inline">\(y_{i}=-1\)</span> if that <span class="math inline">\(i^{th}\)</span> sample is in the other. Our goal is to predict <span class="math inline">\(Y\)</span> based on <span class="math inline">\(X\)</span> – but unlike in linear regression, <span class="math inline">\(Y\)</span> takes on the values of <span class="math inline">\(\pm 1\)</span>.</p>
<p>In the penguin case, we were able to find a line that separated the two classes and then classify points by which side of the line the point was on. We can generalize this notion to higher dimensions. Before attacking that generalization, let’s recall a few facts about the generalization to <span class="math inline">\(\mathbf{R}^{k}\)</span> of the idea of a line.</p>
<h3 id="hyperplanes">Hyperplanes</h3>
<p>The correct generalization of a line given by an equation <span class="math inline">\(w_1 x_1+ w_2 w_2+b=0\)</span> in <span class="math inline">\(\mathbf{R}^{2}\)</span> is an equation <span class="math inline">\(f(x)=0\)</span> where <span class="math inline">\(f(x)\)</span> is a degree one polynomial <span class="math display">\[
f(x) = f(x_1,\ldots, x_k) = w_1 x_1 + w_2 x_2 +\cdots + w_k x_k + b 
\]</span>{#eq:degreeone}</p>
<p>It’s easier to understand the geometry of an equation like <span class="math inline">\(f(x)=0\)</span> in +<span class="citation" data-cites="eq:degreeone">@eq:degreeone</span> if we think of the coefficients <span class="math inline">\(w_i\)</span> as forming a <em>nonzero</em> vector <span class="math inline">\(w = (w_1,\ldots, w_k)\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span> and writing the formula for <span class="math inline">\(f(x)\)</span> as <span class="math display">\[
f(x) = w\cdot x +b
\]</span>.</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(f(x)=w\cdot x+b\)</span> with <span class="math inline">\(w\in\mathbf{R}^{k}\)</span> a nonzero vector and <span class="math inline">\(b\)</span> a constant in <span class="math inline">\(\mathbf{R}\)</span>.</p>
<ul>
<li>The inequalities <span class="math inline">\(f(x)&gt;0\)</span> and <span class="math inline">\(f(x)&lt;0\)</span> divide up <span class="math inline">\(\mathbf{R}^{k}\)</span> into two disjoint subsets (called half spaces), in the way that a line in <span class="math inline">\(\mathbf{R}^{2}\)</span> divides the plane in half.</li>
<li>The vector <span class="math inline">\(w\)</span> is normal vector to the hyperplane <span class="math inline">\(f(x)=0\)</span>. Concretely this means that if <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are any two points in that hyperplane, then <span class="math inline">\(w\cdot (p-q)=0\)</span>.</li>
<li>Let <span class="math inline">\(p=(u_1,\ldots,u_k)\)</span> be a point in <span class="math inline">\(\mathbf{R}^{k}\)</span>. Then the perpendicular distance <span class="math inline">\(D\)</span> from <span class="math inline">\(p\)</span> to the hyperplane <span class="math inline">\(f(x)=0\)</span> is <span class="math display">\[
D = \frac{f(p)}{\|w\|}
\]</span></li>
</ul>
<p><strong>Proof:</strong> The first part is clear since the inequalities are mutually exclusive. For the secon part, suppose that <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> satisfy <span class="math inline">\(f(x)=0\)</span>. Then <span class="math inline">\(w\cdot p+b = w\cdot q+b=0\)</span>. Subtracting these two equations gives <span class="math inline">\(w\cdot (p-q)=0\)</span>, so <span class="math inline">\(p-q\)</span> is orthogonal to <span class="math inline">\(w\)</span>.</p>
<p>For the third part, consider +<span class="citation" data-cites="fig:triangle">@fig:triangle</span>. The point <span class="math inline">\(q\)</span> is an arbitrary point on the hyperplane defined by the equation <span class="math inline">\(w\cdot x+b=0\)</span>. The distance from the hyperplane to <span class="math inline">\(p\)</span> is measured along the dotted line perpendicular to the hyperplane. The dot product <span class="math inline">\(w\cdot (p-q) = \|w\|\|p-q\|\cos(\theta)\)</span> where <span class="math inline">\(\theta\)</span> is the angle between <span class="math inline">\(p-q\)</span> and <span class="math inline">\(w\)</span> – which is complementary to the angle between <span class="math inline">\(p-q\)</span> and the hyperplane. The distance <span class="math inline">\(D\)</span> is therefore <span class="math display">\[
D=\frac{w\cdot(p-q)}{\|w\|}.
\]</span> However, since <span class="math inline">\(q\)</span> lies on the hyperplane, we know that <span class="math inline">\(w\cdot q+b=0\)</span> so <span class="math inline">\(w\cdot q = -b\)</span>. Therefore <span class="math inline">\(w\cdot(p-q)=w\cdot p+b=f(p)\)</span>, which is the formula we seek.</p>
<figure>
<img src="img/triangle.png" id="fig:triangle" style="width:30.0%" alt="Distance to a Hyperplane" /><figcaption aria-hidden="true">Distance to a Hyperplane</figcaption>
</figure>
<h3 id="sec:linearseparable">Linear separability and Margins</h3>
<p>Now we can return to our classification scheme. The following definition generalizes our two dimensional picture from the penguin data.</p>
<p><strong>Definition:</strong> Suppose that we have an <span class="math inline">\(n\times k\)</span> data matrix <span class="math inline">\(X\)</span> and a set of labels <span class="math inline">\(Y\)</span> that assign the <span class="math inline">\(n\)</span> samples to one of two classes. Then the labelled data is said to be <em>linearly separable</em> if there is a vector <span class="math inline">\(w\)</span> and a constant <span class="math inline">\(b\)</span> so that, if <span class="math inline">\(f(x)=w\cdot x+b\)</span>, then <span class="math inline">\(f(x)&gt;0\)</span> whenever <span class="math inline">\(x=(x_1,\ldots, x_k)\)</span> is a row of <span class="math inline">\(X\)</span> – a sample – belonging to the <span class="math inline">\(+1\)</span> class, and <span class="math inline">\(f(x)&lt;0\)</span> whenever <span class="math inline">\(x\)</span> belongs to the <span class="math inline">\(-1\)</span> class. The solutions to the equation <span class="math inline">\(f(x)=0\)</span> in this situation form a hyperplane that is called a <em>separating hyperplane</em> for the data.</p>
<p>In the situation where our data falls into two classes that are linearly separable, our classification strategy is to find a separating hyperplane <span class="math inline">\(f\)</span> for our training data. Then, given a point <span class="math inline">\(x\)</span> whose class we don’t know, we can evaluate <span class="math inline">\(f(x)\)</span> and assign <span class="math inline">\(x\)</span> to a class depending on whether <span class="math inline">\(f(x)&gt;0\)</span> or <span class="math inline">\(f(x)&lt;0\)</span>.</p>
<p>This definition begs two questions about a particular dataset:</p>
<ol type="1">
<li>How do we tell if the two classes are linearly separable?</li>
<li>If the two sets are linearly separable, there are infinitely many separating hyperplanes. To see this, look back at the penguin example and notice that we can ‘wiggle’ the red line a little bit and it will still separate the two sets. Which is the ‘best’ separating hyperplane?</li>
</ol>
<p>Let’s try to make the first of these two questions concrete. We have two sets of points <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span>, and we want to (try to) find a vector <span class="math inline">\(w\)</span> and a constant <span class="math inline">\(b\)</span> so that <span class="math inline">\(f(x)=w\cdot x+b\)</span> takes strictly positive values for <span class="math inline">\(x\in A\)</span> and strictly negative ones for <span class="math inline">\(x\in B\)</span>. Let’s approach the problem by first choosing <span class="math inline">\(w\)</span> and then asking whether there is a <span class="math inline">\(b\)</span> that will work. In the two dimensional case, this is equivalent to choosing the slope of our line, and then asking if we can find an intercept so that the line passes between the two classes.</p>
<p>In algebraic terms, we are trying to solve the following system of inequalities: given <span class="math inline">\(w\)</span>, find <span class="math inline">\(b\)</span> so that: <span class="math display">\[
w\cdot x+b&gt;0 \hbox{ for all $x$ in A}
\]</span> and <span class="math display">\[
w\cdot x+b&lt;0\hbox{ for all $x$ in B}.
\]</span> This is only going to be possible if there is a gap between the smallest value of <span class="math inline">\(w\cdot x\)</span> for <span class="math inline">\(x\in A\)</span> and the largest value of <span class="math inline">\(w\cdot x\)</span> for <span class="math inline">\(x\in B\)</span>. In other words, given <span class="math inline">\(w\)</span> there is a <span class="math inline">\(b\)</span> so that <span class="math inline">\(f(x)=w\cdot x+b\)</span> separates <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> if <span class="math display">\[
\max_{x\in B}w\cdot x &lt; \min_{x\in A} w\cdot x.
\]</span> If this holds, then choose <span class="math inline">\(b\)</span> so that <span class="math inline">\(-b\)</span> lies in this open interval and you will obtain a separating hyperplane.</p>
<p><strong>Proposition:</strong> The sets <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> are linearly separable if there is a <span class="math inline">\(w\)</span> so that <span class="math display">\[
\max_{x\in B}w\cdot x &lt; \min_{x\in A} w\cdot x
\]</span> If this inequality holds for some <span class="math inline">\(w\)</span>, and <span class="math inline">\(-b\)</span> within this open interval, then <span class="math inline">\(f(x)=w\cdot x+b\)</span> is a separating hyperplane for <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span>.</p>
<p>*<span class="citation" data-cites="fig:penguinhwy2">@fig:penguinhwy2</span> is an illustration of this argument for a subset of the penguin data. Here, we have fixed <span class="math inline">\(w=(1.25,-1)\)</span> coming from the line <span class="math inline">\(y=1.25x+2\)</span> that we eyeballed earlier. For each Gentoo (green) point <span class="math inline">\(x_{i}\)</span>, we computed <span class="math inline">\(-b=w\cdot x_{i}\)</span> and drew the line <span class="math inline">\(f(x) = w\cdot x - w\cdot x_{i}\)</span> giving a family of parallel lines through each of the green points. Similarly for each Adelie (blue) point we drew the corresponding line. The maximum value of <span class="math inline">\(w\cdot x\)</span> for the blue points turned out to be <span class="math inline">\(1.998\)</span> and the minimum value of <span class="math inline">\(w\cdot x\)</span> for the green points turned out to be <span class="math inline">\(2.003\)</span>. Thus we have two lines with a gap between them, and any parallel line in that gap will separate the two sets.</p>
<p>Finally, among all the lines <em>with this particular <span class="math inline">\(w\)</span></em>, it seems that the <strong>best</strong> separating line is the one running right down the middle of the gap between the boundary lines. Any other line in the gap will be closer to either the blue or green set that the midpoint line is.</p>
<figure>
<img src="img/penguinhwy2.png" id="fig:penguinhwy2" style="width:50.0%" alt="Lines in Penguin Data for w=(1.25,-1)" /><figcaption aria-hidden="true">Lines in Penguin Data for <span class="math inline">\(w=(1.25,-1)\)</span></figcaption>
</figure>
<p>Let’s put all of this together and see if we can make sense of it in general.</p>
<p>Suppose that <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> are finite point sets in <span class="math inline">\(\mathbf{R}^{k}\)</span> and <span class="math inline">\(w\in\mathbf{R}^{k}\)</span> such that <span class="math display">\[
B^{-}(w)=\max_{x\in A^{-}}w\cdot x &lt; \min_{x\in A^{+}}w\cdot x=B^{+}(w).
\]</span> Let <span class="math inline">\(x^{-}\)</span> be a point in <span class="math inline">\(A^{-}\)</span> with <span class="math inline">\(w\cdot x^{-}=B^{-}(w)\)</span> and <span class="math inline">\(x^{+}\)</span> be a point in <span class="math inline">\(A\)</span> with <span class="math inline">\(w\cdot x^{+}=B^{+}(w)\)</span>. The two hyperplanes <span class="math inline">\(f^{\pm}(x) = w\cdot x - B^{\pm}\)</span> have the property that: <span class="math display">\[
f^{+}(x)\ge 0\hbox{ for }x\in A^{+}\hbox{ and }f^{+}(x)&lt;0\hbox{ for }x\in A^{-}
\]</span> and <span class="math display">\[
f^{-}(x)\le 0\hbox{ for }x\in A^{-}\hbox{ and }f^{-}(x)&gt;0\hbox{ for }x\in A^{+}
\]</span></p>
<p>Hyperplanes like <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span>, which “just touch” a set of points, are called supporting hyperplanes.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(A\)</span> be a set of points in <span class="math inline">\(\mathbf{R}^{k}\)</span>. A hyperplane <span class="math inline">\(f(x)=w\cdot x+b=0\)</span> is called a <em>supporting hyperplane</em> for <span class="math inline">\(A\)</span> if <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(x\in A\)</span> and <span class="math inline">\(f(x)=0\)</span> for at least one point in <span class="math inline">\(A\)</span>, or if <span class="math inline">\(f(x)\le 0\)</span> for all <span class="math inline">\(x\in A\)</span> and <span class="math inline">\(f(x)=0\)</span> for at least one point in <span class="math inline">\(A\)</span>.</p>
<p>The gap between the two supporting hyperplanes <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> is called the <em>margin</em> between <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> for <span class="math inline">\(w\)</span>.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> be as in the discussion above for point sets <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> and vector <span class="math inline">\(w\)</span>. Then the orthogonal distance between the two hyperplanes <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> is called the geometric margin <span class="math inline">\(\tau_{w}(A^{+},A^{-})\)</span> (along <span class="math inline">\(w\)</span>) between <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>. We have <span class="math display">\[
\tau_{w}(A^{+},A^{-})=\frac{B^{+}(w)-B^{-}(w)}{\|w\|}.
\]</span></p>
<p>Now we can propose an answer to our second question about the best classifying hyperplane.</p>
<p><strong>Definition:</strong> The <em>optimal margin</em> <span class="math inline">\(\tau(A^{+},A^{-})\)</span> between <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> is the largest value of <span class="math inline">\(\tau_{w}\)</span> over all possible <span class="math inline">\(w\)</span> for which <span class="math inline">\(B^{-}(w)&lt;B^{+}(w)\)</span>: <span class="math display">\[
\tau(A^{+},A^{-}) = \max_{w} \tau_{w}(A^{+},A^{-}).
\]</span> If <span class="math inline">\(w\)</span> is such that <span class="math inline">\(\tau_{w}=\tau\)</span>, then the hyperplane <span class="math inline">\(f(x)=w\cdot x - \frac{(B^{+}+B^{-})}{2}\)</span> is the <em>optimal margin classifying hyperplane</em>.</p>
<p>The optimal classifying hyperplane runs “down the middle” of the gap between the two supporting hyperplanes <span class="math inline">\(f^{+}\)</span> and <span class="math inline">\(f^{-}\)</span> that give the sides of the optimal margin.</p>
<p>We can make one more observation about the maximal margin. If we find a vector <span class="math inline">\(w\)</span> so that <span class="math inline">\(f^{+}(x) = w\cdot x -B^{+}\)</span> and <span class="math inline">\(f^{-}(x) = w\cdot x-B^{-}\)</span> are the two supporting hyperplanes such that the gap between them is the optimal margin, then this gap gives us an estimate on how close together the points in <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> can be. This is visible in +<span class="citation" data-cites="fig:penguinhwy2">@fig:penguinhwy2</span>, where it’s clear that to get from a blue point to a green one, you have to cross the gap between the two supporting hyperplanes.</p>
<p><strong>Proposition:</strong> The closest distance between points in <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> is greater than or equal to the optimal margin: <span class="math display">\[
\min_{p\in A^{+},q\in A^{-}} \|p-q\|\ge \tau(A^{+},A^{-})
\]</span>.</p>
<p><strong>Proof:</strong> We have <span class="math inline">\(f^{+}(p) = w\cdot p - B^{+}\ge 0\)</span> and <span class="math inline">\(f^{-}(q) = w\cdot q -B^{-}\le 0\)</span>. These two inequalities imply that <span class="math display">\[
w\cdot (p-q)\ge B^{+}-B^{-}&gt;0.
\]</span> Therefore <span class="math display">\[
\|p-q\|\|w\|\ge |w\cdot (p-q)|\ge |B^{+}-B^{-}|
\]</span> and so <span class="math display">\[
\|p-q\| \ge \frac{B^{+}-B^{-}}{\|w\|} = \tau(A^{+},A^{-})
\]</span></p>
<p>If this inequality were always <em>strict</em> – that is, if the optimal margin equalled the minimum distance between points in the two clusters – then this would give us an approach to finding this optimal margin.</p>
<p>Unfortunately, that isn’t the case. In +<span class="citation" data-cites="fig:nonstrict">@fig:nonstrict</span>, we show a very simple case involving only six points in total in which the distance between the closest points in <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> is larger than the optimal margin.</p>
<figure>
<img src="img/margindistance2.png" id="fig:nonstrict" style="height:3in" alt="Shortest distance between + and - points can be greater than the optimal margin" /><figcaption aria-hidden="true">Shortest distance between + and - points can be greater than the optimal margin</figcaption>
</figure>
<p>At least now our problem is clear. Given our two point sets <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>, find <span class="math inline">\(w\)</span> so that <span class="math inline">\(\tau_{w}(A^{+},A^{-})\)</span> is maximal among all <span class="math inline">\(w\)</span> where <span class="math inline">\(B^{-}(w)&lt;B^{+}(w)\)</span>. This is an optimization problem, but unlike the optimization problems that arose in our discussions of linear regression and principal component analysis, it does not have a closed form solution. We will need to find an algorithm to determine <span class="math inline">\(w\)</span> by successive approximations. Developing that algorithm will require thinking about a new concept known as <em>convexity.</em></p>
<h2 id="convexity-convex-hulls-and-margins">Convexity, Convex Hulls, and Margins</h2>
<p>In this section we introduce the notion of a <em>convex set</em> and the particular case of the <em>convex hull</em> of a finite set of points. As we will see, these ideas will give us a different interpretation of the margin between two sets and will eventually lead to an algorithm for finding the optimal margin classifier.</p>
<p><strong>Definition:</strong> A subset <span class="math inline">\(U\)</span> of <span class="math inline">\(\mathbf{R}^{k}\)</span> is <em>convex</em> if, for any pair of points <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> in <span class="math inline">\(U\)</span>, every point <span class="math inline">\(t\)</span> on the line segment joining <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> also belongs to <span class="math inline">\(U\)</span>. In vector form, for every <span class="math inline">\(0\le s\le 1\)</span>, the point <span class="math inline">\(t(s) = (1-s)p+sq\)</span> belongs to <span class="math inline">\(U\)</span>. (Note that <span class="math inline">\(t(0)=p\)</span>, <span class="math inline">\(t(1)=q\)</span>, and so <span class="math inline">\(t(s)\)</span> traces out the segment joining <span class="math inline">\(p\)</span> to <span class="math inline">\(q\)</span>.)</p>
<p>*<span class="citation" data-cites="fig:convexnotconvex">@fig:convexnotconvex</span> illustrates the difference between convex sets and non-convex ones.</p>
<figure>
<img src="img/ConvexNotConvex.png" id="fig:convexnotconvex" style="height:3in" alt="Convex vs Non-Convex Sets" /><figcaption aria-hidden="true">Convex vs Non-Convex Sets</figcaption>
</figure>
<p>The key idea from convexity that we will need to solve our optimization problem and find the optimal margin is the idea of the <em>convex hull</em> of a finite set of points in <span class="math inline">\(\mathbf{R}^{k}\)</span>.</p>
<p><strong>Definition:</strong> Let <span class="math inline">\(S=\{q_1,\ldots, q_{N}\}\)</span> be a finite set of <span class="math inline">\(N\)</span> points in <span class="math inline">\(\mathbf{R}^{k}\)</span>. The <em>convex hull</em> <span class="math inline">\(C(S)\)</span> of <span class="math inline">\(S\)</span> is the set of points <span class="math display">\[
p = \sum_{i=1}^{N} \lambda_{i}q_{i}
\]</span> as <span class="math inline">\(\lambda_{1},\ldots,\lambda_{N}\)</span> runs over all positive real numbers such that <span class="math display">\[
\sum_{i=1}^{N} \lambda_{i} = 1.
\]</span></p>
<p>There are a variety of ways to think about the convex hull <span class="math inline">\(C(S)\)</span> of a set of points <span class="math inline">\(S\)</span>, but perhaps the most useful is that it is the smallest convex set that contains all of the points of <span class="math inline">\(S\)</span>. That is the content of the next lemma.</p>
<p><strong>Lemma:</strong> <span class="math inline">\(C(S)\)</span> is convex. Furthermore, let <span class="math inline">\(U\)</span> be any convex set containing all of the points of <span class="math inline">\(S\)</span>. Then <span class="math inline">\(U\)</span> contains <span class="math inline">\(C(S)\)</span>.</p>
<p><strong>Proof:</strong> To show that <span class="math inline">\(C(S)\)</span> is convex, we apply the definition. Let <span class="math inline">\(p_1\)</span> and <span class="math inline">\(p_2\)</span> be two points in <span class="math inline">\(C(S)\)</span>, so that let <span class="math inline">\(p_{j}=\sum_{i=1}^{N} \lambda^{(j)}_{i}q_{i}\)</span> where <span class="math inline">\(\sum_{i=1}^{N}\lambda^{(j)}_{i} = 1\)</span> for <span class="math inline">\(j=1,2\)</span>. Then a little algebra shows that <span class="math display">\[
(1-s)p_1+sp_{2} = \sum_{i=1}^{N} (s\lambda^{(1)}_{i}+(1-s)\lambda^{(2)}_{i})q_{i}
\]</span> and <span class="math inline">\(\sum_{i=1}^{N} (s\lambda^{(1)}_{i}+(1-s)\lambda^{(2)}_{i}) = 1\)</span>. Therefore all of the points <span class="math inline">\((1-s)p_{1}+sp_{2}\)</span> belong to <span class="math inline">\(C(S)\)</span>, and therefore <span class="math inline">\(C(S)\)</span> is convex.</p>
<p>For the second part, we proceed by induction. Let <span class="math inline">\(U\)</span> be a convex set containing <span class="math inline">\(S\)</span>. Then by the definition of convexity, <span class="math inline">\(U\)</span> contains all sums <span class="math inline">\(\lambda_{i}q_{i}+\lambda_{j}q_{j}\)</span> where <span class="math inline">\(\lambda_i+\lambda_j=1\)</span>. Now suppose that <span class="math inline">\(U\)</span> contains all the sums <span class="math inline">\(\sum_{i=1}^{N} \lambda_{i}q_{i}\)</span> where exactly <span class="math inline">\(m-1\)</span> of the <span class="math inline">\(\lambda_{i}\)</span> are non-zero for some <span class="math inline">\(m&lt;N\)</span>.<br />
Consider a sum <span class="math display">\[
q = \sum_{i=1}^{N}\lambda_{i}q_{i}
\]</span> with exactly <span class="math inline">\(m\)</span> of the <span class="math inline">\(\lambda_{i}\not=0\)</span>. For simplicity let’s assume that <span class="math inline">\(\lambda_{i}\not=0\)</span> for <span class="math inline">\(i=1,\ldots, m\)</span>. Now let <span class="math inline">\(T=\sum_{i=1}^{m-1}\lambda_{i}\)</span> and set <span class="math display">\[
q&#39; = \sum_{i=1}^{m-1}\frac{\lambda_{i}}{T}q_{i}.
\]</span> This point <span class="math inline">\(q&#39;\)</span> belongs to <span class="math inline">\(U\)</span> by the inductive hypothesis. Also, <span class="math inline">\((1-T)=\lambda_{m}\)</span>. Therefore by convexity of <span class="math inline">\(U\)</span>, <span class="math display">\[
q = (1-T)q_{m}+Tq&#39;
\]</span> also belongs to <span class="math inline">\(U\)</span>. It follows that all of <span class="math inline">\(C(S)\)</span> belongs to <span class="math inline">\(U\)</span>.</p>
<p>In +<span class="citation" data-cites="fig:convexhull">@fig:convexhull</span> we show our penguin data together with the convex hull of points corresponding to the two types of penguins. Notice that the boundary of each convex hull is a finite collection of line segments that join the “outermost” points in the point set.</p>
<figure>
<img src="img/penguinswithhulls.png" id="fig:convexhull" style="width:50.0%" alt="The Convex Hull" /><figcaption aria-hidden="true">The Convex Hull</figcaption>
</figure>
<p>One very simple example of a convex set is a half-plane. More specifically, if <span class="math inline">\(f(x)=w\cdot x+b=0\)</span> is a hyperplane, then the two “sides” of the hyperplane, meaning the subsets <span class="math inline">\(\{x: f(x)\ge 0\}\)</span> and <span class="math inline">\(\{x: f(x)\le 0\}\)</span>, are both convex. (This is exercise 1 in +<span class="citation" data-cites="sec:exercises">@sec:exercises</span> ).</p>
<p>As a result of this observation, and the Lemma above, we can conclude that if <span class="math inline">\(f(x)=w\cdot x+b=0\)</span> is a supporting hyperplane for the set <span class="math inline">\(S\)</span> – meaning that either <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(x\in S\)</span>, or <span class="math inline">\(f(x)\le 0\)</span> for all <span class="math inline">\(x\in S\)</span>, with at least one point <span class="math inline">\(x\in S\)</span> such that <span class="math inline">\(f(x)=0\)</span> – then <span class="math inline">\(f(x)=0\)</span> is a supporting hyperplane for the entire convex hull. After all, if <span class="math inline">\(f(x)\ge 0\)</span> for all points <span class="math inline">\(x\in S\)</span>, then <span class="math inline">\(S\)</span> is contained in the convex set of points where <span class="math inline">\(f(x)\ge 0\)</span>, and therefore <span class="math inline">\(C(S)\)</span> is contained in that set as well.</p>
<p>Interestingly, however, the converse is true as well – the supporting hyperplanes of <span class="math inline">\(C(S)\)</span> are exactly the same as those for <span class="math inline">\(S\)</span>.</p>
<p><strong>Lemma:</strong> Let <span class="math inline">\(S\)</span> be a finite set of points in <span class="math inline">\(\mathbf{R}^{k}\)</span> and let <span class="math inline">\(f(x)=w\cdot x +b=0\)</span> be a supporting hyperplane for <span class="math inline">\(C(S)\)</span>. Then <span class="math inline">\(f(x)\)</span> is a supporting hyperplane for <span class="math inline">\(S\)</span>.</p>
<p><strong>Proof:</strong> Suppose <span class="math inline">\(f(x)=0\)</span> is a supporting hyperplane for <span class="math inline">\(C(S)\)</span>. Let’s assume that <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(x\in C(S)\)</span> and <span class="math inline">\(f(x^{*})=0\)</span> for a point <span class="math inline">\(x^{*}\in C(S)\)</span>, since the case where <span class="math inline">\(f(x)\le 0\)</span> is identical. Since <span class="math inline">\(S\subset C(S)\)</span>, we have <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(x\in S\)</span>. To show that <span class="math inline">\(f(x)=0\)</span> is a supporting hyperplane, we need to know that <span class="math inline">\(f(x)=0\)</span> for at least one point <span class="math inline">\(x\in S\)</span>.<br />
Let <span class="math inline">\(x&#39;\)</span> be the point in <span class="math inline">\(S\)</span> where <span class="math inline">\(f(x&#39;)\)</span> is minimal among all <span class="math inline">\(x\in S\)</span>. Note that <span class="math inline">\(f(x&#39;)\ge 0\)</span>. Then the hyperplane <span class="math inline">\(g(x) = f(x)-f(x&#39;)\)</span> has the property that <span class="math inline">\(g(x)\ge 0\)</span> on all of <span class="math inline">\(S\)</span>, and <span class="math inline">\(g(x&#39;)=0\)</span>. Since the halfplane <span class="math inline">\(g(x)\ge 0\)</span> is convex and contains all of <span class="math inline">\(S\)</span>, we have <span class="math inline">\(C(S)\)</span> contained in that halfplane. So, on the one hand we have <span class="math inline">\(g(x^{*})=f(x^{*})-f(x&#39;)\ge 0\)</span>. On the other hand <span class="math inline">\(f(x^{*})=0\)</span>, so <span class="math inline">\(-f(x&#39;)\ge 0\)</span>, so <span class="math inline">\(f(x&#39;)\le 0\)</span>. Since <span class="math inline">\(f(x&#39;)\)</span> is also greater or equal to zero, we have <span class="math inline">\(f(x&#39;)=0\)</span>, and so we have found a point of <span class="math inline">\(S\)</span> on the hyperplane <span class="math inline">\(f(x)=0\)</span>. Therefore <span class="math inline">\(f(x)=0\)</span> is also a supporting hyperplane for <span class="math inline">\(S\)</span>.</p>
<p>This argument can be used to give an alternative description of <span class="math inline">\(C(S)\)</span> as the intersection of all halfplanes containing <span class="math inline">\(S\)</span> arising from supporting hyperplanes for <span class="math inline">\(S\)</span>. This is exercise 2 in +<span class="citation" data-cites="sec:exercises">@sec:exercises</span>. It also has as a corollary that <span class="math inline">\(C(S)\)</span> is a closed set.</p>
<p><strong>Lemma:</strong> <span class="math inline">\(C(S)\)</span> is compact.</p>
<p><strong>Proof:</strong> Exercise 2 in +<span class="citation" data-cites="sec:exercises">@sec:exercises</span> shows that it is the intersection of closed sets in <span class="math inline">\(\mathbf{R}^{k}\)</span>, so it is closed. Exercise 3 shows that <span class="math inline">\(C(S)\)</span> is bounded. Thus it is compact.</p>
<p>Now let’s go back to our optimal margin problem, so that we have linearly separable sets of points <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>. Recall that we showed that the optimal margin was at most the minimal distance between points in <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>, but that there could be a gap between the minimal distance and the optimal margin – see +<span class="citation" data-cites="fig:nonstrict">@fig:nonstrict</span> for a reminder.</p>
<p>It turns out that by considering the minimal distance between <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span>, we can “close this gap.” The following proposition shows that we can change the problem of finding the optimal margin into the problem of finding the closest distance between the convex hulls of <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span>. The following proposition generalizes the Proposition at the end of +<span class="citation" data-cites="sec:linearseparable">@sec:linearseparable</span>.</p>
<p><strong>Proposition:</strong> Let <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> be linearly separable sets in <span class="math inline">\(\mathbf{R}^{k}\)</span>. Let <span class="math inline">\(p\in C(A^{+})\)</span> and <span class="math inline">\(q\in C(A^{-})\)</span> be any two points. Then <span class="math display">\[
\|p-q\|\ge \tau(A^{+},A^{-}).
\]</span></p>
<p><strong>Proof:</strong> As in the earlier proof, choose supporting hyperplanes <span class="math inline">\(f^{+}(x)=w\cdot x-B^{+}=0\)</span> and <span class="math inline">\(f^{-}(x)=w\cdot x-B^{-}\)</span> for <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>. By our discussion above, these are also supporting hyperplanes for <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span>. Therefore if <span class="math inline">\(p\in C(A^{+})\)</span> and <span class="math inline">\(q\in C(A^{-})\)</span>, we have <span class="math inline">\(w\cdot p-B^{+}\ge 0\)</span> and <span class="math inline">\(w\cdot q-B^{-}\le 0\)</span>. As before <span class="math display">\[
w\cdot(p-q)\ge B^{+}-B^{-}&gt;0
\]</span> and so <span class="math display">\[
\|p-q\|\ge\frac{B^{+}-B^{-}}{\|w\|}=\tau_{w}(A^{+},A^{-})
\]</span> Since this holds for any <span class="math inline">\(w\)</span>, we have the result for <span class="math inline">\(\tau(A^{+},A^{-})\)</span>.</p>
<p>The reason this result is useful is that, as we’ve seen, if we restrict <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> to <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>, then there can be a gap between the minimal distance and the optimal margin. If we allow <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> to range over the convex hulls of these sets, then that gap disappears.</p>
<p>One other consequence of this is that if <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> are linearly separable then their convex hulls are disjoint.</p>
<p><strong>Corollary:</strong> If <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span> are linearly separable then <span class="math inline">\(\|p-q\|&gt;0\)</span> for all <span class="math inline">\(p\in C(A^{+})\)</span> and <span class="math inline">\(q\in C(A^{-})\)</span></p>
<p><strong>Proof:</strong> The sets are linearly separable precisely when <span class="math inline">\(\tau&gt;0\)</span>.</p>
<p>Our strategy now is to show that if <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> are points in <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively that are at minimal distance <span class="math inline">\(D\)</span>, and if we set <span class="math inline">\(w=p-q\)</span>, then we obtain supporting hyperplanes with margin equal to <span class="math inline">\(\|p-q\|\)</span>. Since this margin is the <em>largest possible margin</em>, this <span class="math inline">\(w\)</span> must be the optimal <span class="math inline">\(w\)</span>. This transforms the problem of finding the optimal margin into the problem of finding the closest points in the convex hulls.</p>
<p><strong>Lemma:</strong> Let <span class="math display">\[
D=\min_{p\in C(A^{+}),q\in C(A^{-})} \|p-q\|.
\]</span> Then there are points <span class="math inline">\(p^*\in C(A^{+})\)</span> and <span class="math inline">\(q^{*}\in C(A^{-})\)</span> with <span class="math inline">\(\|p^{*}-q^{*}\|=D\)</span>. If <span class="math inline">\(p_1^{*},q_1^{*}\)</span> and <span class="math inline">\(p_2^{*},q_2^{*}\)</span> are two pairs of points satisfying this condition, then <span class="math inline">\(p_1^{*}-q_1^{*}=p_2^{*}-q_{2}^{*}\)</span>.</p>
<p><strong>Proof:</strong> Consider the set of differences <span class="math display">\[
V = \{p-q: p\in C(A^{+}),q\in C(A^{-})\}.
\]</span></p>
<ul>
<li><p><span class="math inline">\(V\)</span> is compact. This is because it is the image of the compact set <span class="math inline">\(C(A^{+})\times C(A^{-})\)</span> in <span class="math inline">\(\mathbf{R}^{k}\times\mathbf{R}^{k}\)</span> under the continuous map <span class="math inline">\(h(x,y)=x-y\)</span>.</p></li>
<li><p>the function <span class="math inline">\(d(v)=\|v\|\)</span> is continuous and satisfies <span class="math inline">\(d(v)\ge D&gt;0\)</span> for all <span class="math inline">\(v\in V\)</span>.</p></li>
</ul>
<p>Since <span class="math inline">\(d\)</span> is a continuous function on a compact set, it attains its minimum <span class="math inline">\(D\)</span> and so there is a <span class="math inline">\(v=p^{*}-q^{*}\)</span> with <span class="math inline">\(d(v)=D\)</span>.</p>
<p>Now suppose that there are two distinct points <span class="math inline">\(v_1=p_1^*-q_1^*\)</span> and <span class="math inline">\(v_2=p_2^*-q_2^*\)</span> with <span class="math inline">\(d(v_1)=d(v_2)=D\)</span>. Consider the line segment <span class="math display">\[
t(s) = (1-s)v_1+sv_2\hbox{ where }0\le s\le 1
\]</span> joining <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span>.<br />
Now <span class="math display">\[
t(s) = ((1-s)p_1^*+sp_2^*)-((1-s)q_1^*+sq_2^*).
\]</span> Both terms in this difference belong to <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively, regardless of <span class="math inline">\(s\)</span>, by convexity, and therefore <span class="math inline">\(t(s)\)</span> belongs to <span class="math inline">\(V\)</span> for all <span class="math inline">\(0\le s\le 1\)</span>.</p>
<p>This little argument shows that <span class="math inline">\(V\)</span> is convex. In geometric terms, <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are two points in the set <span class="math inline">\(V\)</span> equidistant from the origin and the segment joining them is a chord of a circle; as +<span class="citation" data-cites="fig:chord">@fig:chord</span> shows, in that situation there must be a point on the line segment joining them that’s closer to the origin than they are. Since all the points on that segment are in <span class="math inline">\(V\)</span> by convexity, this would contradict the assumption that <span class="math inline">\(v_1\)</span> is the closet point in <span class="math inline">\(V\)</span> to the origin.</p>
<figure>
<img src="img/chord2.png" id="fig:chord" style="width:3in" alt="Chord of a circle" /><figcaption aria-hidden="true">Chord of a circle</figcaption>
</figure>
<p>In algebraic terms, since <span class="math inline">\(D\)</span> is the minimal value of <span class="math inline">\(\|v\|\)</span> for all <span class="math inline">\(v\in V\)</span>, we must have <span class="math inline">\(t(s)\ge D\)</span>.<br />
On the other hand <span class="math display">\[
\frac{d}{ds}\|t(s)\|^2 = \frac{d}{ds}(t(s)\cdot t(s)) =t(s)\cdot \frac{dt(s)}{ds} = t(s)\cdot(v_2-v_1).
\]</span> Therefore <span class="math display">\[
\frac{d}{ds}\|t(s)\|^2|_{s=0} = v_{1}\cdot(v_{2}-v_{1})=v_{1}\cdot v_{2}-\|v_{1}\|^2\le 0
\]</span> since <span class="math inline">\(v_{1}\cdot v_{2}\le D^{2}\)</span> and <span class="math inline">\(\|v_{1}\|^2=D^2\)</span>. If <span class="math inline">\(v_{1}\cdot v_{2}&lt;D^{2}\)</span>, then this derivative would be negative, which would mean that there is a value of <span class="math inline">\(s\)</span> where <span class="math inline">\(t(s)\)</span> would be less than <span class="math inline">\(D\)</span>. Since that can’t happen, we conclude that <span class="math inline">\(v_{1}\cdot v_{2}=D^{2}\)</span> which means that <span class="math inline">\(v_{1}=v_{2}\)</span> – the vectors have the same magnitude <span class="math inline">\(D\)</span> and are parallel. This establishes uniqueness.</p>
<p><strong>Note:</strong> The essential ideas of this argument show that a compact convex set in <span class="math inline">\(\mathbf{R}^{k}\)</span> has a unique point closest to the origin. The convex set in this instance, <span class="math display">\[
V=\{p-q:p\in C(A^{+}),q\in C(A^{-})\},
\]</span> is called the difference <span class="math inline">\(C(A^{+})-C(A^{-})\)</span>, and it is generally true that the difference of convex sets is convex.</p>
<p>Now we can conclude this line of argument.</p>
<p><strong>Theorem:</strong> Let <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> be points in <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively are such that <span class="math inline">\(\|p-q\|\)</span> is minimal among all such pairs. Let <span class="math inline">\(w=p-q\)</span> and set <span class="math inline">\(B^{+}=w\cdot p\)</span> and <span class="math inline">\(B^{-}=w\cdot q\)</span>. Then <span class="math inline">\(f^{+}(x)=w\cdot x-B^{+}=0\)</span> and <span class="math inline">\(f^{-}(x)=w\cdot x-B^{-}\)</span> are supporting hyperplanes for <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively and the associated margin <span class="math display">\[
\tau_{w}(A^{+},A^{-})=\frac{B^{+}-B^{-}}{\|w\|} = \|p-q\|
\]</span> is optimal.</p>
<p><strong>Proof:</strong> First we show that <span class="math inline">\(f^{+}(x)=0\)</span> is a supporting hyperplane for <span class="math inline">\(C(A^{+})\)</span>. Suppose not. Then there is a point <span class="math inline">\(p&#39;\in C(A^{+})\)</span> such that <span class="math inline">\(f^{+}(x)&lt;0\)</span>. Consider the line segment <span class="math inline">\(t(s) = (1-s)p+sp&#39;\)</span> running from <span class="math inline">\(p\)</span> to <span class="math inline">\(p&#39;\)</span>. By convexity it is entirely contained in <span class="math inline">\(C(A^{+})\)</span>. Now look at the distance from points on this segment to <span class="math inline">\(q\)</span>: <span class="math display">\[
D(s)=\|t(s)-q\|^2.
\]</span> We have <span class="math display">\[
\frac{dD(s)}{ds}|_{s=0} = 2(p-q)\cdot (p&#39;-p) = 2w\cdot (p&#39;-p) = 2\left[(f^{+}(p&#39;)+B^{+})-(f^{+}(p)+B^{+})\right]
\]</span> so <span class="math display">\[
\frac{dD(s)}{ds}|_{s=0} = 2(f^{+}(p&#39;)-f^{+}(p))&lt;0
\]</span> since <span class="math inline">\(f(p)=0\)</span>. This means that <span class="math inline">\(D(s)\)</span> is decreasing along <span class="math inline">\(t(s)\)</span> and so there is a point <span class="math inline">\(s&#39;\)</span> along <span class="math inline">\(t(s)\)</span> where <span class="math inline">\(\|t(s&#39;)-q\|&lt;D\)</span>. This contradicts the fact that <span class="math inline">\(D\)</span> is the minimal distance. The same argument shows that <span class="math inline">\(f^{-}(x)=0\)</span> is also a supporting hyperplane.</p>
<p>Now the margin for this <span class="math inline">\(w\)</span> is <span class="math display">\[
\tau_{w}(A^{+},A^{-}) = \frac{w\cdot (p-q)}{\|w\|} = \|p-q\|=D
\]</span> and as <span class="math inline">\(w\)</span> varies we know this is the largest possible <span class="math inline">\(\tau\)</span> that can occur. Thus this is the maximal margin.</p>
<p>*<span class="citation" data-cites="fig:strict">@fig:strict</span> shows how considering the closest point in the convex hulls “fixes” the problem that we saw in +<span class="citation" data-cites="fig:nonstrict">@fig:nonstrict</span>. The closest point occurs at a point on the boundary of the convex hull that is not one of the points in <span class="math inline">\(A^{+}\)</span> or <span class="math inline">\(A^{-}\)</span>.</p>
<figure>
<img src="img/ConvexHullWithMargin.png" id="fig:strict" style="width:50.0%" alt="Closest distance between convex hulls gives optimal margin" /><figcaption aria-hidden="true">Closest distance between convex hulls gives optimal margin</figcaption>
</figure>
<h2 id="finding-the-optimal-margin-classifier">Finding the Optimal Margin Classifier</h2>
<p>Now that we have translated our problem into geometry, we can attempt to develop an algorithm for solving it. To recap, we have two sets of points <span class="math display">\[
A^{+}=\{x^+_1,\ldots, x^+_{n_{+}}\}
\]</span> and <span class="math display">\[
A^{-}=\{x^-_1,\ldots, x^-_{n_{-}}\}
\]</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span> that are linearly separable.</p>
<p>We wish to find points <span class="math inline">\(p\in C(A^{+})\)</span> and <span class="math inline">\(q\in C(A^{-})\)</span> such that <span class="math display">\[
\|p-q\|=\min_{p&#39;\in C(A^{+}),q&#39;\in C(A^{-})} \|p&#39;-q&#39;\|.
\]</span></p>
<p>Using the definition of the convex hull we can express this more concretely. Since <span class="math inline">\(p\in C(A^{+})\)</span>, there are coefficients <span class="math inline">\(\lambda^{+}_{i}\ge 0\)</span> for <span class="math inline">\(i=1,\ldots,n_{+}\)</span> and <span class="math inline">\(\lambda^{-}_{i}\ge 0\)</span> for <span class="math inline">\(i=1,\ldots, n_{-}\)</span> so that <span class="math display">\[
\begin{aligned}
p&amp;=&amp;\sum_{i=1}^{n_{+}}\lambda^{+}_{i} x^{+}_{i} \\
q&amp;=&amp;\sum_{i=1}^{n_{-}}\lambda^{-}_{i} x^{-}_{i} \\
\end{aligned}
\]</span> where <span class="math inline">\(\sum_{i=1}^{n_{\pm}} \lambda_{i}^{\pm}=1\)</span>.</p>
<p>We can summarize this as follows:</p>
<p><strong>Optimization Problem 1:</strong> Write <span class="math inline">\(\lambda^{\pm}=(\lambda^{\pm}_{1},\ldots, \lambda^{\pm}_{n_{\pm}})\)</span> Define <span class="math display">\[
w(\lambda^+,\lambda^-) = \sum_{i=1}^{n_{+}}\lambda^{+}_{i}x^{+}_{i} - \sum_{i=1}^{n_{-}}\lambda^{-}x^{-}_{i}
\]</span> To find the supporting hyperplanes that define the optimal margin between <span class="math inline">\(A^{+}\)</span> and <span class="math inline">\(A^{-}\)</span>, find <span class="math inline">\(\lambda^{+}\)</span> and <span class="math inline">\(\lambda^{-}\)</span> such that <span class="math inline">\(\|w(\lambda^{+},\lambda^{-})\|^2\)</span> is minimal among all such <span class="math inline">\(w\)</span> where all <span class="math inline">\(\lambda^{\pm}_{i}\ge 0\)</span> and <span class="math inline">\(\sum_{i=1}^{n_{\pm}} \lambda^{\pm}_{i}=1\)</span>.</p>
<p>This is an example of a <em>constrained optimization problem.</em> It’s worth observing that the <em>objective function</em> <span class="math inline">\(\|w(\lambda^{+},\lambda^{-})\|^2\)</span> is just a quadratic function in the <span class="math inline">\(\lambda^{\pm}.\)</span> Indeed we can expand <span class="math display">\[
\|w(\lambda^{+},\lambda^{-})\|^2 = (\sum_{i=1}^{n_{+}}\lambda^{+}_{i}x_{i}- \sum_{i=1}^{n_{-}}\lambda^{-}x^{-}_{i})\cdot(\sum_{i=1}^{n_{+}}\lambda^{+}_{i}x_{i}- \sum_{i=1}^{n_{-}}\lambda^{-}x^{-}_{i})
\]</span> to obtain <span class="math display">\[
\|w(\lambda^{+},\lambda^{-})\|^2 = R -2S +T
\]</span> where <span class="math display">\[
\begin{aligned}
R &amp;=&amp; \sum_{i=1}^{n_{+}}\sum_{j=1}^{n_{+}}\lambda^{+}_{i}\lambda^{+}_{j}(x^{+}_{i}\cdot x^{+}_{j}) \\
S &amp;=&amp; \sum_{i=1}^{n_{+}}\sum_{j=1}^{n_{-}}\lambda^{+}_{i}\lambda^{-}_{j}(x^{+}_{i}\cdot x^{-}_{j}) \\
T &amp;=&amp; \sum_{i=1}^{n_{-}}\sum_{j=1}^{n_{-}}\lambda^{-}_{i}\lambda^{-}_{j}(x^{-}_{i}\cdot x^{-}_{j}) \\
\end{aligned}
\]</span>{#eq:kernel} Thus the function we are trying to minimize is relatively simple.</p>
<p>On the other hand, unlike optimization problems we have seen earlier in these lectures, in which we can apply Lagrange multipliers, in this case some of the constraints are inequalities – namely the requirement that all of the <span class="math inline">\(\lambda^{\pm}\ge 0\)</span> – rather than equalities. There is an extensive theory of such problems that derives from the idea of Lagrange multipliers. However, in these notes, we will not dive into that theory but will instead construct an algorithm for solving the problem directly.</p>
<h3 id="relaxing-the-constraints">Relaxing the constraints</h3>
<p>Our first step in attacking this problem is to adjust our constraints and our objective function slightly so that the problem becomes easier to attack.</p>
<p><strong>Optimization Problem 2:</strong> This is a slight revision of problem 1 above. We minimize: <span class="math display">\[
Q(\lambda^{+},\lambda^{-}) = \|w(\lambda^{+},\lambda^{-})\|^2-\sum_{i=1}^{n_{+}}\lambda^{+}_{i}-\sum_{i=1}^{n_{-}}\lambda^{-}_{i}
\]</span> subject to the constraints that all <span class="math inline">\(\lambda^{\pm}_{i}\ge 0\)</span> and <span class="math display">\[
\alpha = \sum_{i=1}^{n_{+}}\lambda^+_{i} = \sum_{i=1}^{n_{-}}\lambda^{-}_{i}.
\]</span></p>
<p>Problem 2 is like problem 1, except we don’t require the sums of the <span class="math inline">\(\lambda^{\pm}_{i}\)</span> to be one, but only that they be equal to each other; and we modify the objective function slightly. It turns out that the solution to this optimization problem easily yields the solution to our original one.</p>
<p><strong>Lemma:</strong> Suppose <span class="math inline">\(\lambda^{+}\)</span> and <span class="math inline">\(\lambda^{-}\)</span> satisfy the constraints of problem 2 and yield the minimal value for the objective function <span class="math inline">\(Q(\lambda^{+},\lambda^{-})\)</span>. Then <span class="math inline">\(\alpha\not=0\)</span>. Rescale the <span class="math inline">\(\lambda^{\pm}\)</span> to have sum equal to one by dividing by <span class="math inline">\(\alpha\)</span>, yielding <span class="math inline">\(\tau^{\pm}=(1/\alpha)\lambda^{\pm}\)</span>. Then <span class="math inline">\(w(\tau^{+},\tau^{-})\)</span> is a solution to optimization problem 1.</p>
<p><strong>Proof:</strong> To show that <span class="math inline">\(\alpha\not=0\)</span>, suppose that <span class="math inline">\(\lambda^{\pm}_{i}=0\)</span> for all <span class="math inline">\(i\not=1\)</span> and <span class="math inline">\(\lambda=\lambda^{+}_{1}=\lambda^{-}_{1}\)</span>. The one-variable quadratic function <span class="math inline">\(Q(\lambda)\)</span> takes its minimum value at <span class="math inline">\(\lambda=1/\|x_{1}^{+}-x_{1}^{-}\|^2\)</span> and its value at that point is negative. Therefore the minimum value of <span class="math inline">\(Q\)</span> is negative, which means <span class="math inline">\(\alpha\not=0\)</span> at that minimum point.</p>
<p>For the equivalence, notice that <span class="math inline">\(\tau^{\pm}\)</span> still satisfy the constraints of problem 2. Therefore <span class="math display">\[
Q(\lambda^{+},\lambda^{-}) = \|w(\lambda^{+},\lambda^{-})\|^2-2\alpha\le \|w(\tau^{+},\tau^{-})\|^2-2.
\]</span> On the other hand, suppose that <span class="math inline">\(\sigma^{\pm}\)</span> are a solution to problem 1. Then <span class="math display">\[
\|w(\sigma^{+},\sigma^{-})\|^2\le \|w(\tau^{+},\tau^{-})\|^2.
\]</span> Therefore <span class="math display">\[
\alpha^2 \|w(\sigma^{+},\sigma^{-})\|^2 = \|w(\alpha\sigma^{+},\alpha\sigma^{-})\|^2\le \|w(\lambda^{+},\lambda^{-})\|^2
\]</span> and finally <span class="math display">\[
\|w(\alpha\sigma^{+},\alpha\sigma^{-})\|^2-2\alpha\le Q(\lambda^{+},\lambda^{-})=\|w(\alpha\tau^{+},\alpha\tau^{-})\|^2-2\alpha.
\]</span> Since <span class="math inline">\(Q\)</span> is the minimal value, we have <span class="math display">\[
\alpha^{2}\|w(\sigma^{+},\sigma^{-})\|^2 = \alpha^{2}\|w(\tau^{+},\tau^{-})\|^2
\]</span> so that indeed <span class="math inline">\(w(\tau^{+},\tau^{-})\)</span> gives a solution to Problem 1.</p>
<h3 id="sequential-minimal-optimization">Sequential Minimal Optimization</h3>
<p>Now we outline an algorithm for solving Problem 2 that is called Sequential Minimal Optimization that was introduced by John Platt in 1998 (See <span class="citation" data-cites="plattSMO">@plattSMO</span> and Chapter 12 of <span class="citation" data-cites="KernelMethodAdvances">@KernelMethodAdvances</span>). The algorithm is based on the principle of “gradient ascent”, where we exploit the fact that the negative gradient of a function points in the direction of its most rapid decrease and we take small steps in the direction of the negative gradient until we reach the minimum.</p>
<p>However, in this case simplify this idea a little. Recall that the objective function <span class="math inline">\(Q(\lambda^{+},\lambda^{-})\)</span> is a quadratic function in the <span class="math inline">\(\lambda\)</span>’s and that we need to preserve the condition that <span class="math inline">\(\sum \lambda^{+}_{i}=\sum\lambda^{-}_{i}\)</span>. So our approach is going to be to take, one at a time, a pair <span class="math inline">\(\lambda^{+}_{i}\)</span> and <span class="math inline">\(\lambda^{-}_{j}\)</span> and change them <em>together</em> so that the equality of the sums is preserved and the change reduces the value of the objective function. Iterating this will take us to a minimum.</p>
<p>So, for example, let’s look at <span class="math inline">\(\lambda^{+}_i\)</span> and <span class="math inline">\(\lambda^{-}_{j}\)</span> and, for the moment, think of all of the other <span class="math inline">\(\lambda\)</span>’s as constants. Then our objective function reduces to a quadratic function of these two variables that looks something like: <span class="math display">\[
Q(\lambda_{i}^{+},\lambda_{j}^{-}) = a(\lambda^{+}_i)^2+b\lambda^{+}_i\lambda^{-}_j+c(\lambda^{-}_{i})^2+d\lambda^{+}_i+e\lambda^{-}_{j}+f.
\]</span> The constraints that remain are <span class="math inline">\(\lambda^{\pm}\ge 0\)</span>, and we are going to try to minimize <span class="math inline">\(Q\)</span> by changing <span class="math inline">\(\lambda_{i}^{+}\)</span> and <span class="math inline">\(\lambda_{j}^{-}\)</span> <em>by the same amount</em> <span class="math inline">\(\delta\)</span>. Furthermore, since we still must have <span class="math inline">\(\lambda_{i}^{+}+\delta\ge 0\)</span> and <span class="math inline">\(\lambda_{j}^{-}+\delta\ge 0\)</span>, we have</p>
<p><span class="math display">\[
\delta\ge M=\max\{-\lambda_{i}^{+},-\lambda_{j}^{-}\}
\]</span>{#eq:delta}</p>
<p>In terms of this single variable <span class="math inline">\(\delta\)</span>, our optimization problem becomes the job of finding the minimum of a quadratic polynomial in one variable subject to the constraint in +<span class="citation" data-cites="eq:delta">@eq:delta</span>. This is easy! There are two cases: the critical point of the quadratic is to the left of <span class="math inline">\(M\)</span>, in which case the minimum value occurs at <span class="math inline">\(M\)</span>; or the critical point of the quadratic is to the right of <span class="math inline">\(M\)</span>, in which case the critical point occurs there. This is illustrated in +<span class="citation" data-cites="fig:quadratics">@fig:quadratics</span>.</p>
<figure>
<img src="img/quadratic.png" id="fig:quadratics" style="width:50.0%" alt="Minimizing the 1-variable quadratic objective function" /><figcaption aria-hidden="true">Minimizing the 1-variable quadratic objective function</figcaption>
</figure>
<p>Computationally, let’s write <span class="math display">\[
w_{\delta,i,j}(\lambda^{+},\lambda^{-}) = w(\lambda^{+},\lambda^{-})+\delta(x^{+}_{i}-x^{-}_{j}).
\]</span> Then <span class="math display">\[
\frac{d}{d\delta}(\|w_{\delta,i,j}(\lambda^{+},\lambda^{-})\|^2-2\alpha)  = 2w_{\delta,i,j}(\lambda^{+},\lambda^{-})\cdot(x^{+}_{i}-x^{-}_{j})-2
\]</span> and using the definition of <span class="math inline">\(w_{\delta,i,j}\)</span> we obtain the following formula for the critical value of <span class="math inline">\(\delta\)</span> by setting this derivative to zero: <span class="math display">\[
\delta_{i,j} = \frac{(1-w(\lambda^{+},\lambda^{-})\cdot(x_{i}^{+}-x_{j}^{-})}{\|x^+_{i}-x^{-}_{j}\|^2}
\]</span></p>
<p>Using this information we can describe the SMO algorithm.</p>
<p><strong>Algorithm (SMO, see <span class="citation" data-cites="plattSMO">@plattSMO</span>):</strong></p>
<p><strong>Given:</strong> Two linearly separable sets of points <span class="math inline">\(A^{+}=\{x_{1}^{+},\ldots,x_{n_{+}}^{+}\}\)</span> and <span class="math inline">\(A^{-}=\{x_{1}^{-},\ldots, x_{n_{-}}^{-}\}\)</span> in <span class="math inline">\(\mathbf{R}^{k}\)</span>.</p>
<p><strong>Find:</strong> Points <span class="math inline">\(p\)</span> and <span class="math inline">\(q\)</span> belonging to <span class="math inline">\(C(A^{+})\)</span> and <span class="math inline">\(C(A^{-})\)</span> respectively such that <span class="math display">\[
\|p-q\|^2=\min_{p&#39;\in C(A^{+}),q&#39;\in C(A^{-})} \|p&#39;-q&#39;\|^2
\]</span></p>
<p><strong>Initialization:</strong> Set <span class="math inline">\(\lambda_{i}^{+}=\frac{1}{n_{+}}\)</span> for <span class="math inline">\(i=1,\ldots, n_{+}\)</span> and <span class="math inline">\(\lambda_{i}^{-}=\frac{1}{n_{-}}\)</span> for <span class="math inline">\(i=1,\ldots, n_{-}\)</span>. Set <span class="math display">\[
p(\lambda^{+})=\sum_{i=1}^{n_{+}}\lambda^{+}_{i}x^{+}_{i}
\]</span> and <span class="math display">\[
q(\lambda^{-})=\sum_{i=1}^{n_{-}}\lambda^{-}_{i}x^{-}_{i}
\]</span> Notice that <span class="math inline">\(w(\lambda^{+},\lambda^{-})=p(\lambda^{+})-q(\lambda^{-})\)</span>. Let <span class="math inline">\(\alpha=\sum_{i=1}^{n_{+}}\lambda^{+}=\sum_{i=1}^{n_{-}}\lambda^{-}\)</span>. These sums will remain equal to each other throughout the operation of the algorithm.</p>
<p>Repeat the following steps until maximum value of <span class="math inline">\(\delta^{*}\)</span> computed in each iteration is smaller than some tolerance (so that the change in all of the <span class="math inline">\(\lambda\)</span>’s is very small):</p>
<ul>
<li>For each pair <span class="math inline">\(i,j\)</span> with <span class="math inline">\(1\le i\le n_{+}\)</span> and <span class="math inline">\(1\le j\le n_{-}\)</span>, compute <span class="math display">\[
M_{i,j} = \max\{-\lambda_{i}^{+},-\lambda_{j}^{-}\}
\]</span> and <span class="math display">\[
\delta_{i,j} = \frac{1-(p(\lambda^{+})-q(\lambda^{-}))\cdot(x_{i}^{+}-x_{j}^{-})}{\|x^+_{i}-x^{-}_{j}\|^2}.
\]</span> If <span class="math inline">\(\delta_{i,j}\ge M\)</span> then set <span class="math inline">\(\delta^{*}=\delta_{i,j}\)</span>; otherwise set <span class="math inline">\(\delta^{*}=M\)</span>. Then update the <span class="math inline">\(\lambda^{\pm}\)</span> by the equations: <span class="math display">\[
\begin{aligned}
\lambda^{+}_{i}&amp;=&amp;\lambda^{+}_{i}+\delta_{i,j}^{*} \\
\lambda^{+}_{j}&amp;=&amp;\lambda^{-}_{j}+\delta_{i,j}^{*} \\
\end{aligned}
\]</span></li>
</ul>
<p>When this algorithm finishes, <span class="math inline">\(p\approx p(\lambda^{+})\)</span> and <span class="math inline">\(q\approx q(\lambda^{-})\)</span> will be very good approximations to the desired closest points.</p>
<p>Recall that if we set <span class="math inline">\(w=p-q\)</span>, then the optimal margin classifier is</p>
<p><span class="math display">\[
f(x)=w\cdot x - \frac{B^{+}+B^{-}}{2}=0
\]</span></p>
<p>where <span class="math inline">\(B^{+}=w\cdot p\)</span> and <span class="math inline">\(B^{-}=w\cdot q\)</span>. Since <span class="math inline">\(w=p-q\)</span> we can simplify this to obtain</p>
<p><span class="math display">\[
f(x)=(p-q)\cdot x -\frac{\|p\|^2-\|q\|^2}{2}=0.
\]</span></p>
<p>In +<span class="citation" data-cites="fig:penguinsolution">@fig:penguinsolution</span>, we show the result of applying this algorithm to the penguin data and illustrate the closest points as found by an implementation of the SMO algorithm, together with the optimal classifying line.</p>
<p>Bearing in mind that the y-axis is scaled by a factor of 200, we obtain the following rule for distinguishing between Adelie and Gentoo penguins – if the culmen depth and body mass put you above the red line, you are a Gentoo penguin, otherwise you are an Adelie.</p>
<figure>
<img src="img/solution.png" id="fig:penguinsolution" style="width:50.0%" alt="Closest points in convex hulls of penguin data" /><figcaption aria-hidden="true">Closest points in convex hulls of penguin data</figcaption>
</figure>
<h2 id="inseparable-sets">Inseparable Sets</h2>
<p>Not surprisingly, real life is often more complicated than the penguin example we’ve discussed at length in these notes. In particular, sometimes we have to work with sets that are not linearly separable. Instead, we might have two point clouds, the bulk of which are separable, but because of some outliers there is no hyperplane we can draw that separates the two sets into two halfplanes.</p>
<p>Fortunately, all is not lost. There are two common ways to address this problem, and while we won’t take the time to develop the theory behind them, we can at least outline how they work.</p>
<h3 id="best-separating-hyperplanes">Best Separating Hyperplanes</h3>
<p>If our sets are not linearly separable, then their convex hulls overlap and so our technique for finding the closest points of the convex hulls won’t work. In this case, we can “shrink” the convex hull by considering combinations of points <span class="math inline">\(\sum_{i}\lambda_{i}x_{i}\)</span> where <span class="math inline">\(\sum\lambda_{i}=1\)</span> and <span class="math inline">\(C\ge\lambda_{i}\ge 0\)</span> for some <span class="math inline">\(C\le 1\)</span>. For <span class="math inline">\(C\)</span> small enough, reduced convex hulls will be linearly separable – although some outlier points from each class will lie outside of them – and we can find hyperplane that separates the reduced hulls.<br />
In practice, this means we allow a few points to lie on the “wrong side” of the hyperplane. Our tolerance for these mistakes depends on <span class="math inline">\(C\)</span>, but we can include <span class="math inline">\(C\)</span> in the optimization problem to try to find the smallest <span class="math inline">\(C\)</span> that “works”.</p>
<h3 id="nonlinear-kernels">Nonlinear kernels</h3>
<p>The second option is to look not for separating hyperplanes but instead for separating curves – perhaps polynomials or even more exotic curves. This can be achieved by taking advantage of the form of +<span class="citation" data-cites="eq:kernel">@eq:kernel</span>. As you see there, the only way the points <span class="math inline">\(x_{i}^{\pm}\)</span> enter in to the function being minimized is through the inner products <span class="math inline">\(x_{i}^{\pm}\cdot x_{j}^{\pm}\)</span>. We can adopt a different inner product than the usual Euclidean one, and reconsider the problem using this different inner product. This amounts to embedding our points in a higher dimensional space where they are more likely to be linearly separable. Again, we will not pursue the mathematics of this further in these notes.</p>
<h2 id="sec:exercises">Exercises</h2>
<ol type="1">
<li><p>Prove that, if <span class="math inline">\(f(x)=w\cdot x+b=0\)</span> is a hyperplane in <span class="math inline">\(\mathbf{R}^{k}\)</span>, then the two “sides” of this hyperplane, consisting of the points where <span class="math inline">\(f(x)\ge 0\)</span> and <span class="math inline">\(f(x)\le 0\)</span>, are both convex sets.</p></li>
<li><p>Prove that <span class="math inline">\(C(S)\)</span> is the intersection of all the halfplanes <span class="math inline">\(f(x)\ge 0\)</span> as <span class="math inline">\(f(x)=w\cdot x+b\)</span> runs through all supporting hyperplanes for <span class="math inline">\(S\)</span> where <span class="math inline">\(f(x)\ge 0\)</span> for all <span class="math inline">\(p\in S\)</span>.</p></li>
<li><p>Prove that <span class="math inline">\(C(S)\)</span> is bounded. Hint: show that <span class="math inline">\(S\)</span> is contained in a sphere of sufficiently large radius centered at zero, and then that <span class="math inline">\(C(S)\)</span> is contained in that sphere as well.</p></li>
<li><p>Confirm the final formula for the optimal margin classifier at the end of the lecture.</p></li>
</ol>
</body>
</html>
